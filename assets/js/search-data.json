{"0": {
    "doc": "About",
    "title": "About",
    "content": "about me . Hello 👋! . | I’m abel. I am a research engineer at Insta360. | Include: Multimodal, Large Language Model and Natural Language Processing. | Previously, I received my bachelor’s and master’s degree from Northeastern University, China. | . | You might come across my work by searching keywords like TechGPT or Multimodal Relation Extraction. However, these projects are still in progress and haven’t reached significant milestones yet (please bear with me on this). | Thank you for visiting. | . Contact me: . | 📬Email | 🐱Github | . ",
    "url": "/about/",
    
    "relUrl": "/about/"
  },"1": {
    "doc": "Hello",
    "title": "1. Create a New Jekyll Project",
    "content": "Use the following command to generate a new Jekyll site: . bundle exec jekyll new myblog . This creates a new directory named myblog with the default Jekyll structure. ",
    "url": "/docs/hello-world.html#1-create-a-new-jekyll-project",
    
    "relUrl": "/docs/hello-world.html#1-create-a-new-jekyll-project"
  },"2": {
    "doc": "Hello",
    "title": "Start a Local Server",
    "content": "Navigate to your project directory and start the development server: . bundle exec jekyll serve . By default, the site will be available at http://localhost:4000. You can customize the server settings if needed: . bundle exec jekyll serve --port 8080 . ",
    "url": "/docs/hello-world.html#start-a-local-server",
    
    "relUrl": "/docs/hello-world.html#start-a-local-server"
  },"3": {
    "doc": "Hello",
    "title": "Build the Static Site",
    "content": "To generate the static files in the _site directory, run: . bundle exec jekyll build . The contents of _site can then be deployed to your web server or hosting platform. ",
    "url": "/docs/hello-world.html#build-the-static-site",
    
    "relUrl": "/docs/hello-world.html#build-the-static-site"
  },"4": {
    "doc": "Hello",
    "title": "initialize The Search",
    "content": "initialize the search by running this rake command that comes with just-the-docs . bundle exec just-the-docs rake search:init . ",
    "url": "/docs/hello-world.html#initialize-the-search",
    
    "relUrl": "/docs/hello-world.html#initialize-the-search"
  },"5": {
    "doc": "Hello",
    "title": "Hello",
    "content": "official . This is just a demo page . This guide covers the basic steps to quickly set up and run a Jekyll project. ",
    "url": "/docs/hello-world.html",
    
    "relUrl": "/docs/hello-world.html"
  },"6": {
    "doc": "Ideas",
    "title": "Ideas",
    "content": "official . A space to capture spontaneous thoughts, random ideas, and moments of inspiration. ",
    "url": "/docs/idea/",
    
    "relUrl": "/docs/idea/"
  },"7": {
    "doc": "Normalization and Attention",
    "title": "1 Normalization",
    "content": "1.1 LayerNorm: . layerNorm是自然语言处理任务中最为常用的一种正则化函数，和BatchNorm不同的在于，它计算的是每个样本的隐藏层的正则。 . 对于输入\\(x \\in \\mathbb{R}^{B \\times L \\times H}\\)（其中B表示batch size，L表示序列长度，H表示特征维度），LayerNorm通过如下步骤完成对特征维度的正则化处理。 . | 按照公式（1.1）得到均值\\(\\mu\\)（一阶原点矩）： | . \\[\\mu = \\frac{1}{H}\\sum^H_{i=1} x_i \\tag{1.1}\\] . | 按照公式（1.2）得到方差\\(\\sigma^2\\)（二阶中心矩）： | . \\[\\sigma^2 = \\frac{1}{H} \\sum ^H _{i=1} (x_i - \\mu)^2 \\tag{1.2}\\] . | 最后再按照公式（1.3）得到正则化值： | . \\[\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta \\tag{1.3}\\] 其中，\\(\\epsilon\\)是为了避免除零错误而引入的一个很小的数，\\(\\gamma\\)和\\(\\beta\\)分别表示可学习的缩放参数和偏置。 . 一个简单的实现如下： . import torch import torch.nn as nn class LayerNorm(nn.Module): def __init__(self, dim: int, eps=1e-5): super().__init__() self.eps = eps self.gamma = nn.Parameter(torch.ones(dim)) self.beta = nn.Parameter(torch.zeros(dim)) def forward(self): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) x_norm = (x - mean) / torch.sqrt(var + self.eps) return self.gamma * x_norm + self.beta . 而在LLaMA等大模型中，用的更多的是RMSNorm（Root Mean Square Norm），具体计算如下： . 1.2 RMSNorm: . | 类似的输入\\(x\\)，首先根据公式（1.4）计算出均方根（二阶原点矩开根号）： | . \\[\\text{RMS}(x) = \\sqrt{\\frac{1}{H} \\sum ^H _{i=1}x^2_i} \\tag{1.4}\\] . | 之后按公式（1.5）得到正则化的值： | . \\[\\text{RMSNorm}(x) = \\gamma \\frac{x}{\\text{RMS}(x) + \\epsilon} \\tag{1.5}\\] 其中\\(\\gamma\\)和\\(\\epsilon\\)分别是可学习的缩放参数和防止除零操作。 . 和LayerNorm做对比，RMSNorm的主要区别在于：直接计算二阶原点矩，而不是先计算二阶中心矩，因此少了求均值的操作（没有了减均值的步骤） . 正是因为少了一步操作，RMSNorm计算步骤更少，因此速度更快，对于对计算量消耗巨大的大模型而言非常有益。 . 实际验证发现RMSNorm与LayerNorm相比，性能上的损失并不大，但其带来的速度增益更可观。 . 其简单的实现如下： . import torch import torch.nn as nn class RMSNorm(nn.Module): def __init__(self, dim: int, eps=1e-8): super().__init__() self.eps = eps self.gamma = nn.Parameter(torch.ones(dim)) def forward(self, x): rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps) x_norm = x / rms return self.gamma * x_norm . 这里做了一个（并不严谨）非常简单的小实验，在NYT24数据集上做分类（不求三元组，只求每个样本中可能存在的关系，即多标签分类） . | 模型方面，使用BERT的分词器和nn.Embedding构建了一个非常简单的模型，其参数量仅为7.92M | . Figure 1.1 LayerNorm和RMSNorm的F1值变化曲线 如图1.1所示，两个不同正则化函数的最好F1值分别为： . | LayerNorm: 0.9585 . | RMSNorm: 0.9559 . | . 可见二者的性能差别并不大。 . Figure 1.2 LayerNorm和RMSNorm的loss变化曲线 图1.2显示了两个正则函数的loss变化，虽然一开始RMSNorm有着略微大一些的loss，但并不影响RMSNorm的学习效率。 . 二者最终的训练时间分别为： . | LayerNorm: 231.2874秒 . | RMSNorm: 214.5342秒 . | . 可见RMSNorm确实有着更少的学习时间，所节省的时间在比7.92M大的多的模型上会更具优势。 . ",
    "url": "/docs/tech/minimind/norm-and-attn/#1-normalization",
    
    "relUrl": "/docs/tech/minimind/norm-and-attn/#1-normalization"
  },"8": {
    "doc": "Normalization and Attention",
    "title": "2 Attention with RoPE",
    "content": "下文需要先了解绝对位置编码和相对位置编码，可参见： . | Attention Is All You Need | 让研究人员绞尽脑汁的Transformer位置编码 | . 旋转位置编码的来源和详细介绍可以参见： . | RoFormer: Enhanced Transformer with Rotary Position Embedding | Transformer升级之路：2、博采众长的旋转式位置编码 | . RoFormer的提出动机非常简单，苏剑林在博客中自称是通过\\(e^{im\\theta}\\bar{e^{in\\theta}} = e^{i(m-n)\\theta}\\)发现可以构建出一种实际上是相对，表达上又是绝对形式的位置编码。 . 在介绍具体含义前，先引入两个前置知识： . 2.1 前置知识 . 2.1.1 二维平面的旋转 . 从计算机图形学的视角来看，什么是旋转？ . 假设有二维向量\\(v = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)，对他做二维平面旋转，旋转角度为\\(\\theta\\)，可以通过旋转矩阵实现。旋转矩阵的形式如公式（2.1）所示。 . \\[R_{\\theta} = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\\\ \\end{bmatrix} \\tag{2.1}\\] 因此旋转后的向量如公式（2.2）所示。 . \\[v' = R_{\\theta} \\cdot v \\tag{2.2}\\] 这个操作在计算机图像学中表示“将图像绕原点旋转\\(\\theta\\)度”（在实现时需要先计算出图像的原点坐标）。 . 2.1.2 二维复平面的旋转 . 复数\\(e^{i\\theta}\\)表示了一个沿着单元圆逆时针旋转角度为\\(\\theta\\)的点的位置（欧拉公式），即\\(e^{i\\theta} = \\cos\\theta + i \\sin \\theta\\)，其中\\(\\cos\\theta\\)是水平方向的坐标（实部），\\(\\sin\\theta\\)是竖直方向的坐标（虚部）。 . 而两个点\\(z_1 = e^{im\\theta}\\)和\\(z_2 = e^{in\\theta}\\)之间的乘积如公式（2.3）表示： . \\[\\begin{equation} \\begin{aligned} z_1 \\times z_2 &amp;= e^{im\\theta} \\times e^{in\\theta}\\\\ &amp; = \\cos m\\theta \\cos n\\theta + i^2 \\sin m \\theta \\sin n \\theta + i (\\cos m \\theta \\sin n \\theta + \\sin m \\theta \\cos n \\theta) \\\\ &amp; = \\cos (m+n)\\theta + i \\sin (m+n)\\theta \\\\ &amp; = e ^ {i (m+n) \\theta} \\end{aligned} \\tag{2.3} \\end{equation}\\] 也就是说，二者的乘积实际上表示了两个点在复平面上的角度之和。 . 如果用\\(z_2\\)的共轭\\(\\bar{z_2} = e^{-i n \\theta}\\)，就变成了公式（2.4）所示的角度之差： . \\[z_1 \\times \\bar{z_2} = e^{i (m-n)\\theta} \\tag{2.4}\\] 因此就能够建模出两个复数之间的相对性。 . 根据前置知识，假设二维平面上有一个点为\\((x, y)\\)，则它旋转\\(\\theta\\)后的坐标如公式（2.5）所示： . \\[\\begin{bmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\ x \\sin \\theta + y \\cos \\theta \\end{bmatrix} \\tag{2.5}\\] 同理，假设二维复平面有 \\(z = x + i y\\)，则乘上旋转角度的复数变化\\(e ^ {i\\theta}\\)后的复数如公式（2.6）所示： . \\[\\begin{equation} \\begin{aligned} e^{i\\theta} \\cdot z &amp; = (\\cos \\theta + i \\sin \\theta)(x + i y) \\\\ &amp; = (x \\cos \\theta - y \\sin \\theta) + i (x \\sin \\theta + y \\cos \\theta) \\end{aligned} \\tag{2.6} \\end{equation}\\] 可见，公式（2.6）变化后的实部和虚部恰好和与公式（2.5）旋转后的坐标相对应，所以本质上一个复数乘以\\(e^{i \\theta}\\)，就是乘了一个旋转矩阵。 . 2.2 旋转位置编码 . 本来想写不少的，但发现已经有不少优秀的博客做了解释，可以直接参考，如： . | 十分钟读懂旋转编码（RoPE） | . 简单来说，旋转位置编码就是实现了使每个token的位置可以直接被计算出来，就像绝对位置编码那样，同时又能在注意力计算时建模出token之间的相对位置关系。 . 在实际计算时，对于位置为\\(i\\)的token，将其特征向量分组（按照奇偶进行分组），从而有公式（2.7）所示的转换。 . \\[\\text{RoPE}(x_i) = (x_i^{even} \\cos i\\theta - x_i^{odd} \\sin i\\theta) \\oplus (x_i^{even} \\sin i\\theta + x_i ^{odd}\\cos i\\theta) \\tag{2.7}\\] 因此，当计算注意力相关性分数\\(q_m^Tk_n\\)时，可以得到含有相对位置信息\\((m-n)\\)的特征。 . 也就是说，RoPE将每个token的特征向量都看作是由若干个二维子空间组成的向量，然后对于每个二维子空间上都根据token所在位置\\(i\\)旋转角度\\(i\\theta\\)。 . 而在计算相关性分数时，由于复数的性质（或者说旋转矩阵的特点），自然的就会得到任意两个token之间的位置信息，相当于把任意两个token看作是一个旋转了\\((i-j)\\theta\\)度。 . ",
    "url": "/docs/tech/minimind/norm-and-attn/#2-attention-with-rope",
    
    "relUrl": "/docs/tech/minimind/norm-and-attn/#2-attention-with-rope"
  },"9": {
    "doc": "Normalization and Attention",
    "title": "Normalization and Attention",
    "content": "tech notes . 本节将对大模型中常用的RMSNorm和基于旋转位置编码的注意力机制做介绍 . ",
    "url": "/docs/tech/minimind/norm-and-attn/",
    
    "relUrl": "/docs/tech/minimind/norm-and-attn/"
  },"10": {
    "doc": "Build LLM from Scratch",
    "title": "Build LLM from Scratch",
    "content": "这部分内容旨在借助开源项目MiniMind，从理论和代码入手，分析大模型的预训练和后训练的步骤，解释各核心部分的工作原理。 . 所写内容不一定正确，如有错误还望指正。 . ",
    "url": "/docs/tech/minimind/#build-llm-from-scratch",
    
    "relUrl": "/docs/tech/minimind/#build-llm-from-scratch"
  },"11": {
    "doc": "Build LLM from Scratch",
    "title": "Build LLM from Scratch",
    "content": "tech notes . ",
    "url": "/docs/tech/minimind/",
    
    "relUrl": "/docs/tech/minimind/"
  },"12": {
    "doc": "Server Management Notes",
    "title": "Linux CUDA配置",
    "content": "nvidia-smi和驱动通信失败 . | 当我重启服务器后，输入nvidia-smi命令后，出现报错： . | NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. | . | 这个时候查询nvcc会发现其实驱动相关的东西其实是还在的： . nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 . | 接着查询当前的驱动版本： . ls /usr/src | grep nvidia # 输入后出现： nvidia-515.43.04 . 记住上面的数字，自己是多少就是多少 . | 输入： . sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 . 等待安装完成即可 . | . ",
    "url": "/docs/tech/server/#linux-cuda%E9%85%8D%E7%BD%AE",
    
    "relUrl": "/docs/tech/server/#linux-cuda配置"
  },"13": {
    "doc": "Server Management Notes",
    "title": "Linux网络配置",
    "content": "内网穿透 . 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。 . 之后我们发现了新的，更简单的免费方法，现记录如下： . | 拥有外网ip的服务器，假设其公网ip为：x.x.x.x . | 需要被映射的内网服务器，假设其内网ip为：y.y.y.y . | 假设公网被访问的端口为8888，内网需要被映射的端口为9999，则： . # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 . | 在公网服务器上输入： . curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 . | 之后，继续在公网服务器上输入： . sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 . | 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888 . | . 端口防火墙 . | 打开某个端口的防火墙 . sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload . | 查看所有打开的端口 . sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp . | . 配置ipv6： . 参考: asimok’s blog . | 检查是否已经启用ipv6支持 . sudo cat /proc/net/if_inet6 . 如果结果不为空，直接下一步，否则： . sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 . | 先找一个比较快的ipv6的DNS，比如清华源等； . | 修改配置文件，添加DNS: . sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 . | 重启DNS服务： . sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved . | 启动配置文件： . sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ . | 检查是否启用成功： . sudo cat /etc/resolv.conf . | . 开机进入紧急模式 . | 机器开机后，出现: welcome to emergency mode，这可能是因为写入的自动挂载脚本问题导致的，输入： . vim /etc/fstab . 查看一下当初挂载了哪些磁盘，尤其是有的磁盘可能会发生变化，从而导致自检不通过 . | 返回命令行，输入： . df -h . 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么： . # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 . | . ",
    "url": "/docs/tech/server/#linux%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE",
    
    "relUrl": "/docs/tech/server/#linux网络配置"
  },"14": {
    "doc": "Server Management Notes",
    "title": "Linux账户配置",
    "content": "root . Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。 . 在具有sudo权限的用户下进行操作； . | 设置root账户密码： . passwd root . | 编辑配置文件： . sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes . | 重启ssh服务： . systemctl restart ssh . | . 需要注意，root用户具有完全权限，比一般的sudo用户更高，使用时务必小心。 . sudo . 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面； . | 查看sudo用户： . # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 . | 将用户添加到sudo组： . sudo usermod -aG sudo username # 将username替换为用户账户名 . | 将用户从sudo组移除： . sudo deluser username sudo . | 禁止用户登录： . sudo passwd -l username # or sudo usermod -L username . | 恢复用户登录： . sudo passwd -u username # or sudo usermod -U username . | . ",
    "url": "/docs/tech/server/#linux%E8%B4%A6%E6%88%B7%E9%85%8D%E7%BD%AE",
    
    "relUrl": "/docs/tech/server/#linux账户配置"
  },"15": {
    "doc": "Server Management Notes",
    "title": "其他",
    "content": "利用Docker配置私人网盘 . | 首先拉取docker： . docker pull cloudreve/cloudreve . | 接着创建必要的文件： . mkdir -vp cloudreve/{uploads,avatar} \\ &amp;&amp; touch cloudreve/conf.ini \\ &amp;&amp; touch cloudreve/cloudreve.db . | 然后启动docker： . | 先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver . | 然后配置文件，并启动： . sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\"Asia/Shanghai\" \\ cloudreve/cloudreve:latest . | . | 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置： . docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword . 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org . | 常用docker命令： . docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx . | . ",
    "url": "/docs/tech/server/#%E5%85%B6%E4%BB%96",
    
    "relUrl": "/docs/tech/server/#其他"
  },"16": {
    "doc": "Server Management Notes",
    "title": "Server Management Notes",
    "content": "tech notes . ",
    "url": "/docs/tech/server/",
    
    "relUrl": "/docs/tech/server/"
  },"17": {
    "doc": "Tech",
    "title": "Tech",
    "content": "official . Engineering knowledge and tips gathered from writing code, debugging, and working through real-world technical challenges. A growing collection of hands-on learning. ",
    "url": "/docs/tech/",
    
    "relUrl": "/docs/tech/"
  },"18": {
    "doc": "Diffusion Models",
    "title": "Diffusion Models",
    "content": "paper notes . ",
    "url": "/docs/paper/diffusion/",
    
    "relUrl": "/docs/paper/diffusion/"
  },"19": {
    "doc": "Paper",
    "title": "Paper",
    "content": "official . Notes and reflections from reading academic papers. This section organizes key points, insights, and personal takeaways to deepen understanding and spark further thinking. ",
    "url": "/docs/paper/",
    
    "relUrl": "/docs/paper/"
  },"20": {
    "doc": "Ideas 2025",
    "title": "10/May/2025",
    "content": ". | 多模态作为我的研究方向和入职后的主要算法工作，是我必须紧跟前沿甚至创造前沿的重要阵地 | 所以打算在博客中增加相关工作的占比，包括论文、项目收集和阅读笔记 | . ",
    "url": "/docs/idea/2025/#10may2025",
    
    "relUrl": "/docs/idea/2025/#10may2025"
  },"21": {
    "doc": "Ideas 2025",
    "title": "09/May/2025",
    "content": ". | 最近的毕业季给我造成了不小的创伤，因为实际的结果和我的预期差别实在太大 | 我以为有多篇论文加顶会在审，有专利，有若干谷歌学术cite，三个创新点都是SOTA的情况下，一定比只有软著专利的同学们强吧，以及应该能拿到类似内审高度评价般的外审结果吧 | 但事实并非如此，思来想去很多次，一度怀疑自己命中和科研无缘。但究其原因，还是自己看得多但写得少；想的多但做的少；宽容他人但忽略了别人不会宽容自己的问题 | 这些事情让我彻底认清自己的缺陷，希望引以为戒 | . ",
    "url": "/docs/idea/2025/#09may2025",
    
    "relUrl": "/docs/idea/2025/#09may2025"
  },"22": {
    "doc": "Ideas 2025",
    "title": "17/Apr/2025",
    "content": ". | 尽管我读了不少关于大模型的论文，也曾在23年初参与了大模型的训练工作，但这个领域发展的太快，导致我对最新的，甚至可以说标准化的大模型模块很生疏 | 因此，我决定从大模型的架构代码入手，了解各模块被采用的深层次原因，并以此发散来回顾和学习近两年大模型的大致情况 | 我将基于开源的项目MiniMind来实现上述想法 | . ",
    "url": "/docs/idea/2025/#17apr2025",
    
    "relUrl": "/docs/idea/2025/#17apr2025"
  },"23": {
    "doc": "Ideas 2025",
    "title": "Ideas 2025",
    "content": "ideas . A Collection of Various Thoughts on 2025 . ",
    "url": "/docs/idea/2025/",
    
    "relUrl": "/docs/idea/2025/"
  }
}
