[{"content":"MUSTIE MUSTIE: Multimodal Structural Transformer for Web Information Extraction. (ACL 2023)\nAbs. \u0026amp; Int. 互联网在过去几十年里呈现出爆炸式的增长，每天都有百万计的网页诞生，这些内容已经成为人们非常重要的信息来源； 如何抽取这些网页信息并结构化对于构建类似网页搜索引擎等应用而言非常重要。而互联网中存在着大量的非结构或者半结构化的信息，例如图片，或者带有网页结构信息的数据，所以如何从繁杂的网页中有效地抽取信息也是非常重要的； 正因为网页里面的非结构化信息是丰富多样的，比如他们包含不同的网络布局结构，存在着文本、图片、表格等多种模态的信息，因此，从网页中抽取信息对工业界和学术界来说都是具有挑战性的； 网页信息抽取，就是希望从网页中抽取对某个对象的相关属性，例如对于某一部电影的网页内容，我们可能希望抽取出电影的名字、导演、演员、类型、上映时间和放映时长等信息； 在对该问题的研究中，人们提出了基于模版的和基于深度学习的方法，在如今语言模型的助力下，人们又将网页表示成类似文本序列或者组成图的方式去处理。近年来，人们开始尝试从文本和视觉信号上提取网页信息的多模态方法； 然而，目前这些方法都存在一些不足： 网页信息是图文并茂地为我们呈现内容，而目前的方法多是对不同的模态用不同的编码器去独立编码，导致模型无法捕获不同模态之间的关联，降低了网页信息的有效性； 他们没有对网页布局和结构进行编码，也就是对DOM树等信息进行编码，从而损失了一些不同领域之间的关联信息（例如在宣传电影的网页中，电影名往往就在图片节点的下面，而电影时长、上映日期等一般是同层次其他节点的信息）； 文本和图片一般只被简单的拼接在一起，使得现有的Transformer模型无法很好地处理大量的网页信息； 为了解决这些问题，文本提出的MUST，就是专门设计了结构化的注意力机制，从多种模态上联合编码DOM树的所有节点，从而学到跨模态的嵌入表示。直观来讲，就是利用网页的布局结构信息，更自然地将各种模态的数据（文本、图像和标签）连接起来，从而更好地计算注意力权重； 本文的主要贡献在于： 提出的多模态结构化Transformer可有效地从网页中建模和提取多模态信息； 专门设计了结构化的注意力机制，从而用来捕获网页中不同模态之间的联系，更好地学习跨模态的嵌入； 在WedSRC和Common Crawl两个数据集上进行测试，得到了SOTA的结果，验证了方法的有效性。 Method. ","date":"2023-08-09T22:34:25+08:00","image":"https://blog.abelcse.cn/p/paper-reading-collection-for-multimodal-information-extraction/mmkg.png","permalink":"https://blog.abelcse.cn/p/paper-reading-collection-for-multimodal-information-extraction/","title":"Paper Reading Collection for Multimodal Information Extraction"},{"content":"ViT An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)\nAbs. \u0026amp; Int. 首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；\n然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；\n本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；\nTransformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；\n因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；\n本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；\n但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA\nModel. 文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \\in R^{H \\times W \\times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\\in R^{N \\times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。\n根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__() # ... self.num_patches = 14 x 14 self.flatten = flatten self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # (B, 3, 14, 14) ==\u0026gt; (B, 768, 14, 14) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1] x = self.proj(x) # (B, 768, 14, 14) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -- BNC (B, 196, 768) x = self.norm(x) return x 通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。\n此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$\n之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。\n接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \\rightarrow (197, 3072) \\rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。\n因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。\n具体流程也可以参考下面的公式：\n注：\n这里的位置编码，原文实验显示，无论使用(1, 2, \u0026hellip;, N)的1D方式，还是(11, 12, 13, \u0026hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；\n考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；\nViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；\n上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；\n位置编码和CLS头可以简单按照下面的方法添加：\n1 2 self.position_embedding = nn.Parameter(torch.zeros(1, 196+1, 768)) self.class_patch = nn.Parameter(torch.zeros(1, 1, 768)) Exp. 数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；\n模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：\n后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；\n比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：\n其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。可以看到，ViT-H/14 要2500个，普通机构是消耗不起的\n但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA; （但是后面也做了实验，实验结果大概是：数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用）\nViT的训练时间也变少了（相对两个baseline来说）\nConclu. ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适； ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的； 混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好） 当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究 CLIP Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)\nAbs. \u0026amp; Int. 先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。\n在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。\n但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？\n事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。\nModel. Dataset. ","date":"2023-07-06T20:25:59+08:00","image":"https://blog.abelcse.cn/p/paper-reading-collection-for-visual-language-models/vit.png","permalink":"https://blog.abelcse.cn/p/paper-reading-collection-for-visual-language-models/","title":"Paper Reading Collection for Visual Language Models"},{"content":"Linux CUDA配置 nvidia-smi和驱动通信失败 当我重启服务器后，输入nvidia-smi命令后，出现报错：\nNVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n这个时候查询nvcc会发现其实驱动相关的东西其实是还在的：\n1 2 3 4 5 6 7 nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 接着查询当前的驱动版本：\n1 2 3 ls /usr/src | grep nvidia # 输入和出现： nvidia-515.43.04 记住上面的数字，自己是多少就是多少：\n输入：\n1 2 3 sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 等待安装完成即可\nLinux网络配置 内网穿透 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。\n之后我们发现了新的，更简单的免费方法，现记录如下：\n拥有外网ip的服务器，假设其公网ip为：x.x.x.x\n需要被映射的内网服务器，假设其内网ip为：y.y.y.y\n假设公网被访问的端口为8888，内网需要被映射的端口为9999，则：\n1 2 3 # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 在公网服务器上输入：\n1 2 curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 之后，继续在公网服务器上输入：\n1 2 3 4 sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888\n端口防火墙 打开某个端口的防火墙\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 查看所有打开的端口\n1 2 3 sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 参考: asimok\u0026rsquo;s blog\n检查是否已经启用ipv6支持\n1 sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则：\n1 2 3 4 5 6 7 8 sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等；\n修改配置文件，添加DNS:\n1 2 3 sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务：\n1 2 sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件：\n1 2 3 sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功：\n1 sudo cat /etc/resolv.conf ipv4地址未出现 注意，以下操作仅在我遇到的问题中可做解决方案，若涉及生产等重要场景，请联系网络和系统管理员协助\n因机房停电维护较长时间，因此再次开机时有两个服务器出现了不同的问题，其中一台输入ifconfig后只有ipv6地址\n此时，输入：\n1 sudo vim /etc/network/interfaces 发现只有lo的地址，也就是local地址，因此可能需要手动配一下ipv4的地址，但这里需要注意两点：\n不一定网卡就叫eth0； 除非机器有申请的固定ip，否则不要直接按照网上给的address netmask gateway修改 因此，首先查看网卡设备名字：\n1 2 3 4 5 6 7 8 9 ip link show # 如果出现eno1，则： ## 还是上面的sudo vim /etc/network/interfaces ## 在这里新增： auto eth0 iface eth0 inet dhcp # 如果出现其他的名字，如enp129s0f0 ## 类似上面的操作，进行dhcp分配即可 但有的人拥有自己固定的IP，不需要DHCP去分配，则：\n1 2 3 4 5 6 7 sudo vim /etc/network/interfaces # 进入后新增： auto eth0 iface eth0 inet static address 你的固定ipv4地址 netmask 255.255.255.0 gateway 192.168.1.1 之后进行重启即可：\n1 2 3 4 5 sudo systemctl restart networking.service # 有时可能依旧会报错，则检查： systemctl status networking.service # 或者直接： sudo systemctl restart NetworkManager.service 开机进入紧急模式 同上，另一台机器开机后，出现: welcome to emergency mode，这大概率是因为写入的自动挂载脚本问题导致的：\n输入：\n1 vim /etc/fstab 查看一下当初挂载了哪些磁盘，尤其是有的磁盘UUID可能会发生变化，从而导致自检不通过\n个人做法：\n返回命令行，输入:\n1 df -h 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么：\n1 2 3 4 # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 Linux账户配置 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。\n在具有sudo权限的用户下进行操作；\n设置root账户密码：\n1 passwd root 编辑配置文件：\n1 2 3 4 sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes 重启ssh服务：\n1 systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。\nsudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面；\n查看sudo用户：\n1 2 3 4 5 6 7 8 # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 将用户添加到sudo组：\n1 2 sudo usermod -aG sudo username # 将username替换为用户账户名 将用户从sudo组移除：\n1 sudo deluser username sudo 其他 利用Docker配置私人网盘 首先拉取docker:\n1 docker pull cloudreve/cloudreve 接着创建必要的文件：\n1 2 3 mkdir -vp cloudreve/{uploads,avatar} \\ \u0026amp;\u0026amp; touch cloudreve/conf.ini \\ \u0026amp;\u0026amp; touch cloudreve/cloudreve.db 然后启动docker：\n先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver\n然后配置文件，并启动：\n1 2 3 4 5 6 7 8 9 sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置：\n1 docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org\n常用docker命令：\n1 2 3 4 docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx ","date":"2023-05-11T09:23:59+08:00","image":"https://blog.abelcse.cn/p/server-management-notes/cover.jpg","permalink":"https://blog.abelcse.cn/p/server-management-notes/","title":"Server Management Notes"},{"content":"W2NER Notes for Paper: Unified Named Entity Recognition as Word-Word Relation Classification\nBackground: 现有的NER问题可以大致分成三种：\n简单实体(flat)，实体的构成比较简单，只识别出实体的开始和结束位置即可； 重叠(嵌套)实体(overlapped)，会出现多个实体包含相同的token的情况； 不连续实体(discontinuous)，实体由位置上不相邻的token构成。 如图1所示，在(a)中的实体$e_1$就是简单实体，而$e_2$则是不连续实体。又因为这两个实体同时出现在同一个句子中，并且有相互重叠的部分，即aching in，因此它们又是重叠的实体。\n现有的NER方法大致可分为四种：\n序列标注方法，简单来讲就是对每个token分配一个标签，以识别每个token在一个实体中扮演的角色。如图1中的$e_1$，我们可以将aching in legs分别标为BIE，以表示他们为开始、中间和结束。虽然这种方式简单直观，但是缺陷也很明显，即出现重叠和非连续实体时，简单的标签就无法完成任务了，并且还需要仔细地设计多种标签，复杂度非常高，也不利于解码；\n基于超图的方法，既然标注法对一个token只能分配一个标签，那么利用节点和边的特点(一个节点可以有多个边)来表示所有的实体span，在一定程度上缓解了标注法的实体嵌套问题，但推理时会受到虚假结构和结构模糊性问题的影响(原文如此说，因为还没读过相关方法的论文，没有理解，暂时划掉)；\n序列生成的方法：既然标注很麻烦，那干脆利用Seq2Seq的方式，直接生成实体，这样会不受嵌套和不连续的影响，但是会受到解码效率和偏差暴露的影响；\n基于span的方法：一般可以列举所有可能的span，然后对span进行分类，但这种方式不仅会受到span的长度限制，还会因为枚举造成大量的资源消耗。\nMotivation： 本文认为，上述方法的核心其实还是在寻找实体的边界，这种思想也许在解决某一个具体问题上有效，但如果想同时解决三种实体识别的问题，也就是建立一个统一的NER模型，那就不能仅仅只看实体的边界了。\n因此，本文认为这种统一模型的主要瓶颈在于如何建模好单词之间的相邻关系，因为只确定边界只是确定了实体的大致范围，至于词之间的关系：是复用的还是不相邻的？需要用其他方式来表示。\n所以本文提出了自己的方法W2NER，该方法主要对词之间的两类关系，准确地说是三类关系进行建模，即：\nNone:无关系； NNW(Next-Neighboring-Word)：下一个邻接词； THW-*:(Tail-Head-Word-*): 头尾词，*表示实体类型 简单来说，THW确定了所有可能的实体边界，NNW确定了实体边界里面的各词之间的关系。如图2所示:\nTHW-S确定了两个S(Symptom)类型的实体范围，(从尾找到头)：aching in legs 和 aching in legs and shoulders； NNW确定了词之间的关系，即当前单词的下一个词是谁，可以看到in和and并没有NNW关系，所以在两个实体范围中，只能解码出:aching in legs和aching in shoulders两个实体，而这样恰好解决了不连续的问题。 这种思想也可以结合图2和图1(b)来直观地感受。\nModel and Experiment: 本文使用了Bert和LSTM作为编码器，利用卷积层提取词之间的表示，最后利用双仿射和多层感知机联合分类出词之间的关系。模型总体结构如图3所示：\n使用卷积层的目的非常直观，因为本文建模的方式就是表的形式，而CNN相对也很适合处理这种结构的表示。在卷积层中，使用了条件层归一化操作(Conditional Layer Normalization)，论文认为这样能够有效产生词对的网格(表)的表示；之后利用类似Bert的方式，增加了词的位置信息和表格的区域信息；最后，使用不同的空洞卷积以捕获不同词距离之间的交互信息。\n在完成上面对表格的表示refine后，论文使用联合的预测器进行最后的token标记分类。因为原文提到先前的工作验证了MLP和biaffine联合使用有利于关系分类。\n具体的实验内容可见原文，这里以ShARe14数据集为例，如下图，可见W2NER模型在重叠和不连续场景长的确都取得了明显的性能提升。\nDecode Strategy: 解码的基本思想是利用词之间的关系来确定词和词之间的路径，文中以四个示例来展示解码的具体操作：\n需要注意的是，图中的下方文字，划线的是具体的实体，大写字母代表了实体中的词。而图中的蓝线表示NNW关系，红色的线表示THW关系。\n有两个实体AB和DE，属于简单实体，因此直接就能解码出来； 有重叠的实体：ABC和BC，但因为ABC和BC均有THW关系，因此也可以解码除了； 有重叠和不连续的实体：ABC和ABD，除了利用THW关系来解决重叠问题外，NNW也从B直接关联到D，从而识别出了不连续的ABD实体； 比较复杂的实体：ACD和BCE，和上面不同的在于，有可能出现ACE和BCD的路径，但是通过THW的限制，使得这两种情况被排除。 Thinking: 并非所有表格表示的东西都适合CNN，如果token的分类和其他token没有太多直接的关系，那么使用CNN不一定会有正向的作用；当然，本文中因为NNW关系本就需要邻近token的信息，所以非常适合，但是核不易太大； 这种方法可以迁移到关系提取(联合提取)上，但是重叠的类型会更多，并且会引入超出实体级别的关系。如果依旧保留NNW这样的关系，可能造成模型学习的负担，并且很容易和其他标签重叠，因此必然需要进一步的修改标签和编码方式。 CoT Notes for Paper: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nBackground 本文主要进行CoT评估的任务分别为：\nArithmetic Reasoning：数学推理，即图1中所示的数学问题； Commonsense Reasoning：常识推理； Symbolic Reasoning：符号推理。例如要求将出现单词的首字母或者尾字母拼接在一起，虽然人类很容易解决该问题，但对模型而言非常具有挑战性。 Motivation 思维链(CoT)在人类思考活动中很常见。当我们思考问题的时候，往往不是直接得到答案，而是将问题分解，然后逐步向正确答案靠近。\n类似地，以CoT方式对模型进行提示，理论上也能得到比较好的结果，这是因为很多数据集在训练时，直接给出问题和答案，然后然后让模型去学习，但是为什么会得到这个答案，模型可能并不了解，而引入CoT后，这些中间步骤会极大丰富模型学会为什么得到此答案的理由。\n如图1所示：\n传统的训练方法中，对于一个问题，直接给出答案，模型难以学到得到这个答案的具体原因；\n而在CoT提示方法中，给出的不仅是答案，而是增加了得到这个答案的中间步骤(也就是思考的过程)，通过这种方式，引导模型在解决类似问题时，也会先生成中间步骤，再得到最终答案，以提高准确性。\n本文发现，单纯增大模型的规模，不足以在一些具有挑战性的任务上提升对应的性能，比如上面提到的三个问题。\n因此，本文通过两个简单的思想，探索了大模型的在不扩大规模的前提下，如何提高模型在这些推理问题上的性能。这两个思想主要是：以前的大量语料和参数量已经给了模型产生中间步骤的能力；通过提示的方式可以进行few-shot学习，而无需微调。\n具体来说，就是人工设置每种任务类型的CoT提示，作为few-shot的学习示例，这里的图2以数学推理为例：\n可以看到，对于标准(传统)提示而言，随着模型规模的增加，性能的确有上升；但要想达到监督模型所得到性能表现还有些困难，并且训练大规模的语言模型，所耗费的资源是很多的。\n而在这些模型上，仅仅通过增加CoT的提示，便有了达到甚至超过监督模型的性能。\n同时也能看出，CoT提示在规模比较大的模型上表现的更好，也许说明了，模型的规模越大，越有利于产生中间结果，越利于进行few-shot学习，再配合上合适的提示，大模型的性能才能被更好地被发挥出来。\nToolformer Notes for Paper: Toolformer: Language Models Can Teach Themselves to Use Tools\nMotivation 大语言模型(LLM)虽然在few-shot和zero-shot方面实现了非常好的提升，并通过参数规模、语料增加而展现了其“涌现”的特点，但这些模型依旧存在一些固有的限制，例如：从最近的事件中获取最新的信息；精确的数学计算；理解低资源语言；缺乏对时间进程的感知等。\n但我们知道在日常生活中，早就有相关的工具能够很好的解决这些问题，那就是各种实用工具，比如搜索引擎、计算器和日历等。如果让大语言模型能够学会如何正确地使用这些工具，而不是寄希望于让他们自己解决所有问题，将极大节省训练的花费。为此，本文提出了Tooformer，以让模型拥有使用外部工具的能力，他们的方法主要有以下几个特点：\n要能以自监督的方式学习，因为大量的人工标注是昂贵的；此外，人类认为有用的信息，对模型而言则不一定，因此让模型自己学习或许更有益； 语言模型不应该失去它的通用性，应该能够自己决定何时、如何使用哪种工具。与现有的方法相比，这使得对工具的使用更加全面，不受特定任务的束缚。 文中的调用方式为：\n分别表示只有调用本身和一个调用包含其结果。下图的示例就是一个调用(c)的工具(a)，输入(i)和结果(r)。\n作者们构建这种使用外部工具的模型的主要方法大致为：\n首先让语言模型自己对大量的数据集按照自己的方式进行可能的API调用标注(因为现有的人工写的好的API调用例子并不多)； 然后，再利用自监督损失来确定哪些API调用切实有助于模型的预测； 最后，利用这些有用的API注释来微调模型。 如图所示：\n首先对于输入文本，先让语言模型利用其上下文学习能力去生成大量可能的API调用示例，再实际去执行这些API调用，然后用空序列调用做对比进行自监督损失以选出更可能有效的API调用，最后再利用这些API进行微调。\n本文主要使用了以下几种工具，利用GPT-J(6B)做微调的模型，实验结果的确有效，很多数据集上甚至比OPT(66B)和GPT3(175B)高得多：1). 问答；2). 计算器；3). 维基百科搜索；4). 机器翻译；5). 日历。\n","date":"2023-05-10T15:39:15+08:00","image":"https://blog.abelcse.cn/p/paper-notes-collection-one/fig3.png","permalink":"https://blog.abelcse.cn/p/paper-notes-collection-one/","title":"Paper Notes Collection One"},{"content":"Important #T000 Attention is All you Need. (#T001 Transformer)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT)\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT)\nAugmenting Reinforcement Learning with Human Feedback. (#T004 RLHF)\nToolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer)\nSWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM)\nRWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV)\nLIMA: Less Is More for Alignment. (#T008 LIMA)\nZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO)\nLLaMA: Open and Efficient Foundation Language Models. (#T010 LLaMA)\nLoRA: Low-Rank Adaptation of Large Language Models. (#T011 LoRA)\nSurvey #S000 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (#S001 Prompt) Multimodal Deep Learning. (#S002 Multimodal) Report #R000 Improving Language Understanding by Generative Pre-Training. (#R001 GPT) Language Models are Unsupervised Multitask Learners. (#R002 GPT2) Language Models are Few-Shot Learners. (#R003 GPT3) Training language models to follow instructions with human feedback. (#R004 InstructGPT) GPT-4 Technical Report. (#R005 GPT4-Report1) Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2) RE #E000 Joint TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker) A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel) A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE) A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE) PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC) OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel) RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN) UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel) Few-shot (#E009 FewRel) FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel) FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)\nFew-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)\nQuery-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)\nRelation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)\nNER #N000 Discontinuous Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER) Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr) Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac) KGE #K000 MRC #M000 MM #C000 An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale. (#C001 ViT) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer) Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. (#C005 NMT) Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF) Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (#C008 ALBEF) Content will be continuously added.\n","date":"2023-05-02T09:46:16+08:00","image":"https://blog.abelcse.cn/p/literature-curation-plan/paper.png","permalink":"https://blog.abelcse.cn/p/literature-curation-plan/","title":"Literature Curation Plan"},{"content":"Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建 Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境：\n1 2 3 4 conda create -n env_name python=3.7 # 自己指定python版本 conda remove -n env_name --all # 如果以后需要删除环境，则可以使用该命令 激活虚拟环境：\n1 2 3 4 5 6 7 conda activate env_name # 或者 source activate env_name # 关闭环境： conda deactivate # 或 source deactivate 安装自己所需要的第三方库：\n1 2 3 4 pip3 install package_name # 或者临时使用清华源 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name # 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样；\n在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH;\n如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK\u0026ndash;\u0026gt;Next:\n如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作\n如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda\u0026ndash;\u0026gt;envs\u0026ndash;\u0026gt;需要的虚拟环境\u0026ndash;\u0026gt;bin\u0026ndash;\u0026gt;python3即可，选择OK和Create，按照提示进入项目中。\n之后，选择Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了；\n类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径；\n如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径；\n完成这些配置后，此时是默认不自动同步的，因此可以进入Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always;\n之后，在PyCharm的右下角，将\u0026lt;no default server\u0026gt;改成上面配置好的SFTP；\n大功告成。\nVSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH；\n在远程资源管理器中的右上角的小齿轮中，输入：\n1 2 3 4 5 Host \u0026lt;远程主机名称\u0026gt; HostName \u0026lt;远程主机IP\u0026gt; User \u0026lt;用户名\u0026gt; Port \u0026lt;ssh端口，默认22\u0026gt; IdentityFile \u0026lt;本机SSH私钥路径\u0026gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到：\n本地：\n1 2 cd ~/.ssh # 复制 id_rsa.pub的内容 服务器：\n1 2 3 4 cd ~/.ssh vim authorized_keys # 然后将刚刚复制的文件粘贴进去 # 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥\n小齿轮还可以再新增其他服务器的或者其他账户的信息；\n需要注意的问题：\n创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter 虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。\n因此，先在虚拟环境中pip install jupyter；\n假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启：\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 之后，初始化jupyter配置：\n产生配置文件：\n1 jupyter notebook --generate-config 设置密码：\n1 jupyter notebook password 复制密钥：\n1 2 3 cd ~/.jupyter vim jupyter_notebook_config.json # 将password的value复制下来 配置端口：\n1 2 vim jupyter_notebook_config.py # 拉到最后 1 2 3 4 c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.password = \u0026#34;刚刚复制的密钥\u0026#34; c.NotebookApp.open_browser = False c.NotebookApp.port = 4399 启动jupyter：\n1 2 3 jupyter notebook # 然后测试一下，例如浏览器输入 http://浏览器ip:4399 # 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里；\n长期挂载：\n1 nohup jupyter notebook \u0026gt; note.log \u0026amp; 则会一直挂在后台，保持运行\n在PyCharm中使用jupyter：\n在项目中新建一个jupyter文件，打开后右上角设置其configuration:\n选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可;\n大功告成！\n","date":"2023-05-01T19:38:22+08:00","image":"https://blog.abelcse.cn/p/deeplearning-environment-setting/jpy.png","permalink":"https://blog.abelcse.cn/p/deeplearning-environment-setting/","title":"DeepLearning Environment Setting"},{"content":"Hexo Demo 此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。\n创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为\u0026quot;xxx.github.io\u0026quot;； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows；\n在Git Bash里面输入：\n1 2 git config --global user.name \u0026#34;your user name\u0026#34; git config --global user.email \u0026#34;your github email\u0026#34; 之后:\n1 2 3 ssh-keygen -t rsa -C \u0026#34;your github email\u0026#34; # 之后按照提示操作即可，或者直接三个回车到底 cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容；\n安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org)\n进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入：\n1 2 3 4 5 npm install hexo-cli -g hexo init your_blog_name cd your_blog_name npm install hexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建\n配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo\n希望创建新的页面，比如about或者links等，输入:\n1 hexo new page \u0026#34;about\u0026#34; 之后在source/about/index.md中进行修改即可\n希望创建新的文章，输入：\n1 hexo new \u0026#34;hello world\u0026#34; 之后在source/_post中修改对应名字的文章即可\n部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为：\n1 2 3 4 deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入：\n1 npm install hexo-deployer-git --save 完成后，分别输入以下命令：\n1 2 3 hexo clean hexo g # 产生静态文件 hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。\n启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包；\n输入以下命令安装包：\n1 npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 mathjax: tags: none # or \u0026#39;ams\u0026#39; or \u0026#39;all\u0026#39; single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。\n1 2 3 4 5 6 --- title: Enable Latex on your article categories: Example date: 1905-06-30 12:00:00 mathjax: true --- 公式示例： 行间公式：\nThis is a in-line latex demo: $\\int^{+\\infty}_{-\\infty}f(x)dx$\n块间公式： $$ { e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$\n安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索；\n考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来；\n假设下载的字体文件为abc.ttf；\n在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl；\n将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下；\n在style.styl中，新增或修改：\n1 2 3 4 5 6 7 8 @font-face{ font-family: \u0026#39;abc\u0026#39;; src: url(\u0026#39;../font/abc.ttf\u0026#39;) } body { font-family: \u0026#39;abc\u0026#39;; } 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。\n然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。\nHexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好；\n在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作；\n首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装：\nnpm install hexo-renderer-marked \u0026ndash;save\n1 2 3 4 5 6 7 8 - 之后，在博客的`_config.yml`中修改和新增： ```bash post_asset_folder: true marked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。\n1 ![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载\u0026ndash;改名\u0026ndash;放入博客theme路径\u0026ndash;修改博客_config中的theme选项\u0026ndash;修改主题文件的配置 这些流程；\n如果需要新建分类：\n1 hexo new page categories 之后进入surce/categories/index.md并修改为：\n1 2 3 4 5 --- title: categories date: 2023-05-01 13:47:40 type: \u0026#34;categories\u0026#34; --- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可\n如果需要新建标签：\n同上，无非是将相关的categories改为tags即可。\n如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上：\n1 \u0026lt;!-- more --\u0026gt; 本地服务器预览\n1 hexo s 新建文章\n1 2 3 4 5 6 hexo n \u0026#34;新文章\u0026#34; # 新建文章 hexo n page new_page # 新建页面 hexo new page --path about/me \u0026#34;About me\u0026#34; # 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于：\nlayout ：page或者post\ntitle：文章标题\ndate：创建日期\nupdated：修改日期\ncomments ：是否开启评论，默认true\ntags：标签名\ncategories：分类名\n产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件)\n1 hexo g 清除缓存\n1 hexo clean 存储草稿\n1 hexo new draft \u0026#34;new draft\u0026#34; 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。\n部署博客\n1 2 3 hexo d hexo clean \u0026amp;\u0026amp; hexo d # 一般可以用组合命令 Hugo Demo 后续若有新增，将会于此补充\n","date":"2023-05-01T15:12:54+08:00","image":"https://blog.abelcse.cn/p/hello-world/cover.jpg","permalink":"https://blog.abelcse.cn/p/hello-world/","title":"Hello World"}]