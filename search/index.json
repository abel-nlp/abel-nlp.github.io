[{"content":"VidOR Annotating Objects and Relations in User-Generated Videos. (ICMR 2019)\nAbs. \u0026amp; Int. ","date":"2023-11-06T10:53:42+08:00","image":"https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/cover.jpg","permalink":"https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/","title":"Paper Notes for Video Visual Relation Detection"},{"content":"ViT An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)\nAbs. \u0026amp; Int. 首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；\n然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；\n本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；\nTransformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；\n因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；\n本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；\n但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA\nModel. 文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \\in R^{H \\times W \\times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\\in R^{N \\times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。\n根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__() # ... self.num_patches = 14 x 14 self.flatten = flatten self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # (B, 3, 14, 14) ==\u0026gt; (B, 768, 14, 14) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1] x = self.proj(x) # (B, 768, 14, 14) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -- BNC (B, 196, 768) x = self.norm(x) return x 通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。\n此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$\n之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。\n接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \\rightarrow (197, 3072) \\rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。\n因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。\n具体流程也可以参考下面的公式：\n注：\n这里的位置编码，原文实验显示，无论使用(1, 2, \u0026hellip;, N)的1D方式，还是(11, 12, 13, \u0026hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；\n考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；\nViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；\n上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；\n位置编码和CLS头可以简单按照下面的方法添加：\n1 2 self.position_embedding = nn.Parameter(torch.zeros(1, 196+1, 768)) self.class_patch = nn.Parameter(torch.zeros(1, 1, 768)) Exp. 数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；\n模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：\n后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；\n比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：\n其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。可以看到，ViT-H/14 要2500个，普通机构是消耗不起的\n但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA; （但是后面也做了实验，实验结果大概是：数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用）\nViT的训练时间也变少了（相对两个baseline来说）\nConclu. ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适； ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的； 混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好） 当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究 CLIP Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)\nAbs. \u0026amp; Int. 先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。\n在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。\n但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？\n事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。\nModel. Dataset. ","date":"2023-07-06T20:25:59+08:00","image":"https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit.png","permalink":"https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/","title":"Paper Notes for Visual Language Models"},{"content":"Linux CUDA配置 nvidia-smi和驱动通信失败 当我重启服务器后，输入nvidia-smi命令后，出现报错：\nNVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n这个时候查询nvcc会发现其实驱动相关的东西其实是还在的：\n1 2 3 4 5 6 7 nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 接着查询当前的驱动版本：\n1 2 3 ls /usr/src | grep nvidia # 输入和出现： nvidia-515.43.04 记住上面的数字，自己是多少就是多少：\n输入：\n1 2 3 sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 等待安装完成即可\nLinux网络配置 内网穿透 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。\n之后我们发现了新的，更简单的免费方法，现记录如下：\n拥有外网ip的服务器，假设其公网ip为：x.x.x.x\n需要被映射的内网服务器，假设其内网ip为：y.y.y.y\n假设公网被访问的端口为8888，内网需要被映射的端口为9999，则：\n1 2 3 # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 在公网服务器上输入：\n1 2 curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 之后，继续在公网服务器上输入：\n1 2 3 4 sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888\n端口防火墙 打开某个端口的防火墙\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 查看所有打开的端口\n1 2 3 sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 参考: asimok\u0026rsquo;s blog\n检查是否已经启用ipv6支持\n1 sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则：\n1 2 3 4 5 6 7 8 sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等；\n修改配置文件，添加DNS:\n1 2 3 sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务：\n1 2 sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件：\n1 2 3 sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功：\n1 sudo cat /etc/resolv.conf ipv4地址未出现 注意，以下操作仅在我遇到的问题中可做解决方案，若涉及生产等重要场景，请联系网络和系统管理员协助\n因机房停电维护较长时间，因此再次开机时有两个服务器出现了不同的问题，其中一台输入ifconfig后只有ipv6地址\n此时，输入：\n1 sudo vim /etc/network/interfaces 发现只有lo的地址，也就是local地址，因此可能需要手动配一下ipv4的地址，但这里需要注意两点：\n不一定网卡就叫eth0； 除非机器有申请的固定ip，否则不要直接按照网上给的address netmask gateway修改 因此，首先查看网卡设备名字：\n1 2 3 4 5 6 7 8 9 ip link show # 如果出现eno1，则： ## 还是上面的sudo vim /etc/network/interfaces ## 在这里新增： auto eth0 iface eth0 inet dhcp # 如果出现其他的名字，如enp129s0f0 ## 类似上面的操作，进行dhcp分配即可 但有的人拥有自己固定的IP，不需要DHCP去分配，则：\n1 2 3 4 5 6 7 sudo vim /etc/network/interfaces # 进入后新增： auto eth0 iface eth0 inet static address 你的固定ipv4地址 netmask 255.255.255.0 gateway 192.168.1.1 之后进行重启即可：\n1 2 3 4 5 sudo systemctl restart networking.service # 有时可能依旧会报错，则检查： systemctl status networking.service # 或者直接： sudo systemctl restart NetworkManager.service 开机进入紧急模式 同上，另一台机器开机后，出现: welcome to emergency mode，这大概率是因为写入的自动挂载脚本问题导致的：\n输入：\n1 vim /etc/fstab 查看一下当初挂载了哪些磁盘，尤其是有的磁盘UUID可能会发生变化，从而导致自检不通过\n个人做法：\n返回命令行，输入:\n1 df -h 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么：\n1 2 3 4 # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 Linux账户配置 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。\n在具有sudo权限的用户下进行操作；\n设置root账户密码：\n1 passwd root 编辑配置文件：\n1 2 3 4 sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes 重启ssh服务：\n1 systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。\nsudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面；\n查看sudo用户：\n1 2 3 4 5 6 7 8 # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 将用户添加到sudo组：\n1 2 sudo usermod -aG sudo username # 将username替换为用户账户名 将用户从sudo组移除：\n1 sudo deluser username sudo 其他 利用Docker配置私人网盘 首先拉取docker:\n1 docker pull cloudreve/cloudreve 接着创建必要的文件：\n1 2 3 mkdir -vp cloudreve/{uploads,avatar} \\ \u0026amp;\u0026amp; touch cloudreve/conf.ini \\ \u0026amp;\u0026amp; touch cloudreve/cloudreve.db 然后启动docker：\n先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver\n然后配置文件，并启动：\n1 2 3 4 5 6 7 8 9 sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置：\n1 docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org\n常用docker命令：\n1 2 3 4 docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx ","date":"2023-05-11T09:23:59+08:00","image":"https://blog.abelcse.cn/p/server-management-notes/cover.jpg","permalink":"https://blog.abelcse.cn/p/server-management-notes/","title":"Server Management Notes"},{"content":"Important #T000 Attention is All you Need. (#T001 Transformer) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT) Augmenting Reinforcement Learning with Human Feedback. (#T004 RLHF) Toolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer) SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM) RWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV) LIMA: Less Is More for Alignment. (#T008 LIMA) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO) LLaMA: Open and Efficient Foundation Language Models. (#T010 LLaMA) LoRA: Low-Rank Adaptation of Large Language Models. (#T011 LoRA) RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.(#T012 RLAIF) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training. (#T013 ZeRO++) Survey #S000 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (#S001 Prompt) Multimodal Deep Learning. (#S002 Multimodal) Multi-Modal Knowledge Graph Construction and Application: A Survey. (#S003 MMKG) A Survey of Large Language Models. (#S004 LLM survey) Report #R000 Improving Language Understanding by Generative Pre-Training. (#R001 GPT) Language Models are Unsupervised Multitask Learners. (#R002 GPT2) Language Models are Few-Shot Learners. (#R003 GPT3) Training language models to follow instructions with human feedback. (#R004 InstructGPT) GPT-4 Technical Report. (#R005 GPT4-Report1) Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2) RE #E000 Joint TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker) A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel) A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE) A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE) PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC) OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel) RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN) UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel) Few-shot (#E009 FewRel) FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel) FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)\nFew-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)\nQuery-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)\nRelation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)\nNER #N000 Discontinuous Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER) Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr) Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac) KGE #K000 MRC #M000 MM #C000 An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale. (#C001 ViT) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer) Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. (#C005 NMT) Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF) Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (#C008 ALBEF) MUSTIE: Multimodal Structural Transformer for Web Information Extraction. (#C009 MUSTIE) FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (#C010 FormNetv2) Content will be continuously added.\n","date":"2023-05-02T09:46:16+08:00","image":"https://blog.abelcse.cn/p/literature-curation-plan/paper.png","permalink":"https://blog.abelcse.cn/p/literature-curation-plan/","title":"Literature Curation Plan"},{"content":"Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建 Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境：\n1 2 3 4 conda create -n env_name python=3.7 # 自己指定python版本 conda remove -n env_name --all # 如果以后需要删除环境，则可以使用该命令 激活虚拟环境：\n1 2 3 4 5 6 7 conda activate env_name # 或者 source activate env_name # 关闭环境： conda deactivate # 或 source deactivate 安装自己所需要的第三方库：\n1 2 3 4 pip3 install package_name # 或者临时使用清华源 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name # 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样；\n在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH;\n如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK\u0026ndash;\u0026gt;Next:\n如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作\n如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda\u0026ndash;\u0026gt;envs\u0026ndash;\u0026gt;需要的虚拟环境\u0026ndash;\u0026gt;bin\u0026ndash;\u0026gt;python3即可，选择OK和Create，按照提示进入项目中。\n之后，选择Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了；\n类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径；\n如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径；\n完成这些配置后，此时是默认不自动同步的，因此可以进入Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always;\n之后，在PyCharm的右下角，将\u0026lt;no default server\u0026gt;改成上面配置好的SFTP；\n大功告成。\nVSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH；\n在远程资源管理器中的右上角的小齿轮中，输入：\n1 2 3 4 5 Host \u0026lt;远程主机名称\u0026gt; HostName \u0026lt;远程主机IP\u0026gt; User \u0026lt;用户名\u0026gt; Port \u0026lt;ssh端口，默认22\u0026gt; IdentityFile \u0026lt;本机SSH私钥路径\u0026gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到：\n本地：\n1 2 cd ~/.ssh # 复制 id_rsa.pub的内容 服务器：\n1 2 3 4 cd ~/.ssh vim authorized_keys # 然后将刚刚复制的文件粘贴进去 # 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥\n小齿轮还可以再新增其他服务器的或者其他账户的信息；\n需要注意的问题：\n创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter 虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。\n因此，先在虚拟环境中pip install jupyter；\n假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启：\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 之后，初始化jupyter配置：\n产生配置文件：\n1 jupyter notebook --generate-config 设置密码：\n1 jupyter notebook password 复制密钥：\n1 2 3 cd ~/.jupyter vim jupyter_notebook_config.json # 将password的value复制下来 配置端口：\n1 2 vim jupyter_notebook_config.py # 拉到最后 1 2 3 4 c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.password = \u0026#34;刚刚复制的密钥\u0026#34; c.NotebookApp.open_browser = False c.NotebookApp.port = 4399 启动jupyter：\n1 2 3 jupyter notebook # 然后测试一下，例如浏览器输入 http://浏览器ip:4399 # 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里；\n长期挂载：\n1 nohup jupyter notebook \u0026gt; note.log \u0026amp; 则会一直挂在后台，保持运行\n在PyCharm中使用jupyter：\n在项目中新建一个jupyter文件，打开后右上角设置其configuration:\n选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可;\n大功告成！\n","date":"2023-05-01T19:38:22+08:00","image":"https://blog.abelcse.cn/p/deeplearning-environment-setting/jpy.png","permalink":"https://blog.abelcse.cn/p/deeplearning-environment-setting/","title":"DeepLearning Environment Setting"},{"content":"Hexo Demo 此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。\n创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为\u0026quot;xxx.github.io\u0026quot;； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows；\n在Git Bash里面输入：\n1 2 git config --global user.name \u0026#34;your user name\u0026#34; git config --global user.email \u0026#34;your github email\u0026#34; 之后:\n1 2 3 ssh-keygen -t rsa -C \u0026#34;your github email\u0026#34; # 之后按照提示操作即可，或者直接三个回车到底 cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容；\n安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org)\n进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入：\n1 2 3 4 5 npm install hexo-cli -g hexo init your_blog_name cd your_blog_name npm install hexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建\n配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo\n希望创建新的页面，比如about或者links等，输入:\n1 hexo new page \u0026#34;about\u0026#34; 之后在source/about/index.md中进行修改即可\n希望创建新的文章，输入：\n1 hexo new \u0026#34;hello world\u0026#34; 之后在source/_post中修改对应名字的文章即可\n部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为：\n1 2 3 4 deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入：\n1 npm install hexo-deployer-git --save 完成后，分别输入以下命令：\n1 2 3 hexo clean hexo g # 产生静态文件 hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。\n启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包；\n输入以下命令安装包：\n1 npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 mathjax: tags: none # or \u0026#39;ams\u0026#39; or \u0026#39;all\u0026#39; single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。\n1 2 3 4 5 6 --- title: Enable Latex on your article categories: Example date: 1905-06-30 12:00:00 mathjax: true --- 公式示例： 行间公式：\nThis is a in-line latex demo: $\\int^{+\\infty}_{-\\infty}f(x)dx$\n块间公式： $$ { e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$\n安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索；\n考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来；\n假设下载的字体文件为abc.ttf；\n在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl；\n将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下；\n在style.styl中，新增或修改：\n1 2 3 4 5 6 7 8 @font-face{ font-family: \u0026#39;abc\u0026#39;; src: url(\u0026#39;../font/abc.ttf\u0026#39;) } body { font-family: \u0026#39;abc\u0026#39;; } 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。\n然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。\nHexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好；\n在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作；\n首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装：\nnpm install hexo-renderer-marked \u0026ndash;save\n1 2 3 4 5 6 7 8 - 之后，在博客的`_config.yml`中修改和新增： ```bash post_asset_folder: true marked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。\n1 ![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载\u0026ndash;改名\u0026ndash;放入博客theme路径\u0026ndash;修改博客_config中的theme选项\u0026ndash;修改主题文件的配置 这些流程；\n如果需要新建分类：\n1 hexo new page categories 之后进入surce/categories/index.md并修改为：\n1 2 3 4 5 --- title: categories date: 2023-05-01 13:47:40 type: \u0026#34;categories\u0026#34; --- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可\n如果需要新建标签：\n同上，无非是将相关的categories改为tags即可。\n如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上：\n1 \u0026lt;!-- more --\u0026gt; 本地服务器预览\n1 hexo s 新建文章\n1 2 3 4 5 6 hexo n \u0026#34;新文章\u0026#34; # 新建文章 hexo n page new_page # 新建页面 hexo new page --path about/me \u0026#34;About me\u0026#34; # 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于：\nlayout ：page或者post\ntitle：文章标题\ndate：创建日期\nupdated：修改日期\ncomments ：是否开启评论，默认true\ntags：标签名\ncategories：分类名\n产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件)\n1 hexo g 清除缓存\n1 hexo clean 存储草稿\n1 hexo new draft \u0026#34;new draft\u0026#34; 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。\n部署博客\n1 2 3 hexo d hexo clean \u0026amp;\u0026amp; hexo d # 一般可以用组合命令 Hugo Demo 后续若有新增，将会于此补充\n","date":"2023-05-01T15:12:54+08:00","image":"https://blog.abelcse.cn/p/hello-world/cover.jpg","permalink":"https://blog.abelcse.cn/p/hello-world/","title":"Hello World"}]