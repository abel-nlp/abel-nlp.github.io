[{"content":"LLaVA Visual Instruction Tuning. NeurIPS 2023.\n背景与动机 在本文之前，无论是传统意义上的“大”模型，还是现在提及的大（语言）模型（LLM），都已经有了广泛的研究。\n例如多模态学习（MM）通过结合计算机视觉和自然语言处理知识在诸如图像-文本检索（Image-Text Retrieval）、图像描述（Image Captioning）和视觉问答（Visual Question Answering）等。\n在LLM出现之后，人们发现现有模型已经具备可观的指令遵循的能力（给定指令后，让LLM能够按照指令的描述完成特定任务），例如训练指令遵循的Agent去完成一系列任务和指令微调后让LLM能够在特定领域按照人类的要求完成任务。\n然而，结合LLM和MM的研究相对还较少，一是因为目前的LLM还是以文本为核心的模型，如何将视觉信息嵌入进语言模型中还有较大的研究空间；二是视觉语言指令微调数据集（Vision-Language）还较缺乏，因为与CLIP这种图像-文本对的收集方式不同，要得到可信的指令微调数据集是有难度的。\n为此，本文的工作可以简单总结为以下两个：\n通过ChatGPT和GPT4构建了一个具备一定规模的多模态指令微调数据集。具体构建方式见后文，但可以预想的是，文本之所以选择这两个模型辅助数据集的构建，一是纯人工的构建必然花费大量的成本，二是这两个模型经过很好的人工反馈强化学习（如InstructGPT）的训练，给出的回复一定程度上更贴近人类的偏好。 利用CLIP作为视觉特征的编码器，利用一个包含大量ChatGPT回复的数据集上微调的LLaMA模型Vicuna作为LLM，从而形成了一个多模态大模型：LLaVA。 数据集构建 LLaVA数据集构建的核心是利用GPT辅助生成视觉指令数据。\n传统的多模态数据缺乏指令相关的内容，以文本内容较为丰富的image caption相关的数据集为例，其数据构成一般是$X_v: X_c$，既对于每一个图像样本$X_v$而言，都有对应地描述图像内容的文本$X_c$。\n在本文中，为了得到指令数据集，作者则利用GPT-4，针对上述提到的$X_c$，让GPT提出可能的问题，也就是假设已经有答案的情况下，让GPT问出符合该答案的问题。假设问题的表示为$X_q$，那么就可以创造出如下格式的指令数据： $X_qX_v$\u0026lt;STOP\u0026gt; Assistant: $X_c$\u0026lt;STOP\u0026gt;。具体的设计方式如图1所示，即给定GPT要求后，收集其产生的回复，从而拼接成固定格式的指令数据。\n然而，仅仅使用这种方式产生的指令数据不仅缺乏多样性，还无法提供更复杂和深层次的逻辑推理信息。因此，本文以人工标注的具备高质量目标检测和描述信息的多模态数据集COCO为基础，充分使用数据集中的描述信息captions和目标位置信息bounding boxes来完成多模态指令数据的生成。\n具体来说，本文希望GPT基于这两类注释和对应的图像，生成三类核心指令数据：\n对话数据：设计一个助手和人对话的场景，产生出助手在看图片，并回答人提出的问题，其实就是我们直观能够理解的多模态对话场景； 详细描述：为了包含对图像全面和丰富的描述，本文创建了和该类问题相关的一系列问题，并通过询问GPT来产生详细的回复（简单理解就是让GPT对captions进行扩充） 复杂推理：上面两类数据重点在描述图像内容本身，为了促进模型的推理能力，本文还创建了很多具备推理要求的问题，从而获得推理指令数据（其实也就是让GPT根据问题生成带推理信息的回复） 基于COCO两类注释得到三类指令数据的大致方式可以见图2：\n简单来讲，本文就是基于COCO的图像描述和目标位置信息，针对对话、描述和推理三类任务，人工先设计了多个对应的问题（作为few-shot）来询问GPT，从而让GPT完成对应的回复，以及自行产生问题和回复。\n最后，本文收集了158K的不同的语言-图像指令数据集，其中包括58K的对话数据，23K的详细描述数据和77K的复杂推理数据。本文还发现，使用GPT-4产生的数据质量比ChatGPT要好，比如空间推理问题。\n任务与方法 本文要解决的任务就是在给定图像的情况下，模型能够根据用户给出的文本指令，准确地进行回复。当然并非必须问和图像相关的问题，毕竟模型核心还是语言模型，但该任务重点强调的是在用户询问和图像内容相关的问题时，模型可以理解并生成对应的回复。这在传统的语言模型、多模态模型和纯文本的LLM上是难以做到的。\n模型结构 LLM本身不是问题，因为现有的开源大模型以及在不同数据集上微调过的大模型都种类繁多，因此本文模型结构的设计重点就在于如何将图像信息提供给大模型。本文使用Vicuna为LLM，既图3中的$\\oint$，而图像编码是CLIP（使用ViT-L/14）。\n总的步骤是：对于提供的图像$X_v$，使用CLIP编码成为$Z_v = CLIP(X_v)$，之后再使用一个简单的线性层将其投影到和文本特征$H_q\t$一致的维度上，即：$H_v = W\\cdot Z_v$，之后将两者拼接起来输入给大模型$H = [H_v;H_q]$。\n需要注意的是，图像特征并没有直接使用CLIP的输出，而是分别验证了CLIP最后一层Transformer Layer之前和之后的特征；以及所谓线性层也确实非常简单，就是一个维度的变换而已。\n模型训练 为了有效地完成多模态的训练，论文采取了两阶段的训练策略。在正式训练之前，对于每张图像$X_v$，会以多轮对话的形式产生训练数据：$(X_q^1, X_a^1, \u0026hellip;, X_q^T, X_a^T)$，$T$是总的对话轮次数。简单来说，就是对每个多轮对话的指令$X_{inst}^t$而言，如果是多轮对话中的第一轮，就随机从$[X_q^1, X_v]$或者$[X_v, X_q^1]$中选一个当第一个样本，而之后的轮次则选择依次选择$X_q^t$。看起来略微有点复杂，其实就是刚提问时的指令为问题+图像的组合，而后的指令就是普通问题，根据图4可知，就是针对某个图像做的多次问答而已。\n图中的绿色部分的token是用来计算LM损失的，其中\u0026lt;STOP\u0026gt;在实际的训练中以###代替。\n有了基本的训练格式，重点就是LLaVA的两阶段训练的详细步骤了：\n第一阶段——特征对齐预训练：\n如果直接进行端到端的训练，理论上会因为图像编码器、全连接层和LLM之间的特征差距导致整个模型无法有效收敛。为此，先将LLM和视觉编码冻住，也就是这个阶段只训练全连接层。\n训练时，从CC3M数据中筛选了595K的图像-文本对数据，并按照单轮对话的方式，以上面一样的GPT产生指令数据的方式让图像-文本对数据变成指令-回答形式的数据。\n接着单轮对话的指令数据输入，然后根据LLM的生成情况来更新全连接层的梯度，从而完成特征对齐预训练过程。注意这里用的CC3M数据集不是上文提到的COCO数据集，与之相比，CC3M的文本描述更多的是来自网络，虽然质量不如人工标注，但是具备更好的多样性特定。\n第二阶段——端到端微调：\n该阶段依旧不更新视觉编码特征，而是对LLM和全连接层进行端到端的微调。训练时，根据三种不同回复类型的特点（对话、描述和推理），只有对话是多轮的，而其他两个还是按照单轮对话的方式给数据。既然有单轮和多轮的训练方式，不难想到LLaVA其实是以多模态的一个聊天机器人的形式提供的。 此外，为了证明模型和方法的有效性，论文还在ScienceQA数据集上做了评估。ScienceQA本身就是一个具备图文信息的科学问答数据集。 结果分析 模型在8张A100显卡上进行训练，超参数设置和Vicuna(v0)一致。在第一阶段中，设置为epoch=1, lr=2e-3, batch_size=128，第二阶段微调时，epoch=3, lr=2e-5, batch_size=32。具体评估分为两类：多模态对话和ScienceQA。\n多模态对话：\n","date":"2024-06-15T21:36:11+08:00","image":"https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/cover.jpg","permalink":"https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/","title":"Multimodal Large Language Models: LLaVA Series"},{"content":"MNRE MNRE: A CHALLENGE MULTIMODAL DATASET FOR NEURAL RELATION EXTRACTION WITH VISUAL EVIDENCE IN SOCIAL MEDIA POSTS. (ICME 2021) (后续简称为 MNRE)\nMultimodal Relation Extraction with Efficient Graph Alignment. (ACM MM 2021) (后续简称为 MEGA)\n背景与动机 关系抽取任务，就是给定文本和实体后，抽取出实体之间的关系。\nMNRE这篇论文认为，现有的关系抽取任务都主要基于新闻类的数据，这些数据组织良好，形式正规，相对来说更容易建模。\n但对于社交媒体类的数据，这些方法会出现较大的性能下降。这是因为社交媒体的文本内容本身就可能不完整，人们往往会配合图片一起发文，所以在进行关系抽取时，还需要额外对图像的内容进行理解。\n如图1-1所示，对于文本JFK and Obama at Harvard @Harvard，可以得到三个实体：JFK、Obama和Harvard，如果不考虑图像信息，可能会错误地得到\u0026lt;JFK, Residence（住在）, Harvard\u0026gt;和\u0026lt;JFK, Spouse（配偶）, Obama\u0026gt;这些关系。\n但如果同时根据图像信息进行理解，就会得到\u0026lt;JFK, Graduated at, Harvard\u0026gt;，因为JFK戴着学位帽，穿着学位服，因此根据文本信息，更容易判断出他毕业于哈佛。\n为此，MNRE提出了一个新的任务：多模态关系抽取（MRE，或者MMRE），并为此构建了数据集MNRE（Multimodal dataset for Neural Relation Extraction），该数据集主要由Twitter-15和Twitter-17构建而来，因此是偏向于社交媒体关系抽取的。作者认为，该工作既推动了多模态关系抽取任务的发展，也为对齐较细粒度的文本-图像信息做出了贡献。\n第2篇论文是同一批作者，主要是对该数据集做了一些改进，对新数据集做了几个baseline，并验证了自己的方法，具体内容将在后续介绍。\n数据集构建 数据集来源：两个多模态实体识别数据集：Twitter-15和Twitter-17，以及部分从Twitter上爬取的内容（2019年1月-2月的内容）；\n数据集涵盖：没有特意挑选具体的领域，比如运动、社会事件，而是尽可能确保实体的多样性，并且去除了非英语以及实体数量低于两个的句子；\n标注方法：源数据、预训练好的NER标注工具、人工标注；\n具体情况见表1-1所示，其中\n第2篇论文对此做了改进，新的数据信息见表1-2和图1-2。由于后续的大量研究都是基于新数据集的，所以MNRE论文后续的方法和内容将不再介绍，而是以第2篇论文MEGA为主。\n图1-2则是对修改后的数据集的关系类别进行了统计，可以看到关系类别的标签还是存在很明显的不平衡问题。\n任务与方法 任务定义非常简单，与文本不同的在于，给定的输入还多了与文本相关联的图片：\n$F(e_1, e_2, S, V) \\rightarrow Y$，既给定句子$S = (w_1, w_2, \u0026hellip;, w_n)$，句子中的实体$e_1, e_2$，以及与句子相关联的视觉内容$V = (v_1, v_2, \u0026hellip;, v_n)$，然后预测出实体$e_1,e_2$之间的关系类别$Y$。（注意，虽然定义中给出的是视觉内容集合，但实际数据集中基本上是一个句子对应一张图像，即使有多个关联情况，数据集也是将其划分为多个样本的）\n为了构建baseline以做对比，MNRE论文分别测试了三类方法：基于CNN的方法Glove+CNN、基于预训练语言模型的方法BertNER、远程监督的方法PCNN，此外，论文还将CNN与PLM结合了一下，即Bert+CNN的方法。而MEGA保留了Glove+CNN和PCNN的方法，并有专门面向关系抽的预训练Bert模型MTB (Matching the Blanks)、BERT结合场景图的方法Bert+SG、以及额外增加了注意力机制的方法Bert+SG+Att。\nMEGA的方法如图1-3所示：\n根据方法图，MEGA的方法大致可以分为三部分：结构化特征表示、多模态特征对齐和实体表示与关系预测。\n结构化特征表示：\n文本结构：依存句法树，MEGA使用ELMo提取给定文本得句法，构建出对应的句法树；\n图像结构：场景图生成(SGG)，MEGA使用场景图生成模型得到针对图像的graph结构信息（场景图就是从图像中抽取出的以 object-relation-object结构为主的Graph）\n从模型结构中可以看到，句法树其实就是文本的图结构，场景图就是图像中物体关系的结构，并且该方法额外还是用了文本的表示，也就是$BERT(Text)$；\n多模态特征对齐：\n结构对齐：即对句法树和场景图进行对齐，通过邻接矩阵构建出各实体和目标之间的映射关系；\n语义对齐：将场景图的信息和从BERT中出来的文本特征进行融合，简单来说就是一个注意力机制，其中Query和Value是文本特征，Key是图像特征；\n关系预测：\n这部分就是图1-3中的最右边的操作，简单来说，就是将之前结构、语义对齐的视觉特征融合，得到最终对齐的视觉语义特征；以及从BERT中出来的文本特征中得到头、尾实体的特征，将他们做个简单的拼接操作，也就是实体$E_{entity} = [head;tail]$，与对齐的视觉特征$E_{visual}$ 做拼接$z = [E_{entity};E_{visual}]$，然后直接全连接预测关系$output = MLP(z)$\n结果分析 MEGA的实验结果如表1-3所示：\n可以看到MEGA是有效的。\n直观上来看，MEGA处理多模态信息时，分别从结构对齐和语义对齐两个方向出发，并进行模态融合，这是常见且有效的一种方法。\n然而，这种方法并非端到端的，场景图生成是有错误的，句法分析也是有错误的，除非MNRE数据集同时有这两个模型的强监督信号，并且在反向传播时也能更新他们的梯度，否则会由于错误积累导致后面模型性能存在不可能太高的上限。\n实际上，哪怕直接利用BERT+ViT的方式进行端到端学习，也能得到比当前结果高的多的性能，甚至不使用视觉信号，BERT也能发挥很好的效果，因此一定程度上需要验证该方法对齐方式的有效性（这也符合对错误累积问题的分析）\n此外，其实数据集本身还存在问题，这些将在后续的论文更新中总结。\nMORE MORE: A Multimodal Object-Entity Relation Extraction Dataset with a Benchmark Evaluation. (ACM MM 2023)\n背景与动机 前面MNRE数据集率先提出了多模态关系抽取任务（MRE｜MMRE），其任务目标是给定文本、对应图片和文本中的实体对后，预测出实体对的关系信息。该任务定义有几个比较强的假设，一是图像-文本是匹配的，也就是图像信息能帮助对应文本的内容理解，二是实体都出现在文本中，虽然不排除图像信息有与之对应的内容，但必须确保文本中含有两个及以上的实体。\n对于第一点，对多模态任务而言是非常合理的，但对数据集的质量有要求。\n针对第二点，本文则认为相对来说是不合理的，多模态的关系三元组构成元素很多都来自于不同的模态，比如某个实体来源于文本，而另一个则是图像中的物体。为此，本文就设计了新的多模态关系抽取任务形式，即给定文本和对应图像，需要对来源于 文本中的实体 和 图像中的物体 所构成的Object-Entity Pair预测关系，具体如图2-1所示。\n也就是说，给定文本和对应图像后，还会从文本中给定若干实体，以及从图像中给定若干物体，最后要求模型对两两的实体-物体组合（Object-Entity Pair）预测出他们之间的关系。为此，本文按照这种任务的形式构建了新的数据集MORE（Multimodal Object-Entity Relation Extraction）\n数据集构建 数据集来源：与MNRE数据集从社交媒体推文中收集不同，该数据集从更正规的新闻媒体（The New York Times English和Yahoo News）中收集，因此数据集的质量相对更有保障，而且图-文匹配的程度更高。\nMORE的数据集构建分为三个步骤：\n利用AllenNLP NER工具对文本中的实体进行抽取；利用YoLo V5对图像中的物体进行抽取；随后由人工对抽取的实体与物体进行筛选和过滤； 让人工标注者对实体-物体之间的关系进行标注，没有关系的数据标注为none关系，当然还有很多分组等确保标注质量、偏见和差距的方法，这里就不提了； 关系过滤，也就是确保只有需要同时根据文本和图像才能得到的关系（过滤掉单独从文本或者单独从图像就能推测出关系的样本） 最终的数据集概要如表2-1:\n可以看到该数据集也与MNRE数据集做了对比，相比之下，MORE的样本数量变少了，但是关系三元组变多了，并且因为新增的Object任务模式（平均每张图有3.8个object，此外，平均每个文本1.5个entity），MORE数据集更难。进一步地，MORE数据集有22.2%的关系Fact是只包含1个实体和1个物体的，而77.8%的关系Fact含有多个实体和物体，所以对MMRE模型有更大的挑战性。\n在关系类别的分布上，大致涵盖了生活、位置等领域，但也呈现标签分布不均衡的问题，具体如图2-2所示。\n任务与方法 与MNRE的任务定义类似，MORE定义的任务形式为：$F = (e, o, S, V) \\rightarrow R$，其中$e \\in S, o \\in V$。\n因为是23年的论文，因此除了较为简单的baseline外，本文还用了一些VLP的模型，后续会列出其性能的比较。\n本文提出的方法为MOREformer，模型结构如图2-3所示。\n论文提到自己的方法主要是基于MKGformer而设计的，因此先介绍MKGformer的基本方法：其使用的文本编码器是BERT，图像编码器是ViT，对于得到的特征，需要进行融合：\n根据公式可以看到，对于文本和图像的特征表示$H^{M_t}, H^{M_v}$，将会被放到前缀指导交互模块（PGI, Prefix Guided Interaction）中，从而得到下一层的特征$\\bar{H}_l^{M_t}, \\bar{H}_l^{M_v}$，而新的两个特征又会放到相关性融合模块（CAF，Correlation-Aware Fusion）中进行融合。\n那么，PGI到底是干什么的？其实就是个注意力机制，而且是个专门处理[CLS] token的模块。对于文本特征而言，就是自注意力机制$M_t[CLS] = Attn(Q_t, K_t, V_t)$，但视觉的前缀则是修改后的注意力，大致为$M_v[CLS] = Attn(Q_v, [K_v, K_t], [V_v, V_t])$，也就是Key和Value是图像与文本的特征拼接，IFAformer也是这种融合的注意力机制。通过PGI，将加强文本[CLS]对自己全局信息的概括能力，以及图像[CLS]对文本-图像全局信息的概括能力。\n而CAF模块则负责模态的融合，具体细节后续整理MKGformer时再列出，大致可以认为该模块就是让文本的token和图像的patch进行交互，最后再送入FFN中。\n本文以上述方法为基础，主要设计了三个模块：\n属性相关的文本编码器（Attribute-Aware Textual Encoder）：这里不需要看公式，根据图2-3可知，就是一个图像描述生成任务的利用，先利用ClipCap生成图像的文本描述，再将该文本描述作为样本中的文本的辅助信息。这样做也是非常常见的一种方法，因为生成的文本本身就是对图像大致内容的描述，分析该文本有助于模型更好地理解图像信息（当然Image Caption模型的好坏会制约生成文本的作用）；具体使用时，本方法则是将整个描述文本当作object来用。也就是说：对于文本$S$和其中的实体$e$，得到BERT分词后的token序列，并将图像生成的描述$caption$拼接上去，之后就是正常通过BERT得到嵌入表示了，即：\n深度相关的视觉编码器（Depth-Aware Visual Encoder）：图像的深度信息能过表示图中各物体的层次结构，本文利用S2RDepthNet得到图像的深度信息，简而言之就是将ViT得到RGB-3通道的patch和深度网络模型得到的深度表示拼接在一起，形成具有深度信息的图像表示；\n位置融合的多模态编码器（Position-Fused Multimodal Encoder）：这里的步骤比较多，简单来讲就是既要利用图像表示、文本表示，还要使用目标的位置信息（因为抽取的图像是标注出来的，因此数据集中含有目标框bbox信息），从而在最后进行MLP关系预测时，能即考虑文本、图像的特征，还考虑实体、图像层次和目标位置的信息，最终做出预测。\n结果分析 本文在实验上做的还是比较充分的，主要的实验结果以及针对本文方法的消融实验分别如图2-4和图2-5所示。\n主实验就不说了，主要看看消融实验，其中P、A和D分别指位置融合、属性（描述文本）和深度信息，按照其结果来看，位置信息最重要，其次是对图像的文本描述，最后是深度信息。这说明文本模块（BERT）发挥了更大的作用，而图像模块（ViT）需要更细致的信息才能起作用。\n其次，和MNRE那里类似，本文也用了很多属于pipeline的模块，例如深度信息生成和图像描述生成，这些模型都没接受梯度更新，因此他们的错误会累积到后续的模块中去。\n","date":"2024-03-25T15:37:28+08:00","image":"https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/cover.jpg","permalink":"https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/","title":"Paper Notes for Multimodal Relation Extraction"},{"content":"ViT An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)\nAbs. \u0026amp; Int. 首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；\n然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；\n本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；\nTransformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；\n因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；\n本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；\n但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA\nModel. 文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \\in R^{H \\times W \\times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\\in R^{N \\times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。\n根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__() # ... self.num_patches = 14 x 14 self.flatten = flatten self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # (B, 3, 14, 14) ==\u0026gt; (B, 768, 14, 14) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1] x = self.proj(x) # (B, 768, 14, 14) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -- BNC (B, 196, 768) x = self.norm(x) return x 通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。\n此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$\n之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。\n接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \\rightarrow (197, 3072) \\rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。\n因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。\n具体流程也可以参考下面的公式：\n注：\n这里的位置编码，原文实验显示，无论使用(1, 2, \u0026hellip;, N)的1D方式，还是(11, 12, 13, \u0026hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；\n考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；\nViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；\n上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；\n位置编码和CLS头可以简单按照下面的方法添加：\n1 2 self.position_embedding = nn.Parameter(torch.zeros(1, 196+1, 768)) self.class_patch = nn.Parameter(torch.zeros(1, 1, 768)) Exp. 数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；\n模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：\n后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；\n比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：\n其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。可以看到，ViT-H/14 要2500个，普通机构是消耗不起的\n但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA; （但是后面也做了实验，实验结果大概是：数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用）\nViT的训练时间也变少了（相对两个baseline来说）\nConclu. ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适； ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的； 混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好） 当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究 CLIP Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)\nAbs. \u0026amp; Int. 先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。\n在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。\n但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？\n事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。\nModel. Dataset. ","date":"2023-07-06T20:25:59+08:00","image":"https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit.png","permalink":"https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/","title":"Paper Notes for Visual Language Models"},{"content":"Linux CUDA配置 nvidia-smi和驱动通信失败 当我重启服务器后，输入nvidia-smi命令后，出现报错：\nNVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n这个时候查询nvcc会发现其实驱动相关的东西其实是还在的：\n1 2 3 4 5 6 7 nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 接着查询当前的驱动版本：\n1 2 3 ls /usr/src | grep nvidia # 输入和出现： nvidia-515.43.04 记住上面的数字，自己是多少就是多少：\n输入：\n1 2 3 sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 等待安装完成即可\nLinux网络配置 内网穿透 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。\n之后我们发现了新的，更简单的免费方法，现记录如下：\n拥有外网ip的服务器，假设其公网ip为：x.x.x.x\n需要被映射的内网服务器，假设其内网ip为：y.y.y.y\n假设公网被访问的端口为8888，内网需要被映射的端口为9999，则：\n1 2 3 # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 在公网服务器上输入：\n1 2 curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 之后，继续在公网服务器上输入：\n1 2 3 4 sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888\n端口防火墙 打开某个端口的防火墙\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 查看所有打开的端口\n1 2 3 sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 参考: asimok\u0026rsquo;s blog\n检查是否已经启用ipv6支持\n1 sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则：\n1 2 3 4 5 6 7 8 sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等；\n修改配置文件，添加DNS:\n1 2 3 sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务：\n1 2 sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件：\n1 2 3 sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功：\n1 sudo cat /etc/resolv.conf ipv4地址未出现 注意，以下操作仅在我遇到的问题中可做解决方案，若涉及生产等重要场景，请联系网络和系统管理员协助\n因机房停电维护较长时间，因此再次开机时有两个服务器出现了不同的问题，其中一台输入ifconfig后只有ipv6地址\n此时，输入：\n1 sudo vim /etc/network/interfaces 发现只有lo的地址，也就是local地址，因此可能需要手动配一下ipv4的地址，但这里需要注意两点：\n不一定网卡就叫eth0； 除非机器有申请的固定ip，否则不要直接按照网上给的address netmask gateway修改 因此，首先查看网卡设备名字：\n1 2 3 4 5 6 7 8 9 ip link show # 如果出现eno1，则： ## 还是上面的sudo vim /etc/network/interfaces ## 在这里新增： auto eth0 iface eth0 inet dhcp # 如果出现其他的名字，如enp129s0f0 ## 类似上面的操作，进行dhcp分配即可 但有的人拥有自己固定的IP，不需要DHCP去分配，则：\n1 2 3 4 5 6 7 sudo vim /etc/network/interfaces # 进入后新增： auto eth0 iface eth0 inet static address 你的固定ipv4地址 netmask 255.255.255.0 gateway 192.168.1.1 之后进行重启即可：\n1 2 3 4 5 sudo systemctl restart networking.service # 有时可能依旧会报错，则检查： systemctl status networking.service # 或者直接： sudo systemctl restart NetworkManager.service 开机进入紧急模式 同上，另一台机器开机后，出现: welcome to emergency mode，这大概率是因为写入的自动挂载脚本问题导致的：\n输入：\n1 vim /etc/fstab 查看一下当初挂载了哪些磁盘，尤其是有的磁盘UUID可能会发生变化，从而导致自检不通过\n个人做法：\n返回命令行，输入:\n1 df -h 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么：\n1 2 3 4 # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 Linux账户配置 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。\n在具有sudo权限的用户下进行操作；\n设置root账户密码：\n1 passwd root 编辑配置文件：\n1 2 3 4 sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes 重启ssh服务：\n1 systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。\nsudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面；\n查看sudo用户：\n1 2 3 4 5 6 7 8 # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 将用户添加到sudo组：\n1 2 sudo usermod -aG sudo username # 将username替换为用户账户名 将用户从sudo组移除：\n1 sudo deluser username sudo 禁止用户登录：\n1 2 3 sudo passwd -l username # or sudo usermod -L username 恢复用户登录：\n1 2 3 sudo passwd -u username # or sudo usermod -U username 其他 利用Docker配置私人网盘 首先拉取docker:\n1 docker pull cloudreve/cloudreve 接着创建必要的文件：\n1 2 3 mkdir -vp cloudreve/{uploads,avatar} \\ \u0026amp;\u0026amp; touch cloudreve/conf.ini \\ \u0026amp;\u0026amp; touch cloudreve/cloudreve.db 然后启动docker：\n先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver\n然后配置文件，并启动：\n1 2 3 4 5 6 7 8 9 sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置：\n1 docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org\n常用docker命令：\n1 2 3 4 docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx ","date":"2023-05-11T09:23:59+08:00","image":"https://blog.abelcode.tech/p/server-management-notes/cover.jpg","permalink":"https://blog.abelcode.tech/p/server-management-notes/","title":"Server Management Notes"},{"content":"Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建 Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境：\n1 2 3 4 conda create -n env_name python=3.7 # 自己指定python版本 conda remove -n env_name --all # 如果以后需要删除环境，则可以使用该命令 激活虚拟环境：\n1 2 3 4 5 6 7 conda activate env_name # 或者 source activate env_name # 关闭环境： conda deactivate # 或 source deactivate 安装自己所需要的第三方库：\n1 2 3 4 pip3 install package_name # 或者临时使用清华源 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name # 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样；\n在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH;\n如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK\u0026ndash;\u0026gt;Next:\n如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作\n如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda\u0026ndash;\u0026gt;envs\u0026ndash;\u0026gt;需要的虚拟环境\u0026ndash;\u0026gt;bin\u0026ndash;\u0026gt;python3即可，选择OK和Create，按照提示进入项目中。\n之后，选择Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了；\n类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径；\n如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径；\n完成这些配置后，此时是默认不自动同步的，因此可以进入Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always;\n之后，在PyCharm的右下角，将\u0026lt;no default server\u0026gt;改成上面配置好的SFTP；\n大功告成。\nVSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH；\n在远程资源管理器中的右上角的小齿轮中，输入：\n1 2 3 4 5 Host \u0026lt;远程主机名称\u0026gt; HostName \u0026lt;远程主机IP\u0026gt; User \u0026lt;用户名\u0026gt; Port \u0026lt;ssh端口，默认22\u0026gt; IdentityFile \u0026lt;本机SSH私钥路径\u0026gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到：\n本地：\n1 2 cd ~/.ssh # 复制 id_rsa.pub的内容 服务器：\n1 2 3 4 cd ~/.ssh vim authorized_keys # 然后将刚刚复制的文件粘贴进去 # 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥\n小齿轮还可以再新增其他服务器的或者其他账户的信息；\n需要注意的问题：\n创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter 虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。\n因此，先在虚拟环境中pip install jupyter；\n假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启：\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 之后，初始化jupyter配置：\n产生配置文件：\n1 jupyter notebook --generate-config 设置密码：\n1 jupyter notebook password 复制密钥：\n1 2 3 cd ~/.jupyter vim jupyter_notebook_config.json # 将password的value复制下来 配置端口：\n1 2 vim jupyter_notebook_config.py # 拉到最后 1 2 3 4 c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.password = \u0026#34;刚刚复制的密钥\u0026#34; c.NotebookApp.open_browser = False c.NotebookApp.port = 4399 启动jupyter：\n1 2 3 jupyter notebook # 然后测试一下，例如浏览器输入 http://浏览器ip:4399 # 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里；\n长期挂载：\n1 nohup jupyter notebook \u0026gt; note.log \u0026amp; 则会一直挂在后台，保持运行\n在PyCharm中使用jupyter：\n在项目中新建一个jupyter文件，打开后右上角设置其configuration:\n选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可;\n大功告成！\n","date":"2023-05-01T19:38:22+08:00","image":"https://blog.abelcode.tech/p/deeplearning-environment-setting/jpy.png","permalink":"https://blog.abelcode.tech/p/deeplearning-environment-setting/","title":"DeepLearning Environment Setting"},{"content":"Hexo Demo 此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。\n创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为\u0026quot;xxx.github.io\u0026quot;； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows；\n在Git Bash里面输入：\n1 2 git config --global user.name \u0026#34;your user name\u0026#34; git config --global user.email \u0026#34;your github email\u0026#34; 之后:\n1 2 3 ssh-keygen -t rsa -C \u0026#34;your github email\u0026#34; # 之后按照提示操作即可，或者直接三个回车到底 cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容；\n安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org)\n进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入：\n1 2 3 4 5 npm install hexo-cli -g hexo init your_blog_name cd your_blog_name npm install hexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建\n配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo\n希望创建新的页面，比如about或者links等，输入:\n1 hexo new page \u0026#34;about\u0026#34; 之后在source/about/index.md中进行修改即可\n希望创建新的文章，输入：\n1 hexo new \u0026#34;hello world\u0026#34; 之后在source/_post中修改对应名字的文章即可\n部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为：\n1 2 3 4 deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入：\n1 npm install hexo-deployer-git --save 完成后，分别输入以下命令：\n1 2 3 hexo clean hexo g # 产生静态文件 hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。\n启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包；\n输入以下命令安装包：\n1 npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 mathjax: tags: none # or \u0026#39;ams\u0026#39; or \u0026#39;all\u0026#39; single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。\n1 2 3 4 5 6 --- title: Enable Latex on your article categories: Example date: 1905-06-30 12:00:00 mathjax: true --- 公式示例： 行间公式：\nThis is a in-line latex demo: $\\int^{+\\infty}_{-\\infty}f(x)dx$\n块间公式： $$ { e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$\n安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索；\n考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来；\n假设下载的字体文件为abc.ttf；\n在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl；\n将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下；\n在style.styl中，新增或修改：\n1 2 3 4 5 6 7 8 @font-face{ font-family: \u0026#39;abc\u0026#39;; src: url(\u0026#39;../font/abc.ttf\u0026#39;) } body { font-family: \u0026#39;abc\u0026#39;; } 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。\n然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。\nHexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好；\n在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作；\n首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装：\nnpm install hexo-renderer-marked \u0026ndash;save\n1 2 3 4 5 6 7 8 - 之后，在博客的`_config.yml`中修改和新增： ```bash post_asset_folder: true marked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。\n1 ![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载\u0026ndash;改名\u0026ndash;放入博客theme路径\u0026ndash;修改博客_config中的theme选项\u0026ndash;修改主题文件的配置 这些流程；\n如果需要新建分类：\n1 hexo new page categories 之后进入surce/categories/index.md并修改为：\n1 2 3 4 5 --- title: categories date: 2023-05-01 13:47:40 type: \u0026#34;categories\u0026#34; --- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可\n如果需要新建标签：\n同上，无非是将相关的categories改为tags即可。\n如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上：\n1 \u0026lt;!-- more --\u0026gt; 本地服务器预览\n1 hexo s 新建文章\n1 2 3 4 5 6 hexo n \u0026#34;新文章\u0026#34; # 新建文章 hexo n page new_page # 新建页面 hexo new page --path about/me \u0026#34;About me\u0026#34; # 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于：\nlayout ：page或者post\ntitle：文章标题\ndate：创建日期\nupdated：修改日期\ncomments ：是否开启评论，默认true\ntags：标签名\ncategories：分类名\n产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件)\n1 hexo g 清除缓存\n1 hexo clean 存储草稿\n1 hexo new draft \u0026#34;new draft\u0026#34; 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。\n部署博客\n1 2 3 hexo d hexo clean \u0026amp;\u0026amp; hexo d # 一般可以用组合命令 Hugo Demo 后续若有新增，将会于此补充\n","date":"2023-05-01T15:12:54+08:00","image":"https://blog.abelcode.tech/p/hello-world/cover.jpg","permalink":"https://blog.abelcode.tech/p/hello-world/","title":"Hello World"}]