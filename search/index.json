[{"content":"MMKG Multi-Modal Knowledge Graph Construction and Application: A Survey. (2022)\nAbs. \u0026amp; Int. 知识图谱对很多下游任务和真实场景都非常有用，比如文本理解、推荐系统和问答，因此现在已有很多知识图谱库，如Cyc, ConceptNet, WordNet, Freebase, DBPedia等； 然而，现在大多的KG库都用纯符号的方式（比如文本）来呈现，削弱了机器理解和描述真实世界的能力。正如一个人无法理解狗，除非他见过狗或者狗的照片，基于此，很多研究者开始建立符号和物理世界的联系，比如将文本和对应的图像、音频和视频联系起来，从而让机器能够产生类似人的经验以更好地理解世界； 此外，目前很多应用场景都有较强的多模态需求，比如在关系抽取中，像partOf、colorOf等关系，仅仅通过文本的理解很可能无法回答，而结合图像，则能够得到极大提升，比如The keyboard and the screen are parts of a laptop.和A banana is usually yellow or yellowish-green but not blue；再比如，增加了图像得信息，可以让模型产生Donald Trump is making a speech 而不只是简单的A tall man with blond hair is making a speech； 本文主要为多模态知识图谱(MMKG)的基本情况做介绍，包含构建和应用两部分。 简而言之就是在传统KG中增加多模态数据很重要，一是因为他们对文本数据有很好的提升作用，二是因为学、业界对多模态的KG有需求。\nDefine. MMKGs 传统的KG可表示为有向图$G = {E, R, A, V, T_R, T_A}$，其中，$E, R, A, V$分别表示实体集合、关系集合、属性集合和属性对应的值的集合；$T_R = E \\times R \\times E$，即关系三元组的集合，如$(s, p, o) \\in T_R$表示实体$s \\in E$和实体$o \\in E$之间存在一个关系$p \\in R$；而$T_A = E \\times A \\times V$表示属性三元组，如$(s, p, o) \\in T_A$表示实体$s \\in E$有一个属性$p \\in A$，该属性的值为$o \\in V$。\n多模态的KG和传统KG一样，但是其中的部分项不再局限于文本，还可以是图片、音频或者视频等。\n现存的工作主要可以分为两类：\n一种是将多模态数据当作实体的特定属性来对待(A-MMKG)，例如对于某个实体，它的属性hasImage​关联的就是一个具体的值，而这个值是一张图片；\n另一种则是将多模态数据当作实体对待(N-MMKG)，也就说一张图片、一个音频，它本身就已经作为了实体，而不再是某个属性关联下的属性值了，例如Eiffel Tower的关系imageOf​所关联的是Eiffel_Tower.jpg，或者Eiffel_Tower_in_Paris.jpg 可能和Eiffel_Tower.jpg存在关系similar​。\n通过图1可以更清晰地了解这两种分类。\n此外，对于N-MMKG而言，一张图片往往会有多种不同的图像描述，比如灰度直方图描述GHD，方向梯度描述HOG和色彩布局描述CLD等；在表1中提供了两种类别的示例。\nConstruct. 进行MMKG构建时，可以大体分为两类，分别是用标记过的图去匹配符号，和用标记过的符号去匹配图片。\nFrom Images to Symbols 主要通过标注的图片去关联文本，常见于CV领域，如图2所示，其中a是进行简单的分割；b是对不同目标的位置进行标注；c是标记出具有所属关系的目标。\n在这类构建方法中，还可以更细地划分为视觉实体（概念）抽取、视觉关系抽取和视觉事件抽取三类任务。\n视觉实体抽取旨在检测和定位图中的视觉物体，并通过KG中的实体符号进行标注。可以采用比如目标检测的方法（对检测出的物体区域分类到具体的实体上）\n该任务的主要挑战在于：如何有效地在没有大规模、细粒度和标注良好的实体-图片数据集上训练出一个细粒度的抽取模型。因为目前cv虽然有很多数据，但多为粗粒度的数据，无法满足MMKG的需求。\n","date":"2023-09-20T15:54:21+08:00","image":"https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/mmkg.png","permalink":"https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/","title":"Survey Multi-Modal Knowledge Graph"},{"content":"MUSTIE MUSTIE: Multimodal Structural Transformer for Web Information Extraction. (ACL 2023)\nAbs. \u0026amp; Int. 互联网在过去几十年里呈现出爆炸式的增长，每天都有百万计的网页诞生，这些内容已经成为人们非常重要的信息来源； 如何抽取这些网页信息并结构化对于构建类似网页搜索引擎等应用而言非常重要。而互联网中存在着大量的非结构或者半结构化的信息，例如图片，或者带有网页结构信息的数据，所以如何从繁杂的网页中有效地抽取信息也是非常重要的； 正因为网页里面的非结构化信息是丰富多样的，比如他们包含不同的网络布局结构，存在着文本、图片、表格等多种模态的信息，因此，从网页中抽取信息对工业界和学术界来说都是具有挑战性的； 网页信息抽取，就是希望从网页中抽取对某个对象的相关属性，例如对于某一部电影的网页内容，我们可能希望抽取出电影的名字、导演、演员、类型、上映时间和放映时长等信息； 在对该问题的研究中，人们提出了基于模版的和基于深度学习的方法，在如今语言模型的助力下，人们又将网页表示成类似文本序列或者组成图的方式去处理。近年来，人们开始尝试从文本和视觉信号上提取网页信息的多模态方法； 然而，目前这些方法都存在一些不足： 网页信息是图文并茂地为我们呈现内容，而目前的方法多是对不同的模态用不同的编码器去独立编码，导致模型无法捕获不同模态之间的关联，降低了网页信息的有效性； 他们没有对网页布局和结构进行编码，也就是对DOM树等信息进行编码，从而损失了一些不同领域之间的关联信息（例如在宣传电影的网页中，电影名往往就在图片节点的下面，而电影时长、上映日期等一般是同层次其他节点的信息）； 文本和图片一般只被简单的拼接在一起，使得现有的Transformer模型无法很好地处理大量的网页信息； 为了解决这些问题，文本提出的模型MUST，就是专门设计了结构化的注意力机制，从多种模态上联合编码DOM树的所有节点，从而学到跨模态的嵌入表示。直观来讲，就是利用网页的布局结构信息，更自然地将各种模态的数据（文本、图像和标签）连接起来，从而更好地计算注意力权重； 本文的主要贡献在于： 提出的多模态结构化Transformer可有效地从网页中建模和提取多模态信息； 专门设计了结构化的注意力机制，从而用来捕获网页中不同模态之间的联系，更好地学习跨模态的嵌入； 在WedSRC和Common Crawl两个数据集上进行测试，得到了SOTA的结果，验证了方法的有效性。 Method. Overview. Web IE：给定一个目标字段T，需要从相应的网页文件中提取出相关的信息。\n在网页中，信息是以DOM树的结构来布局的，往往含有多种模态，最常见的就是文本和图像，而文本和图片本身就是DOM树的叶子节点。为了编码需要提取的目标，本文在原DOM树中增加了Field这类节点，作为后续要提取的属性。除了文本的表示外，对于图像节点，本文使用了OCR去识别其中的文字信息。具体的结构可图1：\nMUST模型主要由三个部分组成，分别是嵌入层、MUST编码器和提取层，嵌入层主要负责将图像和文本token(后续以TI表示，即Text+Image)初始化为嵌入表示，MUST编码器利用结构化注意力机制将DOM树中的多模态信息进行联合编码，提取层则利用Transformer解码器提取Field字段的答案。\nEmbedding Layer 嵌入层的目的是初始化 DOM节点 和 TI token 的向量表示，本文认为，每个DOM节点都可看作是其子树的一个总结，例如head节点就可被用作文档级别的分类依据。对于DOM节点的表示，主要由三部分构成：node嵌入、type嵌入和tag嵌入；对于TI token的嵌入，则由其文本(或图像)的嵌入和type嵌入构成，如果是文本，则由语言模型提供(like bert)，如果是图像，则由patch当作其嵌入(本文使用的ResNet101的线性层投影)。\n其中，type嵌入用于表明该嵌入是DOM节点还是TI token，tag嵌入表明了DOM节点的HTML标签，比如\u0026lt;div\u0026gt;，\u0026lt;img\u0026gt;等。这些嵌入都是可训练的。\nMUST Encoder 编码器通过堆叠L层完全相同的编码器层来学习多模态的知识。在每个编码器层中，都有四个注意力模式，分别是：结构化注意力，旨在迁移DOM树的结构知识；自下而上的注意力，旨在将TI token的信息传递给DOM节点；自上而下的注意力，旨在将DOM节点的信息传递给TI token；局部注意力，旨在从兄弟节点的TI token中更好地学习上下文嵌入；\nDOM-to-DOM注意力：简单说就是专门计算DOM节点的注意力，对于某个节点，它将分别和它的父节点、子节点和兄弟节点进行计算。\nBottom-UP注意力：考虑到随着TI token增长而带来的计算开销，本文仅计算TI token和与之相关的DOM节点的注意力，也就是说，TI token的信息直接传递给它的父节点，而再继续往上的传递，则通过DOM-to-DOM注意力进行；\nTop-Down注意力：本文在此处的计算方式是让TI token和每个DOM节点都相互连接。（图1可以清晰地看出来）\nLocal注意力：本文仅仅计算同一个叶子DOM节点的两个TI token。\nExtraction Layer 提取层直接使用Transformer的解码器，根据词表产生答案。但在本文的这里，并非简单地使用生成方式，而是额外增加了序列标注方法进行span的提取，以及利用\u0026lt;head\u0026gt;节点来对网页文件进行分类，最终得到解码的损失： $$ L = L_D + \\alpha L_{Seq} + \\beta L_{Cls} $$\nExp. 本文主要使用了两个数据集：\nWebSRC：是被设计用于网页的结构化阅读理解和信息抽取的数据集，包含了6.5K的带HTML格式和图像的网页数据，包含10个领域的信息。本文使用的是带有key-value结构的数据，即3214个网页，71个不同的属性字段，简单说就是key被当作属性字段，而value当作答案；\nCommon Crawl：包含超过30亿的网页数据，本文选了其中的Movies、Events和Products三个领域的数据，并通过schema.org的标注数据，对这三个领域的字段进行了筛选，限制和维护了必须是 英语 和 属性的答案只有一个 的网页数据。\n此外，对这两个数据集，本文还单独进行了span的标注，以对序列标注起作用。\n本文使用32核的TPU v3进行训练，使用EM和F1进行评估标注，并重复10次实验计算其平均指标。\nResult. 实验结果表明，本文的方法实现了SOTA的性能，其主要数据见表1。\n从结果中可以看到：\nGraphIE和FreeDOM效果不好，因为他们仅从文本节点中提取文本信息；SimpDOM增加了XPath的信息，因此性能相比前两个要有提升，剩下的几个方法显式地对HTML进行了建模，因此性能也进一步地增加。\n本文的方法则实现了最好的性能，在EM分数上，MUST相比于WebFormer和MarkupLM分别实现了**2.57%和4.61%**的增长，这主要是因为这两个方法虽然引入了多模态信息，但都是分别独立地对不同模型进行建模和编码，而MUST则是统一地进行编码，更好地融合了多模态和网页结构的信息。\n为了验证方法的有效性，文中还对不同的模态所起的作用进行了验证，如图2所示。\n可以看到没有HTML结构的情况下，模型损失的点最多，这意味着在网页文本中，如果仅仅使用文本和图片数据，效果提升是有限的，而DOM信息则非常重要，这也符合网页信息的特点。\n在图3中，实验还分别验证了不同属性字段的情况，可以看出依旧是HTML结构起的作用最大，此外还可以发现如Name，Description这些字段，视觉信息的帮助并不很大，而像Color这种明显需要用到视觉信息的属性则会受益，特别的，对于Brand这个属性，现实中很多商品的品牌名都会画在图片上，因此OCR也能提供较大帮助。这些结果都证明了结合多模态信息对网页信息提取是有用的。\n因为MUST模型自己修改了编码器中的注意力机制，因此本文还对不同注意力机制的影响进行了验证，也就是在保持局部注意力的情况下，分别单独训练了三个模型。在图4中可以看到，从下自上的注意力起的作用最大，文中认为这是由于该机制需要将TI token的信息传递给DOM节点，这对于维护DOM节点的嵌入非常重要。\nConclu. HTML结构信息对于网页信息抽取任务而言是很重要的； 文本信息和视觉信息对不同场景和特点的属性字段起的作用不尽相同； 将文本信息、视觉信息和网页结构统一进行编码，可以更好地学习多模态的信息，相比于分别进行不同模态的编码要有用； 本文的模型目前只关注于单目标的任务，也就是某个属性字段只有一个答案。 FormNetv2 FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (ACL 2023)\n","date":"2023-08-09T22:34:25+08:00","image":"https://blog.abelcse.cn/p/paper-reading-collection-for-multimodal-information-extraction/cover.jpg","permalink":"https://blog.abelcse.cn/p/paper-reading-collection-for-multimodal-information-extraction/","title":"Paper Reading Collection for Multimodal Information Extraction"},{"content":"ViT An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)\nAbs. \u0026amp; Int. 首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；\n然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；\n本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；\nTransformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；\n因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；\n本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；\n但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA\nModel. 文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \\in R^{H \\times W \\times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\\in R^{N \\times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。\n根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__() # ... self.num_patches = 14 x 14 self.flatten = flatten self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # (B, 3, 14, 14) ==\u0026gt; (B, 768, 14, 14) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1] x = self.proj(x) # (B, 768, 14, 14) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -- BNC (B, 196, 768) x = self.norm(x) return x 通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。\n此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$\n之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。\n接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \\rightarrow (197, 3072) \\rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。\n因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。\n具体流程也可以参考下面的公式：\n注：\n这里的位置编码，原文实验显示，无论使用(1, 2, \u0026hellip;, N)的1D方式，还是(11, 12, 13, \u0026hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；\n考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；\nViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；\n上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；\n位置编码和CLS头可以简单按照下面的方法添加：\n1 2 self.position_embedding = nn.Parameter(torch.zeros(1, 196+1, 768)) self.class_patch = nn.Parameter(torch.zeros(1, 1, 768)) Exp. 数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；\n模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：\n后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；\n比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：\n其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。可以看到，ViT-H/14 要2500个，普通机构是消耗不起的\n但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA; （但是后面也做了实验，实验结果大概是：数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用）\nViT的训练时间也变少了（相对两个baseline来说）\nConclu. ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适； ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的； 混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好） 当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究 CLIP Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)\nAbs. \u0026amp; Int. 先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。\n在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。\n但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？\n事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。\nModel. Dataset. ","date":"2023-07-06T20:25:59+08:00","image":"https://blog.abelcse.cn/p/paper-reading-collection-for-visual-language-models/vit.png","permalink":"https://blog.abelcse.cn/p/paper-reading-collection-for-visual-language-models/","title":"Paper Reading Collection for Visual Language Models"},{"content":"Linux CUDA配置 nvidia-smi和驱动通信失败 当我重启服务器后，输入nvidia-smi命令后，出现报错：\nNVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n这个时候查询nvcc会发现其实驱动相关的东西其实是还在的：\n1 2 3 4 5 6 7 nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 接着查询当前的驱动版本：\n1 2 3 ls /usr/src | grep nvidia # 输入和出现： nvidia-515.43.04 记住上面的数字，自己是多少就是多少：\n输入：\n1 2 3 sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 等待安装完成即可\nLinux网络配置 内网穿透 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。\n之后我们发现了新的，更简单的免费方法，现记录如下：\n拥有外网ip的服务器，假设其公网ip为：x.x.x.x\n需要被映射的内网服务器，假设其内网ip为：y.y.y.y\n假设公网被访问的端口为8888，内网需要被映射的端口为9999，则：\n1 2 3 # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 在公网服务器上输入：\n1 2 curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 之后，继续在公网服务器上输入：\n1 2 3 4 sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888\n端口防火墙 打开某个端口的防火墙\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 查看所有打开的端口\n1 2 3 sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 参考: asimok\u0026rsquo;s blog\n检查是否已经启用ipv6支持\n1 sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则：\n1 2 3 4 5 6 7 8 sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等；\n修改配置文件，添加DNS:\n1 2 3 sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务：\n1 2 sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件：\n1 2 3 sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功：\n1 sudo cat /etc/resolv.conf ipv4地址未出现 注意，以下操作仅在我遇到的问题中可做解决方案，若涉及生产等重要场景，请联系网络和系统管理员协助\n因机房停电维护较长时间，因此再次开机时有两个服务器出现了不同的问题，其中一台输入ifconfig后只有ipv6地址\n此时，输入：\n1 sudo vim /etc/network/interfaces 发现只有lo的地址，也就是local地址，因此可能需要手动配一下ipv4的地址，但这里需要注意两点：\n不一定网卡就叫eth0； 除非机器有申请的固定ip，否则不要直接按照网上给的address netmask gateway修改 因此，首先查看网卡设备名字：\n1 2 3 4 5 6 7 8 9 ip link show # 如果出现eno1，则： ## 还是上面的sudo vim /etc/network/interfaces ## 在这里新增： auto eth0 iface eth0 inet dhcp # 如果出现其他的名字，如enp129s0f0 ## 类似上面的操作，进行dhcp分配即可 但有的人拥有自己固定的IP，不需要DHCP去分配，则：\n1 2 3 4 5 6 7 sudo vim /etc/network/interfaces # 进入后新增： auto eth0 iface eth0 inet static address 你的固定ipv4地址 netmask 255.255.255.0 gateway 192.168.1.1 之后进行重启即可：\n1 2 3 4 5 sudo systemctl restart networking.service # 有时可能依旧会报错，则检查： systemctl status networking.service # 或者直接： sudo systemctl restart NetworkManager.service 开机进入紧急模式 同上，另一台机器开机后，出现: welcome to emergency mode，这大概率是因为写入的自动挂载脚本问题导致的：\n输入：\n1 vim /etc/fstab 查看一下当初挂载了哪些磁盘，尤其是有的磁盘UUID可能会发生变化，从而导致自检不通过\n个人做法：\n返回命令行，输入:\n1 df -h 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么：\n1 2 3 4 # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 Linux账户配置 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。\n在具有sudo权限的用户下进行操作；\n设置root账户密码：\n1 passwd root 编辑配置文件：\n1 2 3 4 sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes 重启ssh服务：\n1 systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。\nsudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面；\n查看sudo用户：\n1 2 3 4 5 6 7 8 # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 将用户添加到sudo组：\n1 2 sudo usermod -aG sudo username # 将username替换为用户账户名 将用户从sudo组移除：\n1 sudo deluser username sudo 其他 利用Docker配置私人网盘 首先拉取docker:\n1 docker pull cloudreve/cloudreve 接着创建必要的文件：\n1 2 3 mkdir -vp cloudreve/{uploads,avatar} \\ \u0026amp;\u0026amp; touch cloudreve/conf.ini \\ \u0026amp;\u0026amp; touch cloudreve/cloudreve.db 然后启动docker：\n先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver\n然后配置文件，并启动：\n1 2 3 4 5 6 7 8 9 sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置：\n1 docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org\n常用docker命令：\n1 2 3 4 docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx ","date":"2023-05-11T09:23:59+08:00","image":"https://blog.abelcse.cn/p/server-management-notes/cover.jpg","permalink":"https://blog.abelcse.cn/p/server-management-notes/","title":"Server Management Notes"},{"content":"Important #T000 Attention is All you Need. (#T001 Transformer)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT)\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT)\nAugmenting Reinforcement Learning with Human Feedback. (#T004 RLHF)\nToolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer)\nSWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM)\nRWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV)\nLIMA: Less Is More for Alignment. (#T008 LIMA)\nZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO)\nLLaMA: Open and Efficient Foundation Language Models. (#T010 LLaMA)\nLoRA: Low-Rank Adaptation of Large Language Models. (#T011 LoRA)\nSurvey #S000 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (#S001 Prompt) Multimodal Deep Learning. (#S002 Multimodal) Multi-Modal Knowledge Graph Construction and Application: A Survey. (#S003 MMKG) A Survey of Large Language Models. (#S004 LLM survey) Report #R000 Improving Language Understanding by Generative Pre-Training. (#R001 GPT) Language Models are Unsupervised Multitask Learners. (#R002 GPT2) Language Models are Few-Shot Learners. (#R003 GPT3) Training language models to follow instructions with human feedback. (#R004 InstructGPT) GPT-4 Technical Report. (#R005 GPT4-Report1) Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2) RE #E000 Joint TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker) A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel) A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE) A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE) PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC) OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel) RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN) UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel) Few-shot (#E009 FewRel) FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel) FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)\nFew-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)\nQuery-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)\nRelation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)\nNER #N000 Discontinuous Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER) Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr) Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac) KGE #K000 MRC #M000 MM #C000 An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale. (#C001 ViT) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer) Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. (#C005 NMT) Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF) Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (#C008 ALBEF) MUSTIE: Multimodal Structural Transformer for Web Information Extraction. (#C009 MUSTIE) FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (#C010 FormNetv2) Content will be continuously added.\n","date":"2023-05-02T09:46:16+08:00","image":"https://blog.abelcse.cn/p/literature-curation-plan/paper.png","permalink":"https://blog.abelcse.cn/p/literature-curation-plan/","title":"Literature Curation Plan"},{"content":"Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建 Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境：\n1 2 3 4 conda create -n env_name python=3.7 # 自己指定python版本 conda remove -n env_name --all # 如果以后需要删除环境，则可以使用该命令 激活虚拟环境：\n1 2 3 4 5 6 7 conda activate env_name # 或者 source activate env_name # 关闭环境： conda deactivate # 或 source deactivate 安装自己所需要的第三方库：\n1 2 3 4 pip3 install package_name # 或者临时使用清华源 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name # 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样；\n在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH;\n如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK\u0026ndash;\u0026gt;Next:\n如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作\n如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda\u0026ndash;\u0026gt;envs\u0026ndash;\u0026gt;需要的虚拟环境\u0026ndash;\u0026gt;bin\u0026ndash;\u0026gt;python3即可，选择OK和Create，按照提示进入项目中。\n之后，选择Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了；\n类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径；\n如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径；\n完成这些配置后，此时是默认不自动同步的，因此可以进入Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always;\n之后，在PyCharm的右下角，将\u0026lt;no default server\u0026gt;改成上面配置好的SFTP；\n大功告成。\nVSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH；\n在远程资源管理器中的右上角的小齿轮中，输入：\n1 2 3 4 5 Host \u0026lt;远程主机名称\u0026gt; HostName \u0026lt;远程主机IP\u0026gt; User \u0026lt;用户名\u0026gt; Port \u0026lt;ssh端口，默认22\u0026gt; IdentityFile \u0026lt;本机SSH私钥路径\u0026gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到：\n本地：\n1 2 cd ~/.ssh # 复制 id_rsa.pub的内容 服务器：\n1 2 3 4 cd ~/.ssh vim authorized_keys # 然后将刚刚复制的文件粘贴进去 # 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥\n小齿轮还可以再新增其他服务器的或者其他账户的信息；\n需要注意的问题：\n创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter 虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。\n因此，先在虚拟环境中pip install jupyter；\n假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启：\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 之后，初始化jupyter配置：\n产生配置文件：\n1 jupyter notebook --generate-config 设置密码：\n1 jupyter notebook password 复制密钥：\n1 2 3 cd ~/.jupyter vim jupyter_notebook_config.json # 将password的value复制下来 配置端口：\n1 2 vim jupyter_notebook_config.py # 拉到最后 1 2 3 4 c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.password = \u0026#34;刚刚复制的密钥\u0026#34; c.NotebookApp.open_browser = False c.NotebookApp.port = 4399 启动jupyter：\n1 2 3 jupyter notebook # 然后测试一下，例如浏览器输入 http://浏览器ip:4399 # 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里；\n长期挂载：\n1 nohup jupyter notebook \u0026gt; note.log \u0026amp; 则会一直挂在后台，保持运行\n在PyCharm中使用jupyter：\n在项目中新建一个jupyter文件，打开后右上角设置其configuration:\n选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可;\n大功告成！\n","date":"2023-05-01T19:38:22+08:00","image":"https://blog.abelcse.cn/p/deeplearning-environment-setting/jpy.png","permalink":"https://blog.abelcse.cn/p/deeplearning-environment-setting/","title":"DeepLearning Environment Setting"},{"content":"Hexo Demo 此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。\n创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为\u0026quot;xxx.github.io\u0026quot;； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows；\n在Git Bash里面输入：\n1 2 git config --global user.name \u0026#34;your user name\u0026#34; git config --global user.email \u0026#34;your github email\u0026#34; 之后:\n1 2 3 ssh-keygen -t rsa -C \u0026#34;your github email\u0026#34; # 之后按照提示操作即可，或者直接三个回车到底 cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容；\n安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org)\n进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入：\n1 2 3 4 5 npm install hexo-cli -g hexo init your_blog_name cd your_blog_name npm install hexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建\n配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo\n希望创建新的页面，比如about或者links等，输入:\n1 hexo new page \u0026#34;about\u0026#34; 之后在source/about/index.md中进行修改即可\n希望创建新的文章，输入：\n1 hexo new \u0026#34;hello world\u0026#34; 之后在source/_post中修改对应名字的文章即可\n部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为：\n1 2 3 4 deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入：\n1 npm install hexo-deployer-git --save 完成后，分别输入以下命令：\n1 2 3 hexo clean hexo g # 产生静态文件 hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。\n启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包；\n输入以下命令安装包：\n1 npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 mathjax: tags: none # or \u0026#39;ams\u0026#39; or \u0026#39;all\u0026#39; single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。\n1 2 3 4 5 6 --- title: Enable Latex on your article categories: Example date: 1905-06-30 12:00:00 mathjax: true --- 公式示例： 行间公式：\nThis is a in-line latex demo: $\\int^{+\\infty}_{-\\infty}f(x)dx$\n块间公式： $$ { e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$\n安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索；\n考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来；\n假设下载的字体文件为abc.ttf；\n在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl；\n将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下；\n在style.styl中，新增或修改：\n1 2 3 4 5 6 7 8 @font-face{ font-family: \u0026#39;abc\u0026#39;; src: url(\u0026#39;../font/abc.ttf\u0026#39;) } body { font-family: \u0026#39;abc\u0026#39;; } 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。\n然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。\nHexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好；\n在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作；\n首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装：\nnpm install hexo-renderer-marked \u0026ndash;save\n1 2 3 4 5 6 7 8 - 之后，在博客的`_config.yml`中修改和新增： ```bash post_asset_folder: true marked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。\n1 ![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载\u0026ndash;改名\u0026ndash;放入博客theme路径\u0026ndash;修改博客_config中的theme选项\u0026ndash;修改主题文件的配置 这些流程；\n如果需要新建分类：\n1 hexo new page categories 之后进入surce/categories/index.md并修改为：\n1 2 3 4 5 --- title: categories date: 2023-05-01 13:47:40 type: \u0026#34;categories\u0026#34; --- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可\n如果需要新建标签：\n同上，无非是将相关的categories改为tags即可。\n如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上：\n1 \u0026lt;!-- more --\u0026gt; 本地服务器预览\n1 hexo s 新建文章\n1 2 3 4 5 6 hexo n \u0026#34;新文章\u0026#34; # 新建文章 hexo n page new_page # 新建页面 hexo new page --path about/me \u0026#34;About me\u0026#34; # 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于：\nlayout ：page或者post\ntitle：文章标题\ndate：创建日期\nupdated：修改日期\ncomments ：是否开启评论，默认true\ntags：标签名\ncategories：分类名\n产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件)\n1 hexo g 清除缓存\n1 hexo clean 存储草稿\n1 hexo new draft \u0026#34;new draft\u0026#34; 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。\n部署博客\n1 2 3 hexo d hexo clean \u0026amp;\u0026amp; hexo d # 一般可以用组合命令 Hugo Demo 后续若有新增，将会于此补充\n","date":"2023-05-01T15:12:54+08:00","image":"https://blog.abelcse.cn/p/hello-world/cover.jpg","permalink":"https://blog.abelcse.cn/p/hello-world/","title":"Hello World"}]