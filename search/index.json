[{"content":"Dataset VidVRD (ImageNet-VidVRD) Video Visual Relation Detection. (ACM MM 2017)\nAbs. \u0026amp; Int. 作为连接视觉和语言的桥梁，以关系三元组\u0026lt;主语，谓语，宾语\u0026gt;为形式的目标间视觉关系提供了超过目标本身的更为全面的视觉内容理解，如\u0026lt;person, touch, dog\u0026gt;和\u0026lt;cat, above, sofa\u0026gt;等。\n本文提出了一种新的视觉任务，叫作Video Visual Relation Detection (VidVRD)，用于在视频而非静态的图片上进行关系检测。与静态图相比，视频为检测视觉关系提供了更自然的一组特征，比如动态的关系如\u0026lt;A, follow, B\u0026gt;, \u0026lt;A, towards, B\u0026gt;（A跟着B，A朝着B），以及类似\u0026lt;A, chase, B\u0026gt;然后\u0026lt;A, hold, B\u0026gt;（A追逐B，然后A抓住B）这种时序关系。然而，VidVRD任务相比于图像关系检测任务ImgVRD，在技术上具有更大的挑战性，因为其很难对准确的目标进行追踪和应付多种多样的关系表现。\n为此，本文提出了一个包含目标轨迹小片段候选、短期关系预测和贪心关系关联的VidVRD方法，并提供了一个用于任务评估的数据集，其中包含1000个人工标注的视频。与以前的相关方法相比，本文在该数据集上实现了最佳性能。\n缩小视觉和语言之间的差距对多媒体分析至关重要，相关工作涉及visual concept annotations, semantic description with captioning 和 visual question-answering、字幕语义描述和视觉问答等。视觉关系检测VRD是最近为了更全面地理解目标之外的视觉内容而做出的工作，目的是为了捕获目标之间的各种互动，它可以有效地支持众多的视觉语言任务，如captioning, visual search 和 visual question-answering等。\n视觉关系包括用边界框定位的一对目标和连接他们之间的谓语，也就是关系。如图1-1(a)所示，其中两个目标可以有不同的关系，同一个关系也可连接起不同外观的目标对。在本文中将使用关系三元组来表示由\u0026lt;subject, predicate, object\u0026gt;构成的视觉关系类型。因为这种组合的可能性非常多，传统的目标检测方法在这个任务中并不适合，即使已经有的几个相关任务的方法也更适用于静态图片而非视频。与静态图片相比，视频为检测视觉关系提供了一组更为自然的特征。图1-1(b)中，从视频的时空内容中提取的运动特征有助于区分相似的关系，如walk 和 run。此外，一些动态的视觉关系只能在视频中被检测到，如\u0026lt;dog, run past, person\u0026gt;和\u0026lt;dog, faster than, person\u0026gt;。因此，与ImgVRD相比，VidVRD是一个更通用更可行的任务。\nVidVRD和ImgVRD之间的另一个显著不同是：视频中的视觉关系通常是随时间变化的，而图像中的关系则是固定的。目标可能会因为遮挡或离开画面而导致视觉关系出现和消失，即使两个目标始终出现在同一个视频帧中，它们之间的互动关系也可能随着时间而发生变化。如图1-2所示，其中的狗和飞盘同时出现在$t_2$和$t_7$之间，但它们之间的关系却从追逐变成了撕咬。因此，应当重新设定义VidVRD任务，以处理视觉关系的可变性。\nDefinition 为了与ImgVRD的定义保持一致，本文将VidVRD任务定义如下：给定一组目标类别$C$和谓语(关系)类别$P$，VidVRD旨在检测出视频中的视觉关系实例$C \\times P \\times C$，其中视觉关系实例由关系三元组\u0026lt;subject, predicate, object\u0026gt;$\\in C \\times P \\times C$表示，并带有主语和宾语的轨迹信息$T_s$和$T_o$，具体来说，$T_s$和$T_o$是两个边界框的序列，在视觉关系的最长持续时间内构成主语和宾语。如图1-2中，给定的视频关系可以用三元组\u0026lt;dog, chasem frisbee\u0026gt;和\u0026lt;dog, bite, frisbee\u0026gt;来表示，狗和飞盘分别用$(t_2, t_4)$和$(t_5, t_7)$之间的红色与绿色轨迹定位。\n与ImgVRD相比，VidVRD面临更多的技术挑战。\n首先，VidVRD需要用边界框轨迹来定位目标，这比在ImgVRD中为每个目标提供一个边界框更加困难，因为轨迹准确性受到每一帧的目标定位及目标跟踪的性能影响。本文所提出的VidVRD方法通过在视频的每个重叠短片段中生成目标轨迹小片段，然后根据预测出的关系将小片段和目标轨迹关联起来。其次，VidVRD需要在最长持续时间中时序地定位视觉关系。为了完成该任务，本文提出了一个贪心关联算法，其通过判断相邻片段中检测到的视觉关系是否具有相同关系三元组，且目标轨迹小片段具有足够高的重叠度来决定是否合并他们。最后，与ImgVRD相比，VidVRD需要预测更多类型的视觉关系，因为有的视觉关系只能在视频中检测到，为了有效预测关系，本文提出了关系预测模型，该模型从主语/宾语的轨迹小片段中提取出多种特征，包括外观特征、运动特征和相对特征。然后将这些特征编码成关系特征，再分别使用单独的主语、宾语和关系预测器来预测视觉关系。\n截至目前，还没有针对VidVRD的数据集，不过有几个针对ImgVRD的数据集，如 Visual Relationship dataset, Visual Genome，因此，本文构建了一个VidVRD的数据集进行评估。本文设计了一个谓语描述机制，并从 ILSVRC2016-VID 中构建该数据集。该数据集包含1000个视频，其中有人工标注的视觉关系和目标边界轨迹信息。\n本文的主要贡献在于：\n提出了新颖的VidVRD任务，旨在探索视频中目标之间的各种关系，与ImgVRD相比，它提供了更可行地VRD任务； 提出了一种新的VidVRD方法，该方法通过目标轨迹小片段候选、关系预测和贪心关系关联来检测视频中的视觉关系； 贡献了第一个VidVRD数据集，带有1000个人工标注的视频数据。 Rel. Video object detection 视频目标检测旨在从给定视频中检测出预定义的目标，并通过边界框轨迹对其进行定位。目前最好的方法主要是通过整合目标检测和多目标跟踪来实现。\n最近，复杂的深度神经网络在图像目标检测方面取得了很好的性能。然而视频中存在模糊、运动和遮挡等情况，阻碍了利用边界框轨迹对目标进行精确定位的操作，因此视频中的目标检测仍然存在准确率低的问题。\n另一方面，由于目标检测器漏检率较高，采用逐个检测追踪策略的多目标追踪往往会产生较短的轨迹，因此有研究开发了额外的合并算法，以获得时间上更一致的目标轨迹。\n为此，本文的方法利用视频目标检测器生成短时的目标小轨迹候选来避免上面两个问题，并且该方案适合所有好的目标检测和多目标追踪方法。\nVisual relation detection 近期的研究工作主要集中于图像的关系检测上，人们普遍认为视觉关系检测的基本挑战在于，如何通过从少量训练实例中学习来建模和预测大量的关系。为解决该问题，现有的大部分方法都是分别预测视觉关系三元组中的主语、宾语和谓语。但目前的ImgVRD方法不适用于VidVRD任务，如其中的动态关系和视频关系的可变性。因此，本文提出了一种视频特定关系特征和一个新的训练准则，用于学习单独的预测模型。\n本文的方法是首次在视频上进行视觉关系检测的尝试，虽然之前的一些工作也和视频视觉关系任务相关，但它们所追求的目标和VidVRD完全不同。\nAction recognition 动作是视觉关系中一种主要的谓词类别，VidVRD可以借鉴动作识别方面的进展，在动作识别中，特征表示在处理大的类别差距、背景干扰和摄像机运动方面起着重要的作用。为了解决这些问题，人们开发了手工特征和深度神经网络的方法。受相关工作的启发，本文将改进的密集轨迹(iDT)作为部分特征，因为iDT在大多数动作识别数据集上取得了出色的性能，尤其是在训练数据不足的情况下。值得注意的是，本文的方法旨在检测目标之间比动作更一般的关系，如空间关系和比较（相对）关系。\nDataset. 本文基于 ILSVRC2016-VID 构建了第一个用于评估VidVRD任务的数据集，包含人工标注边界框的30类目标的视频共1000个，视频中视觉关系清晰丰富，忽略了目标单一，视觉关系模糊的视频。数据集被划分为训练集和测试集，分别包含800个和200个视频。\n此外，还补充了5个经常出现的目标类别，即 person, ball, sofa, skateboard 和 frisbee，因此共有35个目标种类。这些类别都是相对比较独立的，也就是不包括目标之间的分属关系，比如\u0026lt;bicycle, with, wheel\u0026gt;。\n通过对及物动词、比较动词、空间描述和不及物动词等进行筛选和变换，共有132种关系被包含在数据集中。\n具体的数据划分等，见表1-1：\nMethod. VidOR Annotating Objects and Relations in User-Generated Videos. (ICMR 2019)\nAbs. \u0026amp; Int. 要进行细粒度的视频内容分析，理解目标之间的关系必不可少。然而，现存的工作受到数据集规模太小和评价指标不直接的限制。这些限制主要是因为构建大规模详细标注的视频数据集费时费力。因此，本文提出了一种pipeline的视频标注方法，能够以少量的花费构建数据集，并构造了新的视频数据集VidOR，该数据集包含了10k个视频（大概84小时），共80个目标类别和50个关系类别。\n细粒度的视频内容分析很重要，目前一些视频字幕描述和问答任务已经证明了这一点。随着粒度的不断细化，理解目标之间的关系也变得重要，目前的研究表明理解目标之间的关系对视频内容分析生成更稳健的表示是有效的。但是目前相关的工作都是较为隐性地建模和评估目标之间的关系，对模型是否和如何很好理解细粒度内容并不很清楚。\n但是在视频中进行目标和关系的识别是很有挑战性的任务，这需要对目标的外观、身份、动作和目标之间的交互。如图1-1中的狗所示，其外观可能因为它的活动、光照和遮挡而发生较大变化，这对给跨视频帧去确定狗的身份带来较大的困扰，而这却是进一步总结视频内容的必要线索。此外，动作和交互表示的差异也对模型学习根本的模式和稳健的推理带来了新的挑战。为此有很多工作，如video object detection和video visual relation detection在深入地研究这些问题，但他们主要受到了可用数据集规模太小的限制。\n对上面的内容，我们可以总结为：\n细粒度的视频内容分析非常重要，比如生成视频字幕，进行视频问答等； 但是进行细粒度的视频内容分析并不容易，这主要是因为现存的数据集规模小，无法设计更好的模型； 而视频中目标的关系检测是很重要的部分，能够帮助进行视频内容分析。而要进行关系检测，需要考虑目标的外观、身份、动作和目标之间的交互，这是具有挑战性的，甚至连一个这样的大规模数据集都还没有。 因此，本文的目的就是构建一个包含详细标注的目标和关系的更大规模数据集，视频均来源于网上以确保反映真实场景。标注信息通过边界框轨迹对目标进行时空定位，对关系进行时间定位。和类似的图片数据集相比，本文希望构建一个视频的benchmark，并且能够对动作和动态关系进行学习和推理。\n但是构建这样的数据集代价高昂，假设平均一个视频150帧，10k个视频就需要标注1.5m张图片，这已经达到了当前一些图像数据集的规模。\n如果假设相邻帧之间具有相似性，那么就可以在固定的间隔中进行手工标注，然后再进行插值。但这种方式只适合电影及监控视频这种连贯的视频类型。而由用户生成的视频内容则是完全自由的，质量一般不太高，视频的连续性也不强，所以不能简单的进行插值，而需要选择一种更好的策略进行关键帧的选取和人工标注。\n此外，如何将标注任务划分为子任务也是比较复杂的问题。假设我们将标注视频中的目标和关系视为一个单一任务，那么标注者就需要接受相关的培训，花费大量的精力。因此，更好的选择是将标注任务变成更宏观的子任务，如判断目标的类型，确定是否有边框，边框是否正确等等。\n为此，本文将设计一种新的pipeline方法，它适用于长度从几秒到几分钟不等的用户生成的视频。\nRel. Video Annotation 当前的时序标注大致可以分为两类，一类是通过标注目标在视频中的开始和结束帧进行时序的定位，因此需要标注者仔细地浏览整个视频；另一类方法是在较短的视频片段钟标记目标是否存在，然后自动合并各片段的结果以实现时间定位，这种方法相对节省时间，但可能产生粗粒度的标注结果。\n在本文的方法中，通过为不同的关系仔细挑选合适的方法，从而结合了两类方法的优点。\nVideo Datasets 视频数据集按照标注程度来划分。一种是按照整个视频级别来进行标注，如CCV、Kinetics和MSR-VTT，他们都是用于事件识别，动作识别和视频字幕任务的典型benchmark；第二类是片段级别的标注，比如THUMOS、MultiTHUMOS和ActivityNet Caption，他们一般用于动作、活动和事件的时序定位的评测；第三类是目标级别的标注，TrackingNet就是其中一个大规模的标注了目标边框轨迹的目标追踪数据集。ImageNet-VID是一个超过30类目标的目标检测数据集。AVA则是一个时空活动定位的视觉动作数据集；最后则是关系级别的标注，VidVRD是基于ImageNet-VID构建的当前唯一一个提供目标和关系的视频数据集，但是该数据集相对较小，并且标注的比较稀疏。\nStat. Annotation Pipeline 为了更好地进行标注，本文提出了如图1-2所示的包含目标和关系标注的流水线标注方案。为了完成标注任务，标注者需要完成三个子任务：浏览视频以发现新的目标；绘制这些目标的边框；将被遮挡或者离开镜头外的目标的帧标注为不可见。\n多是数据集的描述和统计，后续将简要总结出来。\nVRD-GCN Video Relation Detection with Spatio-Temporal Graph. (ACM MM 2019)\nAbs. \u0026amp; Int. 理解视觉信息是计算机视觉的核心目标。视觉关系检测就是其中很有挑战的一项任务，因为我们从视觉内容中获取的信息不仅仅是目标的集合，还包括它们之间的交互关系，也就是需要捕获细粒度的视觉线索，包括目标的位置以及它们之间如何进行交互。\n视频中目标之间的关系是深入理解动态视觉内容的重要组成部分，它对很多高层次的视觉任务，如视觉问答、视觉字幕生成等有更精确的促进作用，但视频中的关系检测和推理却很少被探索。\n目前已经有很多针对静态图片的关系检测方法，并取得了非常好的结果。一种自然的方法是直接使用它们的方法来进行视频的关系检测，然而由于视频和图像的本质区别：增加了时序维度，使得这些方法在视频任务上并不能获得理想的结果，因为时间通道的视频信息使得很多动态关系要和时序与空间进行关联，这使得在视频中进行关系预测变得更为复杂。\n如果我们能够利用时间和空间信息，如在空间维度上，知道了\u0026lt;bicycle, move_right, car\u0026gt;，就能够帮助检测\u0026lt;person, right, car\u0026gt;，在时间维度上，知道\u0026lt;car, faster, bicycle\u0026gt;，就有助于检测到后续的\u0026lt;car, move_past, bicycle\u0026gt;（如图2-1）。\n本文将视频抽象为全连接的空间-时间图，通过在这种3D图中利用图卷积网络传递信息和推理，从而提出了新的方法：VRD-GCN。简单来说，本文通过空间-时间内容更好地完成动态关系的预测，并利用孪生网络的在线关联方法更准确地完成关系实例\u0026lt;subject, predicate, object\u0026gt;的关联。本文在ImageNet-VidVRD数据集上进行了实验，达到了当时的SOTA。\nMethod. 视频视觉关系检测任务的定义是：给定目标集合$O$，关系（谓语）集合$P$，任意长度的视频，利用主语和宾语的轨迹$T_s, T_o$（即主语和宾语目标在不同帧的目标框序列），需要该任务从中检测出所有\u0026lt;subject, predicate, object\u0026gt;$\\in O \\times P \\times O$的关系实例。如图2-2所示，我们可以将任务分解为三个独立的部分：多目标追踪、关系预测和关系实例关联。\n简单来讲，首先将视频切分为片段，并从片段中提取出轨迹段（Trajectory proposals），然后利用本文的方法VRD-GCN来预测所有目标对的关联；最后，利用孪生网络的在线关联方法来关联所有检测到的短期关系实例，形成最终的视频关系实例。需要注意的是，这里的轨迹信息和预处理方法直接使用Dataset-VidVRD(ImageNet-VidVRD)的结果。\n片段关系预测 直接上来看，空间上相互接近的目标有比较强的关联，并且相邻时间段的目标也回保持强关联。因此，本文将视频抽象成全连接的时空图，参考图2-1所示，其中每个目标被视为连接到前一个、当前和下一个片段中所有其他节点的轨迹节点。而处理该图的方法则选用了图卷积神经网络。\n","date":"2023-11-06T10:53:42+08:00","image":"https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/cover.jpg","permalink":"https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/","title":"Paper Notes for Video Visual Relation Detection"},{"content":"ViT An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)\nAbs. \u0026amp; Int. 首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；\n然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；\n本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；\nTransformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；\n因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；\n本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；\n但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA\nModel. 文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \\in R^{H \\times W \\times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\\in R^{N \\times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。\n根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__() # ... self.num_patches = 14 x 14 self.flatten = flatten self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # (B, 3, 14, 14) ==\u0026gt; (B, 768, 14, 14) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1] x = self.proj(x) # (B, 768, 14, 14) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -- BNC (B, 196, 768) x = self.norm(x) return x 通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。\n此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$\n之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。\n接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \\rightarrow (197, 3072) \\rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。\n因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。\n具体流程也可以参考下面的公式：\n注：\n这里的位置编码，原文实验显示，无论使用(1, 2, \u0026hellip;, N)的1D方式，还是(11, 12, 13, \u0026hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；\n考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；\nViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；\n上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；\n位置编码和CLS头可以简单按照下面的方法添加：\n1 2 self.position_embedding = nn.Parameter(torch.zeros(1, 196+1, 768)) self.class_patch = nn.Parameter(torch.zeros(1, 1, 768)) Exp. 数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；\n模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：\n后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；\n比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：\n其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。可以看到，ViT-H/14 要2500个，普通机构是消耗不起的\n但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA; （但是后面也做了实验，实验结果大概是：数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用）\nViT的训练时间也变少了（相对两个baseline来说）\nConclu. ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适； ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的； 混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好） 当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究 CLIP Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)\nAbs. \u0026amp; Int. 先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。\n在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。\n但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？\n事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。\nModel. Dataset. ","date":"2023-07-06T20:25:59+08:00","image":"https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit.png","permalink":"https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/","title":"Paper Notes for Visual Language Models"},{"content":"Linux CUDA配置 nvidia-smi和驱动通信失败 当我重启服务器后，输入nvidia-smi命令后，出现报错：\nNVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n这个时候查询nvcc会发现其实驱动相关的东西其实是还在的：\n1 2 3 4 5 6 7 nvcc -V # 输入命令后，出现： nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 接着查询当前的驱动版本：\n1 2 3 ls /usr/src | grep nvidia # 输入和出现： nvidia-515.43.04 记住上面的数字，自己是多少就是多少：\n输入：\n1 2 3 sudo apt-get install dkms # then sudo dkms install -m nvidia -v 515.43.04 等待安装完成即可\nLinux网络配置 内网穿透 最近在实验室服务器上搭建了大模型在线体验服务，但仅限校园网用户可以访问，无法为校外用户提供服务，恰好我们有一台腾讯的云服务器，前期我们尝试在云服务器上配置校园网，但校园网的映射比较固定导致不仅没能成功，还停掉了外网的地址，不得不重启服务器。\n之后我们发现了新的，更简单的免费方法，现记录如下：\n拥有外网ip的服务器，假设其公网ip为：x.x.x.x\n需要被映射的内网服务器，假设其内网ip为：y.y.y.y\n假设公网被访问的端口为8888，内网需要被映射的端口为9999，则：\n1 2 3 # 内网服务器输入以下命令，让外网服务器的8888端口可访问内网的9999端口 ssh -o ServerAliveInterval=60 -f -N -R 8888:y.y.y.y:9999 root@x.x.x.x # 回车后需要输入外网的账户密码，注意这里默认账户名是root，请根据实际情况进行修改 在公网服务器上输入：\n1 2 curl http://127.0.0.1:8888 # 一般会出现所映射到端口的信息，例如映射22，则出现SSH相关的信息 之后，继续在公网服务器上输入：\n1 2 3 4 sysctl -w net.ipv4.conf.eth0.route_localnet=1 # 允许127回环转发 iptables -t nat -A PREROUTING -p tcp --dport 8888 -j DNAT --to-destination 127.0.0.1:8888 # 表示让公网服务器允许将8888端口的请求转发到127回路上 最后，按照请求内网服务器一样请求外网服务器即可，如：x.x.x.x:8888\n端口防火墙 打开某个端口的防火墙\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 查看所有打开的端口\n1 2 3 sudo firewall-cmd --zone=public --list-ports # 或者限定端口的开放协议 如tcp sudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 参考: asimok\u0026rsquo;s blog\n检查是否已经启用ipv6支持\n1 sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则：\n1 2 3 4 5 6 7 8 sudo vim /etc/sysctl.conf # 添加以下内容: net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 # 之后，执行： sudo sysctl -p # 检查是否启用： sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等；\n修改配置文件，添加DNS:\n1 2 3 sudo vim /etc/systemd/resolved.conf # 添加DNS，比如： DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务：\n1 2 sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件：\n1 2 3 sudo mv /etc/resolv.conf /etc/resolv.conf.bak # 先将原来的文件备份 sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功：\n1 sudo cat /etc/resolv.conf ipv4地址未出现 注意，以下操作仅在我遇到的问题中可做解决方案，若涉及生产等重要场景，请联系网络和系统管理员协助\n因机房停电维护较长时间，因此再次开机时有两个服务器出现了不同的问题，其中一台输入ifconfig后只有ipv6地址\n此时，输入：\n1 sudo vim /etc/network/interfaces 发现只有lo的地址，也就是local地址，因此可能需要手动配一下ipv4的地址，但这里需要注意两点：\n不一定网卡就叫eth0； 除非机器有申请的固定ip，否则不要直接按照网上给的address netmask gateway修改 因此，首先查看网卡设备名字：\n1 2 3 4 5 6 7 8 9 ip link show # 如果出现eno1，则： ## 还是上面的sudo vim /etc/network/interfaces ## 在这里新增： auto eth0 iface eth0 inet dhcp # 如果出现其他的名字，如enp129s0f0 ## 类似上面的操作，进行dhcp分配即可 但有的人拥有自己固定的IP，不需要DHCP去分配，则：\n1 2 3 4 5 6 7 sudo vim /etc/network/interfaces # 进入后新增： auto eth0 iface eth0 inet static address 你的固定ipv4地址 netmask 255.255.255.0 gateway 192.168.1.1 之后进行重启即可：\n1 2 3 4 5 sudo systemctl restart networking.service # 有时可能依旧会报错，则检查： systemctl status networking.service # 或者直接： sudo systemctl restart NetworkManager.service 开机进入紧急模式 同上，另一台机器开机后，出现: welcome to emergency mode，这大概率是因为写入的自动挂载脚本问题导致的：\n输入：\n1 vim /etc/fstab 查看一下当初挂载了哪些磁盘，尤其是有的磁盘UUID可能会发生变化，从而导致自检不通过\n个人做法：\n返回命令行，输入:\n1 df -h 发现原本写在fstab中的有一项磁盘路径没出现在这里，我这里没出现的磁盘叫作data2，那么：\n1 2 3 4 # 则最简单的办法是：注释或删除 vim /etc/fstab ## 将data2对应的UUID注释掉，返回再重启即可 # 其他办法是：利用lsblk 和 fdisk等命令，查看未挂载磁盘的UUID信息，重新修改，具体请自信检索 Linux账户配置 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。\n在具有sudo权限的用户下进行操作；\n设置root账户密码：\n1 passwd root 编辑配置文件：\n1 2 3 4 sudo vim /etc/ssh/sshd_config # 然后输入以下命令： PermitRootLogin yes PasswordAuthentication yes 重启ssh服务：\n1 systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。\nsudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面；\n查看sudo用户：\n1 2 3 4 5 6 7 8 # 查看sudo用户有哪些 # 先安装一个包 sudo apt-get install members # 再查看 members sudo # 或者在某个用户的终端下输入groups groups # 以查看该用户当前所属的组 将用户添加到sudo组：\n1 2 sudo usermod -aG sudo username # 将username替换为用户账户名 将用户从sudo组移除：\n1 sudo deluser username sudo 禁止用户登录：\n1 2 3 sudo passwd -l username # or sudo usermod -L username 恢复用户登录：\n1 2 3 sudo passwd -u username # or sudo usermod -U username 其他 利用Docker配置私人网盘 首先拉取docker:\n1 docker pull cloudreve/cloudreve 接着创建必要的文件：\n1 2 3 mkdir -vp cloudreve/{uploads,avatar} \\ \u0026amp;\u0026amp; touch cloudreve/conf.ini \\ \u0026amp;\u0026amp; touch cloudreve/cloudreve.db 然后启动docker：\n先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver\n然后配置文件，并启动：\n1 2 3 4 5 6 7 8 9 sudo docker run -d \\ --name docker-image-name \\ -p 5212:5212 \\ --mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\ --mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\ -v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\ -v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\ -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置：\n1 docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org\n常用docker命令：\n1 2 3 4 docker ps # 查看运行中容器 docker stop xxxx docker rm -f xxxx docker restart xxxx ","date":"2023-05-11T09:23:59+08:00","image":"https://blog.abelcse.cn/p/server-management-notes/cover.jpg","permalink":"https://blog.abelcse.cn/p/server-management-notes/","title":"Server Management Notes"},{"content":"Important #T000 Attention is All you Need. (#T001 Transformer) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT) Augmenting Reinforcement Learning with Human Feedback. (#T004 RLHF) Toolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer) SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM) RWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV) LIMA: Less Is More for Alignment. (#T008 LIMA) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO) LLaMA: Open and Efficient Foundation Language Models. (#T010 LLaMA) LoRA: Low-Rank Adaptation of Large Language Models. (#T011 LoRA) RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.(#T012 RLAIF) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training. (#T013 ZeRO++) Survey #S000 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (#S001 Prompt) Multimodal Deep Learning. (#S002 Multimodal) Multi-Modal Knowledge Graph Construction and Application: A Survey. (#S003 MMKG) A Survey of Large Language Models. (#S004 LLM survey) Report #R000 Improving Language Understanding by Generative Pre-Training. (#R001 GPT) Language Models are Unsupervised Multitask Learners. (#R002 GPT2) Language Models are Few-Shot Learners. (#R003 GPT3) Training language models to follow instructions with human feedback. (#R004 InstructGPT) GPT-4 Technical Report. (#R005 GPT4-Report1) Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2) RE #E000 Joint TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker) A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel) A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE) A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE) PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC) OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel) RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN) UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel) Few-shot (#E009 FewRel) FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel) FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)\nFew-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)\nQuery-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)\nRelation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)\nNER #N000 Discontinuous Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER) Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr) Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac) KGE #K000 MRC #M000 MM #C000 An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale. (#C001 ViT) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer) Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. (#C005 NMT) Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF) Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (#C008 ALBEF) MUSTIE: Multimodal Structural Transformer for Web Information Extraction. (#C009 MUSTIE) FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (#C010 FormNetv2) Content will be continuously added.\n","date":"2023-05-02T09:46:16+08:00","image":"https://blog.abelcse.cn/p/literature-curation-plan/paper.png","permalink":"https://blog.abelcse.cn/p/literature-curation-plan/","title":"Literature Curation Plan"},{"content":"Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建 Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境：\n1 2 3 4 conda create -n env_name python=3.7 # 自己指定python版本 conda remove -n env_name --all # 如果以后需要删除环境，则可以使用该命令 激活虚拟环境：\n1 2 3 4 5 6 7 conda activate env_name # 或者 source activate env_name # 关闭环境： conda deactivate # 或 source deactivate 安装自己所需要的第三方库：\n1 2 3 4 pip3 install package_name # 或者临时使用清华源 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name # 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样；\n在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH;\n如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK\u0026ndash;\u0026gt;Next:\n如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作\n如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda\u0026ndash;\u0026gt;envs\u0026ndash;\u0026gt;需要的虚拟环境\u0026ndash;\u0026gt;bin\u0026ndash;\u0026gt;python3即可，选择OK和Create，按照提示进入项目中。\n之后，选择Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了；\n类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径；\n如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径；\n完成这些配置后，此时是默认不自动同步的，因此可以进入Tools\u0026ndash;\u0026gt;Deployment\u0026ndash;\u0026gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always;\n之后，在PyCharm的右下角，将\u0026lt;no default server\u0026gt;改成上面配置好的SFTP；\n大功告成。\nVSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH；\n在远程资源管理器中的右上角的小齿轮中，输入：\n1 2 3 4 5 Host \u0026lt;远程主机名称\u0026gt; HostName \u0026lt;远程主机IP\u0026gt; User \u0026lt;用户名\u0026gt; Port \u0026lt;ssh端口，默认22\u0026gt; IdentityFile \u0026lt;本机SSH私钥路径\u0026gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到：\n本地：\n1 2 cd ~/.ssh # 复制 id_rsa.pub的内容 服务器：\n1 2 3 4 cd ~/.ssh vim authorized_keys # 然后将刚刚复制的文件粘贴进去 # 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥\n小齿轮还可以再新增其他服务器的或者其他账户的信息；\n需要注意的问题：\n创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter 虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。\n因此，先在虚拟环境中pip install jupyter；\n假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启：\n1 2 sudo firewall-cmd --zone=public --add-port=4399/tcp --permanent sudo firewall-cmd --reload 之后，初始化jupyter配置：\n产生配置文件：\n1 jupyter notebook --generate-config 设置密码：\n1 jupyter notebook password 复制密钥：\n1 2 3 cd ~/.jupyter vim jupyter_notebook_config.json # 将password的value复制下来 配置端口：\n1 2 vim jupyter_notebook_config.py # 拉到最后 1 2 3 4 c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.password = \u0026#34;刚刚复制的密钥\u0026#34; c.NotebookApp.open_browser = False c.NotebookApp.port = 4399 启动jupyter：\n1 2 3 jupyter notebook # 然后测试一下，例如浏览器输入 http://浏览器ip:4399 # 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里；\n长期挂载：\n1 nohup jupyter notebook \u0026gt; note.log \u0026amp; 则会一直挂在后台，保持运行\n在PyCharm中使用jupyter：\n在项目中新建一个jupyter文件，打开后右上角设置其configuration:\n选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可;\n大功告成！\n","date":"2023-05-01T19:38:22+08:00","image":"https://blog.abelcse.cn/p/deeplearning-environment-setting/jpy.png","permalink":"https://blog.abelcse.cn/p/deeplearning-environment-setting/","title":"DeepLearning Environment Setting"},{"content":"Hexo Demo 此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。\n创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为\u0026quot;xxx.github.io\u0026quot;； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows；\n在Git Bash里面输入：\n1 2 git config --global user.name \u0026#34;your user name\u0026#34; git config --global user.email \u0026#34;your github email\u0026#34; 之后:\n1 2 3 ssh-keygen -t rsa -C \u0026#34;your github email\u0026#34; # 之后按照提示操作即可，或者直接三个回车到底 cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容；\n安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org)\n进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入：\n1 2 3 4 5 npm install hexo-cli -g hexo init your_blog_name cd your_blog_name npm install hexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建\n配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo\n希望创建新的页面，比如about或者links等，输入:\n1 hexo new page \u0026#34;about\u0026#34; 之后在source/about/index.md中进行修改即可\n希望创建新的文章，输入：\n1 hexo new \u0026#34;hello world\u0026#34; 之后在source/_post中修改对应名字的文章即可\n部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为：\n1 2 3 4 deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入：\n1 npm install hexo-deployer-git --save 完成后，分别输入以下命令：\n1 2 3 hexo clean hexo g # 产生静态文件 hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。\n启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包；\n输入以下命令安装包：\n1 npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 mathjax: tags: none # or \u0026#39;ams\u0026#39; or \u0026#39;all\u0026#39; single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。\n1 2 3 4 5 6 --- title: Enable Latex on your article categories: Example date: 1905-06-30 12:00:00 mathjax: true --- 公式示例： 行间公式：\nThis is a in-line latex demo: $\\int^{+\\infty}_{-\\infty}f(x)dx$\n块间公式： $$ { e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$\n安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索；\n考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来；\n假设下载的字体文件为abc.ttf；\n在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl；\n将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下；\n在style.styl中，新增或修改：\n1 2 3 4 5 6 7 8 @font-face{ font-family: \u0026#39;abc\u0026#39;; src: url(\u0026#39;../font/abc.ttf\u0026#39;) } body { font-family: \u0026#39;abc\u0026#39;; } 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。\n然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。\nHexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好；\n在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作；\n首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装：\nnpm install hexo-renderer-marked \u0026ndash;save\n1 2 3 4 5 6 7 8 - 之后，在博客的`_config.yml`中修改和新增： ```bash post_asset_folder: true marked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。\n1 ![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载\u0026ndash;改名\u0026ndash;放入博客theme路径\u0026ndash;修改博客_config中的theme选项\u0026ndash;修改主题文件的配置 这些流程；\n如果需要新建分类：\n1 hexo new page categories 之后进入surce/categories/index.md并修改为：\n1 2 3 4 5 --- title: categories date: 2023-05-01 13:47:40 type: \u0026#34;categories\u0026#34; --- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可\n如果需要新建标签：\n同上，无非是将相关的categories改为tags即可。\n如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上：\n1 \u0026lt;!-- more --\u0026gt; 本地服务器预览\n1 hexo s 新建文章\n1 2 3 4 5 6 hexo n \u0026#34;新文章\u0026#34; # 新建文章 hexo n page new_page # 新建页面 hexo new page --path about/me \u0026#34;About me\u0026#34; # 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于：\nlayout ：page或者post\ntitle：文章标题\ndate：创建日期\nupdated：修改日期\ncomments ：是否开启评论，默认true\ntags：标签名\ncategories：分类名\n产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件)\n1 hexo g 清除缓存\n1 hexo clean 存储草稿\n1 hexo new draft \u0026#34;new draft\u0026#34; 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。\n部署博客\n1 2 3 hexo d hexo clean \u0026amp;\u0026amp; hexo d # 一般可以用组合命令 Hugo Demo 后续若有新增，将会于此补充\n","date":"2023-05-01T15:12:54+08:00","image":"https://blog.abelcse.cn/p/hello-world/cover.jpg","permalink":"https://blog.abelcse.cn/p/hello-world/","title":"Hello World"}]