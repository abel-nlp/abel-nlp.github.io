<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Survey on abel&#39;s blog</title>
        <link>https://blog.abelcse.cn/categories/survey/</link>
        <description>Recent content in Survey on abel&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Wed, 20 Sep 2023 15:54:21 +0800</lastBuildDate><atom:link href="https://blog.abelcse.cn/categories/survey/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Survey Multi-Modal Knowledge Graph</title>
        <link>https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/</link>
        <pubDate>Wed, 20 Sep 2023 15:54:21 +0800</pubDate>
        
        <guid>https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/</guid>
        <description>&lt;img src="https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/mmkg.png" alt="Featured image of post Survey Multi-Modal Knowledge Graph" /&gt;&lt;h2 id=&#34;mmkg&#34;&gt;MMKG&lt;/h2&gt;
&lt;p&gt;Multi-Modal Knowledge Graph Construction and Application: A Survey.  (2022)&lt;/p&gt;
&lt;h3 id=&#34;abs--int&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;知识图谱对很多下游任务和真实场景都非常有用，比如文本理解、推荐系统和问答，因此现在已有很多知识图谱库，如Cyc, ConceptNet, WordNet, Freebase, DBPedia等；&lt;/li&gt;
&lt;li&gt;然而，现在大多的KG库都用纯符号的方式（比如文本）来呈现，削弱了机器理解和描述真实世界的能力。正如一个人无法理解&lt;code&gt;狗&lt;/code&gt;，除非他见过&lt;code&gt;狗&lt;/code&gt;或者&lt;code&gt;狗的照片&lt;/code&gt;，基于此，很多研究者开始建立符号和物理世界的联系，比如将文本和对应的图像、音频和视频联系起来，从而让机器能够产生类似人的&lt;code&gt;经验&lt;/code&gt;以更好地理解世界；&lt;/li&gt;
&lt;li&gt;此外，目前很多应用场景都有较强的多模态需求，比如在关系抽取中，像&lt;code&gt;partOf&lt;/code&gt;、&lt;code&gt;colorOf&lt;/code&gt;等关系，仅仅通过文本的理解很可能无法回答，而结合图像，则能够得到极大提升，比如&lt;em&gt;The keyboard and the screen are parts of a laptop.&lt;em&gt;和&lt;/em&gt;A banana is usually yellow or yellowish-green but not blue&lt;/em&gt;；再比如，增加了图像得信息，可以让模型产生&lt;em&gt;Donald Trump is making a speech&lt;/em&gt; 而不只是简单的&lt;em&gt;A tall man with blond hair is making a speech&lt;/em&gt;；&lt;/li&gt;
&lt;li&gt;本文主要为多模态知识图谱(MMKG)的基本情况做介绍，包含&lt;strong&gt;构建&lt;/strong&gt;和&lt;strong&gt;应用&lt;/strong&gt;两部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简而言之就是在传统KG中增加多模态数据很重要，一是因为他们对文本数据有很好的提升作用，二是因为学、业界对多模态的KG有需求。&lt;/p&gt;
&lt;h3 id=&#34;define&#34;&gt;Define.&lt;/h3&gt;
&lt;h4 id=&#34;mmkgs&#34;&gt;MMKGs&lt;/h4&gt;
&lt;p&gt;传统的KG可表示为有向图$G = {E, R, A, V, T_R, T_A}$，其中，$E, R, A, V$分别表示实体集合、关系集合、属性集合和属性对应的值的集合；$T_R = E \times R \times E$，即关系三元组的集合，如$(s, p, o) \in T_R$表示实体$s \in E$和实体$o \in E$之间存在一个关系$p \in R$；而$T_A = E \times A \times V$表示属性三元组，如$(s, p, o) \in T_A$表示实体$s \in E$有一个属性$p \in A$，该属性的值为$o \in V$。&lt;/p&gt;
&lt;p&gt;多模态的KG和传统KG一样，但是其中的部分项不再局限于文本，还可以是图片、音频或者视频等。&lt;/p&gt;
&lt;p&gt;现存的工作主要可以分为两类：&lt;/p&gt;
&lt;p&gt;一种是将多模态数据&lt;strong&gt;当作实体的特定属性&lt;/strong&gt;来对待(A-MMKG)，例如对于某个实体，它的属性&lt;em&gt;hasImage&lt;/em&gt;​关联的就是一个具体的值，而这个值是一张图片；&lt;/p&gt;
&lt;p&gt;另一种则是将多模态数据&lt;strong&gt;当作实体&lt;/strong&gt;对待(N-MMKG)，也就说一张图片、一个音频，它本身就已经作为了实体，而不再是某个属性关联下的属性值了，例如&lt;em&gt;Eiffel Tower&lt;/em&gt;的关系&lt;em&gt;imageOf&lt;/em&gt;​所关联的是&lt;em&gt;Eiffel_Tower.jpg&lt;/em&gt;，或者&lt;em&gt;Eiffel_Tower_in_Paris.jpg&lt;/em&gt; 可能和&lt;em&gt;Eiffel_Tower.jpg&lt;/em&gt;存在关系&lt;em&gt;similar​&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;通过图1可以更清晰地了解这两种分类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/mmkg.png&#34;
	width=&#34;1611&#34;
	height=&#34;559&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;288&#34;
		data-flex-basis=&#34;691px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;此外，对于N-MMKG而言，一张图片往往会有多种不同的图像描述，比如灰度直方图描述GHD，方向梯度描述HOG和色彩布局描述CLD等；在表1中提供了两种类别的示例。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/mmkg_t1.png&#34;
	width=&#34;1539&#34;
	height=&#34;320&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;480&#34;
		data-flex-basis=&#34;1154px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;construct&#34;&gt;Construct.&lt;/h3&gt;
&lt;p&gt;进行MMKG构建时，可以大体分为两类，分别是用标记过的图去匹配符号，和用标记过的符号去匹配图片。&lt;/p&gt;
&lt;h4 id=&#34;from-images-to-symbols&#34;&gt;From Images to Symbols&lt;/h4&gt;
&lt;p&gt;主要通过标注的图片去关联文本，常见于CV领域，如图2所示，其中a是进行简单的分割；b是对不同目标的位置进行标注；c是标记出具有所属关系的目标。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/survey-multi-modal-knowledge-graph/fig2.png&#34;
	width=&#34;843&#34;
	height=&#34;321&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;262&#34;
		data-flex-basis=&#34;630px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在这类构建方法中，还可以更细地划分为视觉实体（概念）抽取、视觉关系抽取和视觉事件抽取三类任务。&lt;/p&gt;
&lt;p&gt;视觉实体抽取旨在检测和定位图中的视觉物体，并通过KG中的实体符号进行标注。可以采用比如目标检测的方法（对检测出的物体区域分类到具体的实体上）&lt;/p&gt;
&lt;p&gt;该任务的主要挑战在于：如何有效地在没有大规模、细粒度和标注良好的实体-图片数据集上训练出一个细粒度的抽取模型。因为目前cv虽然有很多数据，但多为粗粒度的数据，无法满足MMKG的需求。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
