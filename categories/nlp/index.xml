<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>NLP on abel&#39;s blog</title>
        <link>https://blog.abelcse.cn/categories/nlp/</link>
        <description>Recent content in NLP on abel&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Wed, 10 May 2023 15:39:15 +0800</lastBuildDate><atom:link href="https://blog.abelcse.cn/categories/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Paper Notes Collection One</title>
        <link>https://blog.abelcse.cn/p/paper-notes-collection-one/</link>
        <pubDate>Wed, 10 May 2023 15:39:15 +0800</pubDate>
        
        <guid>https://blog.abelcse.cn/p/paper-notes-collection-one/</guid>
        <description>&lt;img src="https://blog.abelcse.cn/p/paper-notes-collection-one/fig3.png" alt="Featured image of post Paper Notes Collection One" /&gt;&lt;h2 id=&#34;w2ner&#34;&gt;W2NER&lt;/h2&gt;
&lt;p&gt;Notes for Paper: &lt;strong&gt;Unified Named Entity Recognition as Word-Word Relation Classification&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background:&lt;/h3&gt;
&lt;p&gt;现有的NER问题可以大致分成三种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简单实体(flat)，实体的构成比较简单，只识别出实体的开始和结束位置即可；&lt;/li&gt;
&lt;li&gt;重叠(嵌套)实体(overlapped)，会出现多个实体包含相同的token的情况；&lt;/li&gt;
&lt;li&gt;不连续实体(discontinuous)，实体由位置上不相邻的token构成。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如图1所示，在(a)中的实体$e_1$就是简单实体，而$e_2$则是不连续实体。又因为这两个实体同时出现在同一个句子中，并且有相互重叠的部分，即&lt;em&gt;aching in&lt;/em&gt;，因此它们又是重叠的实体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/fig1.png&#34;
	width=&#34;897&#34;
	height=&#34;372&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;fig 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;578px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;现有的NER方法大致可分为四种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;序列标注方法，简单来讲就是对每个token分配一个标签，以识别每个token在一个实体中扮演的角色。如图1中的$e_1$，我们可以将&lt;em&gt;aching in legs&lt;/em&gt;分别标为&lt;em&gt;BIE&lt;/em&gt;，以表示他们为开始、中间和结束。虽然这种方式简单直观，但是缺陷也很明显，即出现重叠和非连续实体时，简单的标签就无法完成任务了，并且还需要仔细地设计多种标签，复杂度非常高，也不利于解码；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于超图的方法，既然标注法对一个token只能分配一个标签，那么利用节点和边的特点(一个节点可以有多个边)来表示所有的实体span，在一定程度上缓解了标注法的实体嵌套问题，&lt;!-- raw HTML omitted --&gt;但推理时会受到虚假结构和结构模糊性问题的影响&lt;!-- raw HTML omitted --&gt;(原文如此说，因为还没读过相关方法的论文，没有理解，暂时划掉)；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;序列生成的方法：既然标注很麻烦，那干脆利用Seq2Seq的方式，直接生成实体，这样会不受嵌套和不连续的影响，但是会受到解码效率和偏差暴露的影响；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于span的方法：一般可以列举所有可能的span，然后对span进行分类，但这种方式不仅会受到span的长度限制，还会因为枚举造成大量的资源消耗。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation：&lt;/h3&gt;
&lt;p&gt;本文认为，上述方法的核心其实还是在寻找实体的边界，这种思想也许在解决某一个具体问题上有效，但如果想同时解决三种实体识别的问题，也就是建立一个统一的NER模型，那就不能仅仅只看实体的边界了。&lt;/p&gt;
&lt;p&gt;因此，本文认为这种统一模型的主要瓶颈在于如何建模好单词之间的相邻关系，因为只确定边界只是确定了实体的大致范围，至于词之间的关系：是复用的还是不相邻的？需要用其他方式来表示。&lt;/p&gt;
&lt;p&gt;所以本文提出了自己的方法&lt;strong&gt;W2NER&lt;/strong&gt;，该方法主要对词之间的两类关系，准确地说是三类关系进行建模，即：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;None&lt;/strong&gt;:无关系；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NNW&lt;/strong&gt;(Next-Neighboring-Word)：下一个邻接词；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;THW-*&lt;/strong&gt;:(Tail-Head-Word-*): 头尾词，*表示实体类型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简单来说，&lt;strong&gt;THW&lt;/strong&gt;确定了所有可能的实体边界，&lt;strong&gt;NNW&lt;/strong&gt;确定了实体边界里面的各词之间的关系。如图2所示:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;THW-S&lt;/strong&gt;确定了两个S(Symptom)类型的实体范围，(从尾找到头)：&lt;em&gt;aching in legs&lt;/em&gt; 和 &lt;em&gt;aching in legs and shoulders&lt;/em&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NNW&lt;/strong&gt;确定了词之间的关系，即当前单词的下一个词是谁，可以看到&lt;em&gt;in&lt;/em&gt;和&lt;em&gt;and&lt;/em&gt;并没有NNW关系，所以在两个实体范围中，只能解码出:&lt;em&gt;aching in legs&lt;/em&gt;和&lt;em&gt;aching in shoulders&lt;/em&gt;两个实体，而这样恰好解决了不连续的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/fig2.png&#34;
	width=&#34;927&#34;
	height=&#34;764&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;fig 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;121&#34;
		data-flex-basis=&#34;291px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这种思想也可以结合图2和图1(b)来直观地感受。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;model-and-experiment&#34;&gt;Model and Experiment:&lt;/h3&gt;
&lt;p&gt;本文使用了Bert和LSTM作为编码器，利用卷积层提取词之间的表示，最后利用双仿射和多层感知机联合分类出词之间的关系。模型总体结构如图3所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/fig3.png&#34;
	width=&#34;2175&#34;
	height=&#34;693&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;fig 3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;313&#34;
		data-flex-basis=&#34;753px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用卷积层的目的非常直观，因为本文建模的方式就是表的形式，而CNN相对也很适合处理这种结构的表示。在卷积层中，使用了条件层归一化操作(Conditional Layer Normalization)，论文认为这样能够有效产生词对的网格(表)的表示；之后利用类似Bert的方式，增加了词的位置信息和表格的区域信息；最后，使用不同的空洞卷积以捕获不同词距离之间的交互信息。&lt;/p&gt;
&lt;p&gt;在完成上面对表格的表示refine后，论文使用联合的预测器进行最后的token标记分类。因为原文提到先前的工作验证了MLP和biaffine联合使用有利于关系分类。&lt;/p&gt;
&lt;p&gt;具体的实验内容可见原文，这里以&lt;strong&gt;ShARe14&lt;/strong&gt;数据集为例，如下图，可见&lt;strong&gt;W2NER&lt;/strong&gt;模型在重叠和不连续场景长的确都取得了明显的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/fig4.png&#34;
	width=&#34;1061&#34;
	height=&#34;513&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;fig 4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;decode-strategy&#34;&gt;Decode Strategy:&lt;/h3&gt;
&lt;p&gt;解码的基本思想是利用词之间的关系来确定词和词之间的路径，文中以四个示例来展示解码的具体操作：&lt;/p&gt;
&lt;p&gt;需要注意的是，图中的下方文字，划线的是具体的实体，大写字母代表了实体中的词。而图中的蓝线表示NNW关系，红色的线表示THW关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/fig5.png&#34;
	width=&#34;1042&#34;
	height=&#34;317&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;fig 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;328&#34;
		data-flex-basis=&#34;788px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有两个实体AB和DE，属于简单实体，因此直接就能解码出来；&lt;/li&gt;
&lt;li&gt;有重叠的实体：ABC和BC，但因为ABC和BC均有THW关系，因此也可以解码除了；&lt;/li&gt;
&lt;li&gt;有重叠和不连续的实体：ABC和ABD，除了利用THW关系来解决重叠问题外，NNW也从B直接关联到D，从而识别出了不连续的ABD实体；&lt;/li&gt;
&lt;li&gt;比较复杂的实体：ACD和BCE，和上面不同的在于，有可能出现ACE和BCD的路径，但是通过THW的限制，使得这两种情况被排除。&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;thinking&#34;&gt;Thinking:&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;并非所有表格表示的东西都适合CNN，如果token的分类和其他token没有太多直接的关系，那么使用CNN不一定会有正向的作用；当然，本文中因为NNW关系本就需要邻近token的信息，所以非常适合，但是核不易太大；&lt;/li&gt;
&lt;li&gt;这种方法可以迁移到关系提取(联合提取)上，但是重叠的类型会更多，并且会引入超出实体级别的关系。如果依旧保留NNW这样的关系，可能造成模型学习的负担，并且很容易和其他标签重叠，因此必然需要进一步的修改标签和编码方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cot&#34;&gt;CoT&lt;/h2&gt;
&lt;p&gt;Notes for Paper: &lt;strong&gt;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;background-1&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;本文主要进行CoT评估的任务分别为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Arithmetic Reasoning：数学推理，即图1中所示的数学问题；&lt;/li&gt;
&lt;li&gt;Commonsense Reasoning：常识推理；&lt;/li&gt;
&lt;li&gt;Symbolic Reasoning：符号推理。例如要求将出现单词的首字母或者尾字母拼接在一起，虽然人类很容易解决该问题，但对模型而言非常具有挑战性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;motivation-1&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;思维链(CoT)在人类思考活动中很常见。当我们思考问题的时候，往往不是直接得到答案，而是将问题分解，然后逐步向正确答案靠近。&lt;/p&gt;
&lt;p&gt;类似地，以CoT方式对模型进行提示，理论上也能得到比较好的结果，这是因为很多数据集在训练时，直接给出问题和答案，然后然后让模型去学习，但是为什么会得到这个答案，模型可能并不了解，而引入CoT后，这些中间步骤会极大丰富模型学会为什么得到此答案的理由。&lt;/p&gt;
&lt;p&gt;如图1所示：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;传统的训练方法中，对于一个问题，直接给出答案，模型难以学到得到这个答案的具体原因；&lt;/p&gt;
&lt;p&gt;而在CoT提示方法中，给出的不仅是答案，而是增加了得到这个答案的中间步骤(也就是思考的过程)，通过这种方式，引导模型在解决类似问题时，也会先生成中间步骤，再得到最终答案，以提高准确性。&lt;/p&gt;
&lt;p&gt;本文发现，单纯增大模型的规模，不足以在一些具有挑战性的任务上提升对应的性能，比如上面提到的三个问题。&lt;/p&gt;
&lt;p&gt;因此，本文通过两个简单的思想，探索了大模型的在不扩大规模的前提下，如何提高模型在这些推理问题上的性能。这两个思想主要是：以前的大量语料和参数量已经给了模型产生中间步骤的能力；通过提示的方式可以进行few-shot学习，而无需微调。&lt;/p&gt;
&lt;p&gt;具体来说，就是人工设置每种任务类型的CoT提示，作为few-shot的学习示例，这里的图2以数学推理为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/cot-fig2.png&#34;
	width=&#34;738&#34;
	height=&#34;1024&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;cot fig2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;72&#34;
		data-flex-basis=&#34;172px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看到，对于标准(传统)提示而言，随着模型规模的增加，性能的确有上升；但要想达到监督模型所得到性能表现还有些困难，并且训练大规模的语言模型，所耗费的资源是很多的。&lt;/p&gt;
&lt;p&gt;而在这些模型上，仅仅通过增加CoT的提示，便有了达到甚至超过监督模型的性能。&lt;/p&gt;
&lt;p&gt;同时也能看出，CoT提示在规模比较大的模型上表现的更好，也许说明了，模型的规模越大，越有利于产生中间结果，越利于进行few-shot学习，再配合上合适的提示，大模型的性能才能被更好地被发挥出来。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;toolformer&#34;&gt;Toolformer&lt;/h2&gt;
&lt;p&gt;Notes for Paper: &lt;strong&gt;Toolformer: Language Models Can Teach Themselves to Use Tools&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation-2&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;大语言模型(LLM)虽然在few-shot和zero-shot方面实现了非常好的提升，并通过参数规模、语料增加而展现了其“涌现”的特点，但这些模型依旧存在一些固有的限制，例如：从最近的事件中获取最新的信息；精确的数学计算；理解低资源语言；缺乏对时间进程的感知等。&lt;/p&gt;
&lt;p&gt;但我们知道在日常生活中，早就有相关的工具能够很好的解决这些问题，那就是各种实用工具，比如搜索引擎、计算器和日历等。如果让大语言模型能够学会如何正确地使用这些工具，而不是寄希望于让他们自己解决所有问题，将极大节省训练的花费。为此，本文提出了Tooformer，以让模型拥有使用外部工具的能力，他们的方法主要有以下几个特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;要能以自监督的方式学习，因为大量的人工标注是昂贵的；此外，人类认为有用的信息，对模型而言则不一定，因此让模型自己学习或许更有益；&lt;/li&gt;
&lt;li&gt;语言模型不应该失去它的通用性，应该能够自己决定何时、如何使用哪种工具。与现有的方法相比，这使得对工具的使用更加全面，不受特定任务的束缚。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文中的调用方式为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/tf-eq.png&#34;
	width=&#34;452&#34;
	height=&#34;89&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;tf-eq&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;507&#34;
		data-flex-basis=&#34;1218px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;分别表示只有调用本身和一个调用包含其结果。下图的示例就是一个调用(c)的工具(a)，输入(i)和结果(r)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/tf-fig1.png&#34;
	width=&#34;430&#34;
	height=&#34;462&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;tooformer-fig1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;93&#34;
		data-flex-basis=&#34;223px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;作者们构建这种使用外部工具的模型的主要方法大致为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先让语言模型自己对大量的数据集按照自己的方式进行可能的API调用标注(因为现有的人工写的好的API调用例子并不多)；&lt;/li&gt;
&lt;li&gt;然后，再利用自监督损失来确定哪些API调用切实有助于模型的预测；&lt;/li&gt;
&lt;li&gt;最后，利用这些有用的API注释来微调模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-collection-one/tf-fig2.png&#34;
	width=&#34;947&#34;
	height=&#34;205&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;toolformer-fig2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;461&#34;
		data-flex-basis=&#34;1108px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;首先对于输入文本，先让语言模型利用其上下文学习能力去生成大量可能的API调用示例，再实际去执行这些API调用，然后用空序列调用做对比进行自监督损失以选出更可能有效的API调用，最后再利用这些API进行微调。&lt;/p&gt;
&lt;p&gt;本文主要使用了以下几种工具，利用GPT-J(6B)做微调的模型，实验结果的确有效，很多数据集上甚至比OPT(66B)和GPT3(175B)高得多：1). 问答；2). 计算器；3). 维基百科搜索；4). 机器翻译；5). 日历。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
        </item>
        
    </channel>
</rss>
