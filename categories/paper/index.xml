<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper on abel&#39;s blog</title>
        <link>https://blog.abelcode.tech/categories/paper/</link>
        <description>Recent content in Paper on abel&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sat, 15 Jun 2024 21:36:11 +0800</lastBuildDate><atom:link href="https://blog.abelcode.tech/categories/paper/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Multimodal Large Language Models: LLaVA Series</title>
        <link>https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/</link>
        <pubDate>Sat, 15 Jun 2024 21:36:11 +0800</pubDate>
        
        <guid>https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/</guid>
        <description>&lt;img src="https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/cover.jpg" alt="Featured image of post Multimodal Large Language Models: LLaVA Series" /&gt;&lt;h2 id=&#34;llava&#34;&gt;LLaVA&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.2304.08485&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Visual Instruction Tuning&lt;/a&gt;.  NeurIPS 2023.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;背景与动机&#34;&gt;背景与动机&lt;/h3&gt;
&lt;p&gt;在本文之前，无论是传统意义上的“大”模型，还是现在提及的大（语言）模型（LLM），都已经有了广泛的研究。&lt;/p&gt;
&lt;p&gt;例如多模态学习（MM）通过结合计算机视觉和自然语言处理知识在诸如图像-文本检索（Image-Text Retrieval）、图像描述（Image Captioning）和视觉问答（Visual Question Answering）等。&lt;/p&gt;
&lt;p&gt;在LLM出现之后，人们发现现有模型已经具备可观的指令遵循的能力（给定指令后，让LLM能够按照指令的描述完成特定任务），例如训练指令遵循的Agent去完成一系列任务和指令微调后让LLM能够在特定领域按照人类的要求完成任务。&lt;/p&gt;
&lt;p&gt;然而，结合LLM和MM的研究相对还较少，一是因为目前的LLM还是以文本为核心的模型，如何将视觉信息嵌入进语言模型中还有较大的研究空间；二是视觉语言指令微调数据集（Vision-Language）还较缺乏，因为与&lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.2103.00020&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CLIP&lt;/a&gt;这种图像-文本对的收集方式不同，要得到可信的指令微调数据集是有难度的。&lt;/p&gt;
&lt;p&gt;为此，本文的工作可以简单总结为以下两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过&lt;a class=&#34;link&#34; href=&#34;https://openai.com/index/chatgpt/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatGPT&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://openai.com/index/gpt-4/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GPT4&lt;/a&gt;构建了一个具备一定规模的&lt;strong&gt;多模态指令微调数据集&lt;/strong&gt;。具体构建方式见后文，但可以预想的是，文本之所以选择这两个模型辅助数据集的构建，一是纯人工的构建必然花费大量的成本，二是这两个模型经过很好的人工反馈强化学习（如&lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;InstructGPT&lt;/a&gt;）的训练，给出的回复一定程度上更贴近人类的偏好。&lt;/li&gt;
&lt;li&gt;利用CLIP作为视觉特征的编码器，利用一个包含大量ChatGPT回复的数据集上微调的LLaMA模型&lt;a class=&#34;link&#34; href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vicuna&lt;/a&gt;作为LLM，从而形成了一个多模态大模型：LLaVA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据集构建&#34;&gt;数据集构建&lt;/h3&gt;
&lt;p&gt;LLaVA数据集构建的核心是利用GPT辅助生成视觉指令数据。&lt;/p&gt;
&lt;p&gt;传统的多模态数据缺乏指令相关的内容，以文本内容较为丰富的image caption相关的数据集为例，其数据构成一般是$X_v: X_c$，既对于每一个图像样本$X_v$而言，都有对应地描述图像内容的文本$X_c$。&lt;/p&gt;
&lt;p&gt;在本文中，为了得到指令数据集，作者则利用GPT-4，针对上述提到的$X_c$，让GPT提出可能的问题，也就是假设已经有答案的情况下，让GPT问出符合该答案的问题。假设问题的表示为$X_q$，那么就可以创造出如下格式的指令数据： $X_qX_v$&lt;code&gt;&amp;lt;STOP&amp;gt; Assistant: &lt;/code&gt;$X_c$&lt;code&gt;&amp;lt;STOP&amp;gt;&lt;/code&gt;。具体的设计方式如图1所示，即给定GPT要求后，收集其产生的回复，从而拼接成固定格式的指令数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/fig1.png&#34;
	width=&#34;896&#34;
	height=&#34;603&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;356px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然而，仅仅使用这种方式产生的指令数据不仅缺乏多样性，还无法提供更复杂和深层次的逻辑推理信息。因此，本文以人工标注的具备高质量目标检测和描述信息的多模态数据集&lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.48550/arXiv.1405.0312&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;COCO&lt;/a&gt;为基础，充分使用数据集中的描述信息&lt;code&gt;captions&lt;/code&gt;和目标位置信息&lt;code&gt;bounding boxes&lt;/code&gt;来完成多模态指令数据的生成。&lt;/p&gt;
&lt;p&gt;具体来说，本文希望GPT基于这两类注释和对应的图像，生成三类核心指令数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对话数据：设计一个助手和人对话的场景，产生出助手在看图片，并回答人提出的问题，其实就是我们直观能够理解的多模态对话场景；&lt;/li&gt;
&lt;li&gt;详细描述：为了包含对图像全面和丰富的描述，本文创建了和该类问题相关的一系列问题，并通过询问GPT来产生详细的回复（简单理解就是让GPT对&lt;code&gt;captions&lt;/code&gt;进行扩充）&lt;/li&gt;
&lt;li&gt;复杂推理：上面两类数据重点在描述图像内容本身，为了促进模型的推理能力，本文还创建了很多具备推理要求的问题，从而获得推理指令数据（其实也就是让GPT根据问题生成带推理信息的回复）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于COCO两类注释得到三类指令数据的大致方式可以见图2：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/fig2.png&#34;
	width=&#34;855&#34;
	height=&#34;923&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;222px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;简单来讲，本文就是基于COCO的图像描述和目标位置信息，针对对话、描述和推理三类任务，人工先设计了多个对应的问题（作为few-shot）来询问GPT，从而让GPT完成对应的回复，以及自行产生问题和回复。&lt;/p&gt;
&lt;p&gt;最后，本文收集了158K的不同的语言-图像指令数据集，其中包括58K的对话数据，23K的详细描述数据和77K的复杂推理数据。本文还发现，使用GPT-4产生的数据质量比ChatGPT要好，比如空间推理问题。&lt;/p&gt;
&lt;h3 id=&#34;任务与方法&#34;&gt;任务与方法&lt;/h3&gt;
&lt;p&gt;本文要解决的任务就是在给定图像的情况下，模型能够根据用户给出的文本指令，准确地进行回复。当然并非必须问和图像相关的问题，毕竟模型核心还是语言模型，但该任务重点强调的是在用户询问和图像内容相关的问题时，模型可以理解并生成对应的回复。这在传统的语言模型、多模态模型和纯文本的LLM上是难以做到的。&lt;/p&gt;
&lt;h4 id=&#34;模型结构&#34;&gt;模型结构&lt;/h4&gt;
&lt;p&gt;LLM本身不是问题，因为现有的开源大模型以及在不同数据集上微调过的大模型都种类繁多，因此本文模型结构的设计重点就在于如何将图像信息提供给大模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/multimodal-large-language-models-llava-series/fig3.png&#34;
	width=&#34;688&#34;
	height=&#34;205&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;335&#34;
		data-flex-basis=&#34;805px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;结果分析&#34;&gt;结果分析&lt;/h3&gt;
&lt;p&gt;etc.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
        </item>
        <item>
        <title>Paper Notes for Multimodal Relation Extraction</title>
        <link>https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/</link>
        <pubDate>Mon, 25 Mar 2024 15:37:28 +0800</pubDate>
        
        <guid>https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/</guid>
        <description>&lt;img src="https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/cover.jpg" alt="Featured image of post Paper Notes for Multimodal Relation Extraction" /&gt;&lt;h2 id=&#34;mnre&#34;&gt;MNRE&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;MNRE: A CHALLENGE MULTIMODAL DATASET FOR NEURAL RELATION EXTRACTION WITH VISUAL EVIDENCE IN SOCIAL MEDIA POSTS. (ICME 2021) (后续简称为 MNRE)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multimodal Relation Extraction with Efficient Graph Alignment. (ACM MM 2021) (后续简称为 MEGA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;背景与动机&#34;&gt;背景与动机&lt;/h3&gt;
&lt;p&gt;关系抽取任务，就是给定文本和实体后，抽取出实体之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MNRE&lt;/strong&gt;这篇论文认为，现有的关系抽取任务都主要基于新闻类的数据，这些数据组织良好，形式正规，相对来说更容易建模。&lt;/p&gt;
&lt;p&gt;但对于社交媒体类的数据，这些方法会出现较大的性能下降。这是因为社交媒体的文本内容本身就可能不完整，人们往往会配合图片一起发文，所以在进行关系抽取时，还需要额外对图像的内容进行理解。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig1-1.png&#34;
	width=&#34;809&#34;
	height=&#34;854&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;227px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如图1-1所示，对于文本&lt;code&gt;JFK and Obama at Harvard @Harvard&lt;/code&gt;，可以得到三个实体：&lt;em&gt;JFK&lt;/em&gt;、&lt;em&gt;Obama&lt;/em&gt;和&lt;em&gt;Harvard&lt;/em&gt;，如果不考虑图像信息，可能会错误地得到&lt;code&gt;&amp;lt;JFK, Residence（住在）, Harvard&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;JFK, Spouse（配偶）, Obama&amp;gt;&lt;/code&gt;这些关系。&lt;/p&gt;
&lt;p&gt;但如果同时根据图像信息进行理解，就会得到&lt;code&gt;&amp;lt;JFK, Graduated at, Harvard&amp;gt;&lt;/code&gt;，因为&lt;em&gt;JFK&lt;/em&gt;戴着学位帽，穿着学位服，因此根据文本信息，更容易判断出他毕业于哈佛。&lt;/p&gt;
&lt;p&gt;为此，&lt;strong&gt;MNRE&lt;/strong&gt;提出了一个新的任务：多模态关系抽取（MRE，或者MMRE），并为此构建了数据集MNRE（Multimodal dataset for Neural Relation Extraction），该数据集主要由Twitter-15和Twitter-17构建而来，因此是偏向于社交媒体关系抽取的。作者认为，该工作既推动了多模态关系抽取任务的发展，也为对齐较细粒度的文本-图像信息做出了贡献。&lt;/p&gt;
&lt;p&gt;第2篇论文是同一批作者，主要是对该数据集做了一些改进，对新数据集做了几个baseline，并验证了自己的方法，具体内容将在后续介绍。&lt;/p&gt;
&lt;h3 id=&#34;数据集构建&#34;&gt;数据集构建&lt;/h3&gt;
&lt;p&gt;数据集来源：两个多模态实体识别数据集：Twitter-15和Twitter-17，以及部分从Twitter上爬取的内容（2019年1月-2月的内容）；&lt;/p&gt;
&lt;p&gt;数据集涵盖：没有特意挑选具体的领域，比如运动、社会事件，而是尽可能确保实体的多样性，并且去除了非英语以及实体数量低于两个的句子；&lt;/p&gt;
&lt;p&gt;标注方法：源数据、预训练好的NER标注工具、人工标注；&lt;/p&gt;
&lt;p&gt;具体情况见表1-1所示，其中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/tab1-1.png&#34;
	width=&#34;1617&#34;
	height=&#34;375&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 1-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;431&#34;
		data-flex-basis=&#34;1034px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;第2篇论文对此做了改进，新的数据信息见表1-2和图1-2。由于后续的大量研究都是基于新数据集的，所以&lt;code&gt;MNRE&lt;/code&gt;论文后续的方法和内容将不再介绍，而是以第2篇论文&lt;code&gt;MEGA&lt;/code&gt;为主。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/tab1-2.png&#34;
	width=&#34;617&#34;
	height=&#34;352&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 1-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图1-2则是对修改后的数据集的关系类别进行了统计，可以看到关系类别的标签还是存在很明显的不平衡问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig1-2.png&#34;
	width=&#34;884&#34;
	height=&#34;602&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;146&#34;
		data-flex-basis=&#34;352px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;任务与方法&#34;&gt;任务与方法&lt;/h3&gt;
&lt;p&gt;任务定义非常简单，与文本不同的在于，给定的输入还多了与文本相关联的图片：&lt;/p&gt;
&lt;p&gt;$F(e_1, e_2, S, V) \rightarrow Y$，既给定句子$S = (w_1, w_2, &amp;hellip;, w_n)$，句子中的实体$e_1, e_2$，以及与句子相关联的视觉内容$V = (v_1, v_2, &amp;hellip;, v_n)$，然后预测出实体$e_1,e_2$之间的关系类别$Y$。（注意，虽然定义中给出的是视觉内容集合，但实际数据集中基本上是一个句子对应一张图像，即使有多个关联情况，数据集也是将其划分为多个样本的）&lt;/p&gt;
&lt;p&gt;为了构建baseline以做对比，&lt;code&gt;MNRE&lt;/code&gt;论文分别测试了三类方法：基于CNN的方法&lt;em&gt;Glove+CNN&lt;/em&gt;、基于预训练语言模型的方法&lt;em&gt;BertNER&lt;/em&gt;、远程监督的方法&lt;em&gt;PCNN&lt;/em&gt;，此外，论文还将CNN与PLM结合了一下，即&lt;em&gt;Bert+CNN&lt;/em&gt;的方法。而&lt;code&gt;MEGA&lt;/code&gt;保留了&lt;em&gt;Glove+CNN&lt;/em&gt;和&lt;em&gt;PCNN&lt;/em&gt;的方法，并有专门面向关系抽的预训练Bert模型&lt;em&gt;MTB&lt;/em&gt; (Matching the Blanks)、BERT结合场景图的方法&lt;em&gt;Bert+SG&lt;/em&gt;、以及额外增加了注意力机制的方法&lt;em&gt;Bert+SG+Att&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;MEGA的方法如图1-3所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig1-3.png&#34;
	width=&#34;2008&#34;
	height=&#34;798&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;251&#34;
		data-flex-basis=&#34;603px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;根据方法图，MEGA的方法大致可以分为三部分：结构化特征表示、多模态特征对齐和实体表示与关系预测。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;结构化特征表示：&lt;/p&gt;
&lt;p&gt;文本结构：依存句法树，MEGA使用ELMo提取给定文本得句法，构建出对应的句法树；&lt;/p&gt;
&lt;p&gt;图像结构：场景图生成(SGG)，MEGA使用场景图生成模型得到针对图像的graph结构信息（场景图就是从图像中抽取出的以 object-relation-object结构为主的Graph）&lt;/p&gt;
&lt;p&gt;从模型结构中可以看到，句法树其实就是文本的图结构，场景图就是图像中物体关系的结构，并且该方法额外还是用了文本的表示，也就是$BERT(Text)$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多模态特征对齐：&lt;/p&gt;
&lt;p&gt;结构对齐：即对句法树和场景图进行对齐，通过邻接矩阵构建出各实体和目标之间的映射关系；&lt;/p&gt;
&lt;p&gt;语义对齐：将场景图的信息和从BERT中出来的文本特征进行融合，简单来说就是一个注意力机制，其中Query和Value是文本特征，Key是图像特征；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关系预测：&lt;/p&gt;
&lt;p&gt;这部分就是图1-3中的最右边的操作，简单来说，就是将之前结构、语义对齐的视觉特征融合，得到最终对齐的视觉语义特征；以及从BERT中出来的文本特征中得到头、尾实体的特征，将他们做个简单的拼接操作，也就是实体$E_{entity} = [head;tail]$，与对齐的视觉特征$E_{visual}$ 做拼接$z = [E_{entity};E_{visual}]$，然后直接全连接预测关系$output = MLP(z)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;结果分析&#34;&gt;结果分析&lt;/h3&gt;
&lt;p&gt;MEGA的实验结果如表1-3所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/tab1-3.png&#34;
	width=&#34;858&#34;
	height=&#34;355&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 1-3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;580px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看到MEGA是有效的。&lt;/p&gt;
&lt;p&gt;直观上来看，MEGA处理多模态信息时，分别从&lt;code&gt;结构对齐&lt;/code&gt;和&lt;code&gt;语义对齐&lt;/code&gt;两个方向出发，并进行模态融合，这是常见且有效的一种方法。&lt;/p&gt;
&lt;p&gt;然而，这种方法并非端到端的，场景图生成是有错误的，句法分析也是有错误的，除非MNRE数据集同时有这两个模型的强监督信号，并且在反向传播时也能更新他们的梯度，否则会由于错误积累导致后面模型性能存在不可能太高的上限。&lt;/p&gt;
&lt;p&gt;实际上，哪怕直接利用&lt;code&gt;BERT+ViT&lt;/code&gt;的方式进行端到端学习，也能得到比当前结果高的多的性能，甚至不使用视觉信号，BERT也能发挥很好的效果，因此一定程度上需要验证该方法对齐方式的有效性（这也符合对错误累积问题的分析）&lt;/p&gt;
&lt;p&gt;此外，其实数据集本身还存在问题，这些将在后续的论文更新中总结。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;more&#34;&gt;MORE&lt;/h2&gt;
&lt;p&gt;MORE: A Multimodal Object-Entity Relation Extraction Dataset with a Benchmark Evaluation.  (ACM MM 2023)&lt;/p&gt;
&lt;h3 id=&#34;背景与动机-1&#34;&gt;背景与动机&lt;/h3&gt;
&lt;p&gt;前面MNRE数据集率先提出了多模态关系抽取任务（MRE｜MMRE），其任务目标是给定文本、对应图片和文本中的实体对后，预测出实体对的关系信息。该任务定义有几个比较强的假设，一是图像-文本是匹配的，也就是图像信息能帮助对应文本的内容理解，二是实体都出现在文本中，虽然不排除图像信息有与之对应的内容，但必须确保文本中含有两个及以上的实体。&lt;/p&gt;
&lt;p&gt;对于第一点，对多模态任务而言是非常合理的，但对数据集的质量有要求。&lt;/p&gt;
&lt;p&gt;针对第二点，本文则认为相对来说是不合理的，多模态的关系三元组构成元素很多都来自于不同的模态，比如某个实体来源于文本，而另一个则是图像中的物体。为此，本文就设计了新的多模态关系抽取任务形式，即给定文本和对应图像，需要对来源于 文本中的实体 和 图像中的物体 所构成的&lt;code&gt;Object-Entity Pair&lt;/code&gt;预测关系，具体如图2-1所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig2-1.png&#34;
	width=&#34;896&#34;
	height=&#34;832&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;107&#34;
		data-flex-basis=&#34;258px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;也就是说，给定文本和对应图像后，还会从文本中给定若干实体，以及从图像中给定若干物体，最后要求模型对两两的实体-物体组合（Object-Entity Pair）预测出他们之间的关系。为此，本文按照这种任务的形式构建了新的数据集&lt;strong&gt;MORE&lt;/strong&gt;（Multimodal Object-Entity Relation Extraction）&lt;/p&gt;
&lt;h3 id=&#34;数据集构建-1&#34;&gt;数据集构建&lt;/h3&gt;
&lt;p&gt;数据集来源：与MNRE数据集从社交媒体推文中收集不同，该数据集从更正规的新闻媒体（The New York Times English和Yahoo News）中收集，因此数据集的质量相对更有保障，而且图-文匹配的程度更高。&lt;/p&gt;
&lt;p&gt;MORE的数据集构建分为三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用AllenNLP NER工具对文本中的实体进行抽取；利用YoLo V5对图像中的物体进行抽取；随后由人工对抽取的实体与物体进行筛选和过滤；&lt;/li&gt;
&lt;li&gt;让人工标注者对实体-物体之间的关系进行标注，没有关系的数据标注为&lt;code&gt;none&lt;/code&gt;关系，当然还有很多分组等确保标注质量、偏见和差距的方法，这里就不提了；&lt;/li&gt;
&lt;li&gt;关系过滤，也就是确保只有需要同时根据文本和图像才能得到的关系（过滤掉单独从文本或者单独从图像就能推测出关系的样本）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最终的数据集概要如表2-1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/tab2-1.png&#34;
	width=&#34;1110&#34;
	height=&#34;296&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 2-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;375&#34;
		data-flex-basis=&#34;900px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看到该数据集也与MNRE数据集做了对比，相比之下，MORE的样本数量变少了，但是关系三元组变多了，并且因为新增的Object任务模式（平均每张图有3.8个object，此外，平均每个文本1.5个entity），MORE数据集更难。进一步地，MORE数据集有22.2%的关系Fact是只包含1个实体和1个物体的，而77.8%的关系Fact含有多个实体和物体，所以对MMRE模型有更大的挑战性。&lt;/p&gt;
&lt;p&gt;在关系类别的分布上，大致涵盖了生活、位置等领域，但也呈现标签分布不均衡的问题，具体如图2-2所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig2-2.png&#34;
	width=&#34;1068&#34;
	height=&#34;788&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;任务与方法-1&#34;&gt;任务与方法&lt;/h3&gt;
&lt;p&gt;与MNRE的任务定义类似，MORE定义的任务形式为：$F = (e, o, S, V) \rightarrow R$，其中$e \in S, o \in V$。&lt;/p&gt;
&lt;p&gt;因为是23年的论文，因此除了较为简单的baseline外，本文还用了一些VLP的模型，后续会列出其性能的比较。&lt;/p&gt;
&lt;p&gt;本文提出的方法为&lt;code&gt;MOREformer&lt;/code&gt;，模型结构如图2-3所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig2-3.png&#34;
	width=&#34;2220&#34;
	height=&#34;1382&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2-3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;385px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;论文提到自己的方法主要是基于MKGformer而设计的，因此先介绍MKGformer的基本方法：其使用的文本编码器是BERT，图像编码器是ViT，对于得到的特征，需要进行融合：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fom2-1.png&#34;
	width=&#34;834&#34;
	height=&#34;196&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;formula 2-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;425&#34;
		data-flex-basis=&#34;1021px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;根据公式可以看到，对于文本和图像的特征表示$H^{M_t}, H^{M_v}$，将会被放到前缀指导交互模块（PGI, Prefix Guided Interaction）中，从而得到下一层的特征$\bar{H}_l^{M_t}, \bar{H}_l^{M_v}$，而新的两个特征又会放到相关性融合模块（CAF，Correlation-Aware Fusion）中进行融合。&lt;/p&gt;
&lt;p&gt;那么，PGI到底是干什么的？其实就是个注意力机制，而且是个专门处理[CLS] token的模块。对于文本特征而言，就是自注意力机制$M_t[CLS] = Attn(Q_t, K_t, V_t)$，但视觉的前缀则是修改后的注意力，大致为$M_v[CLS] = Attn(Q_v, [K_v, K_t], [V_v, V_t])$，也就是Key和Value是图像与文本的特征拼接，IFAformer也是这种融合的注意力机制。通过PGI，将加强文本[CLS]对自己全局信息的概括能力，以及图像[CLS]对文本-图像全局信息的概括能力。&lt;/p&gt;
&lt;p&gt;而CAF模块则负责模态的融合，具体细节后续整理MKGformer时再列出，大致可以认为该模块就是让文本的token和图像的patch进行交互，最后再送入FFN中。&lt;/p&gt;
&lt;p&gt;本文以上述方法为基础，主要设计了三个模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;属性相关的文本编码器（Attribute-Aware Textual Encoder）：这里不需要看公式，根据图2-3可知，就是一个图像描述生成任务的利用，先利用ClipCap生成图像的文本描述，再将该文本描述作为样本中的文本的辅助信息。这样做也是非常常见的一种方法，因为生成的文本本身就是对图像大致内容的描述，分析该文本有助于模型更好地理解图像信息（当然Image Caption模型的好坏会制约生成文本的作用）；具体使用时，本方法则是将整个描述文本当作object来用。也就是说：对于文本$S$和其中的实体$e$，得到BERT分词后的token序列，并将图像生成的描述$caption$拼接上去，之后就是正常通过BERT得到嵌入表示了，即：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fom2-2.png&#34;
	width=&#34;858&#34;
	height=&#34;228&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;formula 2-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;376&#34;
		data-flex-basis=&#34;903px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;深度相关的视觉编码器（Depth-Aware Visual Encoder）：图像的深度信息能过表示图中各物体的层次结构，本文利用S2RDepthNet得到图像的深度信息，简而言之就是将ViT得到RGB-3通道的patch和深度网络模型得到的深度表示拼接在一起，形成具有深度信息的图像表示；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;位置融合的多模态编码器（Position-Fused Multimodal Encoder）：这里的步骤比较多，简单来讲就是既要利用图像表示、文本表示，还要使用目标的位置信息（因为抽取的图像是标注出来的，因此数据集中含有目标框bbox信息），从而在最后进行MLP关系预测时，能即考虑文本、图像的特征，还考虑实体、图像层次和目标位置的信息，最终做出预测。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;结果分析-1&#34;&gt;结果分析&lt;/h3&gt;
&lt;p&gt;本文在实验上做的还是比较充分的，主要的实验结果以及针对本文方法的消融实验分别如图2-4和图2-5所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig2-4.png&#34;
	width=&#34;1144&#34;
	height=&#34;510&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2-4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;224&#34;
		data-flex-basis=&#34;538px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-multimodal-relation-extraction/fig2-5.png&#34;
	width=&#34;748&#34;
	height=&#34;582&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 2-5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;308px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;主实验就不说了，主要看看消融实验，其中P、A和D分别指位置融合、属性（描述文本）和深度信息，按照其结果来看，位置信息最重要，其次是对图像的文本描述，最后是深度信息。这说明文本模块（BERT）发挥了更大的作用，而图像模块（ViT）需要更细致的信息才能起作用。&lt;/p&gt;
&lt;p&gt;其次，和MNRE那里类似，本文也用了很多属于pipeline的模块，例如深度信息生成和图像描述生成，这些模型都没接受梯度更新，因此他们的错误会累积到后续的模块中去。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
        </item>
        <item>
        <title>Paper Notes for Visual Language Models</title>
        <link>https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/</link>
        <pubDate>Thu, 06 Jul 2023 20:25:59 +0800</pubDate>
        
        <guid>https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/</guid>
        <description>&lt;img src="https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit.png" alt="Featured image of post Paper Notes for Visual Language Models" /&gt;&lt;h2 id=&#34;vit&#34;&gt;ViT&lt;/h2&gt;
&lt;p&gt;An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)&lt;/p&gt;
&lt;h3 id=&#34;abs--int&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；&lt;/p&gt;
&lt;p&gt;然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；&lt;/p&gt;
&lt;p&gt;Transformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model.&lt;/h3&gt;
&lt;p&gt;文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \in R^{H \times W \times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\in R^{N \times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。&lt;/p&gt;
&lt;p&gt;根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PatchEmbed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;224&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_chans&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_patches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_chans&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# (B, 3, 14, 14) ==&amp;gt; (B, 768, 14, 14)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Identity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (B, 768, 14, 14)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# BCHW -- BNC (B, 196, 768)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit.png&#34;
	width=&#34;934&#34;
	height=&#34;461&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;202&#34;
		data-flex-basis=&#34;486px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$&lt;/p&gt;
&lt;p&gt;之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。&lt;/p&gt;
&lt;p&gt;接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \rightarrow (197, 3072) \rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。&lt;/p&gt;
&lt;p&gt;因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。&lt;/p&gt;
&lt;p&gt;具体流程也可以参考下面的公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit_eq.png&#34;
	width=&#34;1148&#34;
	height=&#34;171&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT-equation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;671&#34;
		data-flex-basis=&#34;1611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;注：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;这里的位置编码，原文实验显示，无论使用(1, 2, &amp;hellip;, N)的1D方式，还是(11, 12, 13, &amp;hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;位置编码和CLS头可以简单按照下面的方法添加：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;position_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;196&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;class_patch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;exp&#34;&gt;Exp.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit_model.png&#34;
	width=&#34;690&#34;
	height=&#34;163&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT variants&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;423&#34;
		data-flex-basis=&#34;1015px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcode.tech/p/paper-notes-for-visual-language-models/vit_exp_1.png&#34;
	width=&#34;953&#34;
	height=&#34;308&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT-Exp1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;309&#34;
		data-flex-basis=&#34;742px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。&lt;em&gt;可以看到，ViT-H/14 要2500个，普通机构是消耗不起的&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的&lt;strong&gt;继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA&lt;/strong&gt;; （但是后面也做了实验，实验结果大概是：&lt;strong&gt;数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用&lt;/strong&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ViT的训练时间也变少了（相对两个baseline来说）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclu&#34;&gt;Conclu.&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适；&lt;/li&gt;
&lt;li&gt;ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的；&lt;/li&gt;
&lt;li&gt;混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，&lt;em&gt;个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好&lt;/em&gt;）&lt;/li&gt;
&lt;li&gt;当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;clip&#34;&gt;CLIP&lt;/h2&gt;
&lt;p&gt;Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)&lt;/p&gt;
&lt;h3 id=&#34;abs--int-1&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;p&gt;先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。&lt;/p&gt;
&lt;p&gt;在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。&lt;/p&gt;
&lt;p&gt;但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？&lt;/p&gt;
&lt;p&gt;事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。&lt;/p&gt;
&lt;h3 id=&#34;model-1&#34;&gt;Model.&lt;/h3&gt;
&lt;h4 id=&#34;dataset&#34;&gt;Dataset.&lt;/h4&gt;
</description>
        </item>
        
    </channel>
</rss>
