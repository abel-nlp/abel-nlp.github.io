<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper on abel&#39;s blog</title>
        <link>https://blog.abelcse.cn/categories/paper/</link>
        <description>Recent content in Paper on abel&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 06 Nov 2023 10:53:42 +0800</lastBuildDate><atom:link href="https://blog.abelcse.cn/categories/paper/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Paper Notes for Video Visual Relation Detection</title>
        <link>https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/</link>
        <pubDate>Mon, 06 Nov 2023 10:53:42 +0800</pubDate>
        
        <guid>https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/</guid>
        <description>&lt;img src="https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/cover.jpg" alt="Featured image of post Paper Notes for Video Visual Relation Detection" /&gt;&lt;h2 id=&#34;vidvrd&#34;&gt;VidVRD&lt;/h2&gt;
&lt;p&gt;Video Visual Relation Detection. (ACM MM 2017)&lt;/p&gt;
&lt;h3 id=&#34;abs--int&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;p&gt;作为连接视觉和语言的桥梁，以关系三元组&lt;code&gt;&amp;lt;主语，谓语，宾语&amp;gt;&lt;/code&gt;为形式的目标间视觉关系提供了超过目标本身的更为全面的视觉内容理解，如&lt;code&gt;&amp;lt;person, touch, dog&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;cat, above, sofa&amp;gt;&lt;/code&gt;等。&lt;/p&gt;
&lt;p&gt;本文提出了一种新的视觉任务，叫作&lt;strong&gt;Video Visual Relation Detection (VidVRD)&lt;/strong&gt;，用于在视频而非静态的图片上进行关系检测。与静态图相比，视频为检测视觉关系提供了更自然的一组特征，比如动态的关系如&lt;code&gt;&amp;lt;A, follow, B&amp;gt;, &amp;lt;A, towards, B&amp;gt;&lt;/code&gt;（A跟着B，A朝着B），以及类似&lt;code&gt;&amp;lt;A, chase, B&amp;gt;&lt;/code&gt;然后&lt;code&gt;&amp;lt;A, hold, B&amp;gt;&lt;/code&gt;（A追逐B，然后A抓住B）这种时序关系。然而，VidVRD任务相比于图像关系检测任务ImgVRD，在技术上具有更大的挑战性，因为其很难对准确的目标进行追踪和应付多种多样的关系表现。&lt;/p&gt;
&lt;p&gt;为此，本文提出了一个包含目标轨迹小片段候选、短期关系预测和贪心关系关联的VidVRD方法，并提供了一个用于任务评估的数据集，其中包含1000个人工标注的视频。与以前的相关方法相比，本文在该数据集上实现了最佳性能。&lt;/p&gt;
&lt;p&gt;缩小视觉和语言之间的差距对多媒体分析至关重要，相关工作涉及&lt;em&gt;visual concept annotations, semantic description with captioning&lt;/em&gt; 和 &lt;em&gt;visual question-answering&lt;/em&gt;、字幕语义描述和视觉问答等。视觉关系检测&lt;em&gt;VRD&lt;/em&gt;是最近为了更全面地理解目标之外的视觉内容而做出的工作，目的是为了捕获目标之间的各种互动，它可以有效地支持众多的视觉语言任务，如&lt;em&gt;captioning, visual search&lt;/em&gt; 和 &lt;em&gt;visual question-answering&lt;/em&gt;等。&lt;/p&gt;
&lt;p&gt;视觉关系包括用边界框定位的一对目标和连接他们之间的谓语，也就是关系。如图1-1(a)所示，其中两个目标可以有不同的关系，同一个关系也可连接起不同外观的目标对。在本文中将使用关系三元组来表示由&lt;code&gt;&amp;lt;subject, predicate, object&amp;gt;&lt;/code&gt;构成的视觉关系类型。因为这种组合的可能性非常多，传统的目标检测方法在这个任务中并不适合，即使已经有的几个相关任务的方法也更适用于静态图片而非视频。与静态图片相比，视频为检测视觉关系提供了一组更为自然的特征。图1-1(b)中，从视频的时空内容中提取的运动特征有助于区分相似的关系，如&lt;em&gt;walk&lt;/em&gt; 和 &lt;em&gt;run&lt;/em&gt;。此外，一些动态的视觉关系只能在视频中被检测到，如&lt;code&gt;&amp;lt;dog, run past, person&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;dog, faster than, person&amp;gt;&lt;/code&gt;。因此，与ImgVRD相比，VidVRD是一个更通用更可行的任务。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/p1_1.png&#34;
	width=&#34;1021&#34;
	height=&#34;519&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;196&#34;
		data-flex-basis=&#34;472px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;VidVRD和ImgVRD之间的另一个显著不同是：视频中的视觉关系通常是随时间变化的，而图像中的关系则是固定的。目标可能会因为遮挡或离开画面而导致视觉关系出现和消失，即使两个目标始终出现在同一个视频帧中，它们之间的互动关系也可能随着时间而发生变化。如图1-2所示，其中的狗和飞盘同时出现在$t_2$和$t_7$之间，但它们之间的关系却从追逐变成了撕咬。因此，应当重新设定义VidVRD任务，以处理视觉关系的可变性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/p1_2.png&#34;
	width=&#34;1039&#34;
	height=&#34;407&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;612px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;
&lt;p&gt;为了与ImgVRD的定义保持一致，本文将VidVRD任务定义如下：给定一组目标类别$C$和谓语(关系)类别$P$，VidVRD旨在检测出视频中的视觉关系实例$C \times P \times C$，其中视觉关系实例由关系三元组&lt;code&gt;&amp;lt;subject, predicate, object&amp;gt;&lt;/code&gt;$\in C \times P \times C$表示，并带有主语和宾语的轨迹信息$T_s$和$T_o$，具体来说，$T_s$和$T_o$是两个边界框的序列，在视觉关系的最长持续时间内构成主语和宾语。如图1-2中，给定的视频关系可以用三元组&lt;code&gt;&amp;lt;dog, chasem frisbee&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;dog, bite, frisbee&amp;gt;&lt;/code&gt;来表示，狗和飞盘分别用$(t_2, t_4)$和$(t_5, t_7)$之间的红色与绿色轨迹定位。&lt;/p&gt;
&lt;p&gt;与ImgVRD相比，VidVRD面临更多的技术挑战。&lt;/p&gt;
&lt;p&gt;首先，VidVRD需要用边界框轨迹来定位目标，这比在ImgVRD中为每个目标提供一个边界框更加困难，因为轨迹准确性受到每一帧的目标定位及目标跟踪的性能影响。本文所提出的VidVRD方法通过在视频的每个重叠短片段中生成目标轨迹小片段，然后根据预测出的关系将小片段和目标轨迹关联起来。其次，VidVRD需要在最长持续时间中时序地定位视觉关系。为了完成该任务，本文提出了一个贪心关联算法，其通过判断相邻片段中检测到的视觉关系是否具有相同关系三元组，且目标轨迹小片段具有足够高的重叠度来决定是否合并他们。最后，与ImgVRD相比，VidVRD需要预测更多类型的视觉关系，因为有的视觉关系只能在视频中检测到，为了有效预测关系，本文提出了关系预测模型，该模型从主语/宾语的轨迹小片段中提取出多种特征，包括外观特征、运动特征和相对特征。然后将这些特征编码成关系特征，再分别使用单独的主语、宾语和关系预测器来预测视觉关系。&lt;/p&gt;
&lt;p&gt;截至目前，还没有针对VidVRD的数据集，不过有几个针对ImgVRD的数据集，如 &lt;em&gt;Visual Relationship dataset, Visual Genome&lt;/em&gt;，因此，本文构建了一个VidVRD的数据集进行评估。本文设计了一个谓语描述机制，并从 &lt;em&gt;ILSVRC2016-VID&lt;/em&gt; 中构建该数据集。该数据集包含1000个视频，其中有人工标注的视觉关系和目标边界轨迹信息。&lt;/p&gt;
&lt;p&gt;本文的主要贡献在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了新颖的VidVRD任务，旨在探索视频中目标之间的各种关系，与ImgVRD相比，它提供了更可行地VRD任务；&lt;/li&gt;
&lt;li&gt;提出了一种新的VidVRD方法，该方法通过目标轨迹小片段候选、关系预测和贪心关系关联来检测视频中的视觉关系；&lt;/li&gt;
&lt;li&gt;贡献了第一个VidVRD数据集，带有1000个人工标注的视频数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;rel&#34;&gt;Rel.&lt;/h3&gt;
&lt;h4 id=&#34;video-object-detection&#34;&gt;Video object detection&lt;/h4&gt;
&lt;p&gt;视频目标检测旨在从给定视频中检测出预定义的目标，并通过边界框轨迹对其进行定位。目前最好的方法主要是通过整合目标检测和多目标跟踪来实现。&lt;/p&gt;
&lt;p&gt;最近，复杂的深度神经网络在图像目标检测方面取得了很好的性能。然而视频中存在模糊、运动和遮挡等情况，阻碍了利用边界框轨迹对目标进行精确定位的操作，因此视频中的目标检测仍然存在准确率低的问题。&lt;/p&gt;
&lt;p&gt;另一方面，由于目标检测器漏检率较高，采用逐个检测追踪策略的多目标追踪往往会产生较短的轨迹，因此有研究开发了额外的合并算法，以获得时间上更一致的目标轨迹。&lt;/p&gt;
&lt;p&gt;为此，本文的方法利用视频目标检测器生成短时的目标小轨迹候选来避免上面两个问题，并且该方案适合所有好的目标检测和多目标追踪方法。&lt;/p&gt;
&lt;h4 id=&#34;visual-relation-detection&#34;&gt;Visual relation detection&lt;/h4&gt;
&lt;p&gt;近期的研究工作主要集中于图像的关系检测上，人们普遍认为视觉关系检测的基本挑战在于，如何通过从少量训练实例中学习来建模和预测大量的关系。为解决该问题，现有的大部分方法都是分别预测视觉关系三元组中的主语、宾语和谓语。但目前的ImgVRD方法不适用于VidVRD任务，如其中的动态关系和视频关系的可变性。因此，本文提出了一种视频特定关系特征和一个新的训练准则，用于学习单独的预测模型。&lt;/p&gt;
&lt;p&gt;本文的方法是首次在视频上进行视觉关系检测的尝试，虽然之前的一些工作也和视频视觉关系任务相关，但它们所追求的目标和VidVRD完全不同。&lt;/p&gt;
&lt;h4 id=&#34;action-recognition&#34;&gt;Action recognition&lt;/h4&gt;
&lt;p&gt;动作是视觉关系中一种主要的谓词类别，VidVRD可以借鉴动作识别方面的进展，在动作识别中，特征表示在处理大的类别差距、背景干扰和摄像机运动方面起着重要的作用。为了解决这些问题，人们开发了手工特征和深度神经网络的方法。受相关工作的启发，本文将改进的密集轨迹(iDT)作为部分特征，因为iDT在大多数动作识别数据集上取得了出色的性能，尤其是在训练数据不足的情况下。值得注意的是，本文的方法旨在检测目标之间比动作更一般的关系，如空间关系和比较（相对）关系。&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset.&lt;/h3&gt;
&lt;p&gt;本文基于 &lt;em&gt;ILSVRC2016-VID&lt;/em&gt; 构建了第一个用于评估VidVRD任务的数据集，包含人工标注边界框的30类目标的视频共1000个，视频中视觉关系清晰丰富，忽略了目标单一，视觉关系模糊的视频。数据集被划分为训练集和测试集，分别包含800个和200个视频。&lt;/p&gt;
&lt;p&gt;此外，还补充了5个经常出现的目标类别，即 &lt;em&gt;person, ball, sofa, skateboard&lt;/em&gt; 和 &lt;em&gt;frisbee&lt;/em&gt;，因此共有35个目标种类。这些类别都是相对比较独立的，也就是不包括目标之间的分属关系，比如&lt;code&gt;&amp;lt;bicycle, with, wheel&amp;gt;&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;通过对及物动词、比较动词、空间描述和不及物动词等进行筛选和变换，共有132种关系被包含在数据集中。&lt;/p&gt;
&lt;p&gt;具体的数据划分等，见表1-1：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/t1_1.png&#34;
	width=&#34;874&#34;
	height=&#34;446&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;table 1-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;195&#34;
		data-flex-basis=&#34;470px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;method&#34;&gt;Method.&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;vidor&#34;&gt;VidOR&lt;/h2&gt;
&lt;p&gt;Annotating Objects and Relations in User-Generated Videos. (ICMR 2019)&lt;/p&gt;
&lt;h3 id=&#34;abs--int-1&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;p&gt;要进行细粒度的视频内容分析，理解目标之间的关系必不可少。然而，现存的工作受到数据集规模太小和评价指标不直接的限制。这些限制主要是因为构建大规模详细标注的视频数据集费时费力。因此，本文提出了一种pipeline的视频标注方法，能够以少量的花费构建数据集，并构造了新的视频数据集VidOR，该数据集包含了10k个视频（大概84小时），共80个目标类别和50个关系类别。&lt;/p&gt;
&lt;p&gt;细粒度的视频内容分析很重要，目前一些视频字幕描述和问答任务已经证明了这一点。随着粒度的不断细化，理解目标之间的关系也变得重要，目前的研究表明理解目标之间的关系对视频内容分析生成更稳健的表示是有效的。但是目前相关的工作都是较为隐性地建模和评估目标之间的关系，对模型是否和如何很好理解细粒度内容并不很清楚。&lt;/p&gt;
&lt;p&gt;但是在视频中进行目标和关系的识别是很有挑战性的任务，这需要对目标的外观、身份、动作和目标之间的交互。如图1-1中的狗所示，其外观可能因为它的活动、光照和遮挡而发生较大变化，这对给跨视频帧去确定狗的身份带来较大的困扰，而这却是进一步总结视频内容的必要线索。此外，动作和交互表示的差异也对模型学习根本的模式和稳健的推理带来了新的挑战。为此有很多工作，如&lt;em&gt;video object detection&lt;/em&gt;和&lt;em&gt;video visual relation detection&lt;/em&gt;在深入地研究这些问题，但他们主要受到了可用数据集规模太小的限制。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/vidor_1.png&#34;
	width=&#34;2092&#34;
	height=&#34;357&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;585&#34;
		data-flex-basis=&#34;1406px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对上面的内容，我们可以总结为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;细粒度的视频内容分析非常重要，比如生成视频字幕，进行视频问答等；&lt;/li&gt;
&lt;li&gt;但是进行细粒度的视频内容分析并不容易，这主要是因为现存的数据集规模小，无法设计更好的模型；&lt;/li&gt;
&lt;li&gt;而视频中目标的关系检测是很重要的部分，能够帮助进行视频内容分析。而要进行关系检测，需要考虑目标的外观、身份、动作和目标之间的交互，这是具有挑战性的，甚至连一个这样的大规模数据集都还没有。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此，本文的目的就是构建一个包含详细标注的目标和关系的更大规模数据集，视频均来源于网上以确保反映真实场景。标注信息通过边界框轨迹对目标进行时空定位，对关系进行时间定位。和类似的图片数据集相比，本文希望构建一个视频的benchmark，并且能够对动作和动态关系进行学习和推理。&lt;/p&gt;
&lt;p&gt;但是构建这样的数据集代价高昂，假设平均一个视频150帧，10k个视频就需要标注1.5m张图片，这已经达到了当前一些图像数据集的规模。&lt;/p&gt;
&lt;p&gt;如果假设相邻帧之间具有相似性，那么就可以在固定的间隔中进行手工标注，然后再进行插值。但这种方式只适合电影及监控视频这种连贯的视频类型。而由用户生成的视频内容则是完全自由的，质量一般不太高，视频的连续性也不强，所以不能简单的进行插值，而需要选择一种更好的策略进行关键帧的选取和人工标注。&lt;/p&gt;
&lt;p&gt;此外，如何将标注任务划分为子任务也是比较复杂的问题。假设我们将标注视频中的目标和关系视为一个单一任务，那么标注者就需要接受相关的培训，花费大量的精力。因此，更好的选择是将标注任务变成更宏观的子任务，如判断目标的类型，确定是否有边框，边框是否正确等等。&lt;/p&gt;
&lt;p&gt;为此，本文将设计一种新的pipeline方法，它适用于长度从几秒到几分钟不等的用户生成的视频。&lt;/p&gt;
&lt;h3 id=&#34;rel-1&#34;&gt;Rel.&lt;/h3&gt;
&lt;h4 id=&#34;video-annotation&#34;&gt;Video Annotation&lt;/h4&gt;
&lt;p&gt;当前的时序标注大致可以分为两类，一类是通过标注目标在视频中的开始和结束帧进行时序的定位，因此需要标注者仔细地浏览整个视频；另一类方法是在较短的视频片段钟标记目标是否存在，然后自动合并各片段的结果以实现时间定位，这种方法相对节省时间，但可能产生粗粒度的标注结果。&lt;/p&gt;
&lt;p&gt;在本文的方法中，通过为不同的关系仔细挑选合适的方法，从而结合了两类方法的优点。&lt;/p&gt;
&lt;h4 id=&#34;video-datasets&#34;&gt;Video Datasets&lt;/h4&gt;
&lt;p&gt;视频数据集按照标注程度来划分。一种是按照整个视频级别来进行标注，如CCV、Kinetics和MSR-VTT，他们都是用于事件识别，动作识别和视频字幕任务的典型benchmark；第二类是片段级别的标注，比如THUMOS、MultiTHUMOS和ActivityNet Caption，他们一般用于动作、活动和事件的时序定位的评测；第三类是目标级别的标注，TrackingNet就是其中一个大规模的标注了目标边框轨迹的目标追踪数据集。ImageNet-VID是一个超过30类目标的目标检测数据集。AVA则是一个时空活动定位的视觉动作数据集；最后则是关系级别的标注，VidVRD是基于ImageNet-VID构建的当前唯一一个提供目标和关系的视频数据集，但是该数据集相对较小，并且标注的比较稀疏。&lt;/p&gt;
&lt;h3 id=&#34;stat&#34;&gt;Stat.&lt;/h3&gt;
&lt;h4 id=&#34;annotation-pipeline&#34;&gt;Annotation Pipeline&lt;/h4&gt;
&lt;p&gt;为了更好地进行标注，本文提出了如图1-2所示的包含目标和关系标注的流水线标注方案。为了完成标注任务，标注者需要完成三个子任务：浏览视频以发现新的目标；绘制这些目标的边框；将被遮挡或者离开镜头外的目标的帧标注为不可见。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/vidor_2.png&#34;
	width=&#34;2018&#34;
	height=&#34;777&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;figure 1-2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;259&#34;
		data-flex-basis=&#34;623px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;多是数据集的描述和统计，后续将简要总结出来。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;vrd-gcn&#34;&gt;VRD-GCN&lt;/h2&gt;
&lt;p&gt;Video Relation Detection with Spatio-Temporal Graph. (ACM MM 2019)&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Paper Notes for Visual Language Models</title>
        <link>https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/</link>
        <pubDate>Thu, 06 Jul 2023 20:25:59 +0800</pubDate>
        
        <guid>https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/</guid>
        <description>&lt;img src="https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit.png" alt="Featured image of post Paper Notes for Visual Language Models" /&gt;&lt;h2 id=&#34;vit&#34;&gt;ViT&lt;/h2&gt;
&lt;p&gt;An Image is worth 16x16 Words: Transformers for Image Recognition at Scale. (ICLR 2021)&lt;/p&gt;
&lt;h3 id=&#34;abs--int&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先，Transformer在NLP已经很普遍了，并且得到了很大的进步(无论是数据还是模型参数)，而CV这边尝试的大多是让自注意力和CNN进行配合；&lt;/p&gt;
&lt;p&gt;然而，如果让每个像素点都参与到SA的计算中来，将是无法接受的计算开销。因此，有将SA仅仅用作局部的查询的工作，也有让SA分别在不同的块中使用的办法，但这些办法都需要复杂的工程能力才能在硬件加速器上跑起来。和ViT最近似的工作是从输入中提取2x2大小的patch(块)，然后使用SA，但这种方式只适合低分辨率的图像，而ViT不仅几乎和文本Transformer一样，还能处理中等分辨率的图像，并能达到或超过CNN的性能；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本文希望能够参考transformer，尽可能做少的修改，让其像处理文本一样处理图像信息，以利用transformer的计算效率特性和扩展性；&lt;/p&gt;
&lt;p&gt;Transformer原文中说，对于每一层的计算复杂度，SA和CNN分别是$O(n^2·d)$和$O(k·n·d^2)$，因为n往往都比d小，所以SA是比CNN更高效的；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此，本文将图像拆成patch，这个patch可以看成是NLP那边的token一样；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本文发现，在中等的图像数据集ImageNet上，不经过正则化，得到的模型比ResNet要低几个点，这主要是因为transformer没有CNN的两个归纳性偏好：平移不变性和局部性；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但在继续增大训练数据的规模后（从14M扩大到300M），Visual-Transformer的性能在逐步增加，并实现SOTA&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model.&lt;/h3&gt;
&lt;p&gt;文本的Transformer处理的是一维的序列，因此ViT需要先将2D的图像输入也变成类似的。对于一张图片$X \in R^{H \times W \times C}$，其中$(H, W)$为图片的高和宽，而$C$为图像的通道，首先将其展成2D的patch，每个patch的大小为$X_p\in R^{N \times (P^2 · C)}$，这里的$(P, P)$为patch的高和宽，$N$为patch的数量，不难得到$N = HW/P^2$。&lt;/p&gt;
&lt;p&gt;根据上面的表示，对于一个224x224x3的图，如果需要patch为16x16x3的分辨率，那么将得到$224^2 / 16^2$，也就是14x14个（共196个）patch。当然，这样来讲也并不容易让人理解，所以直接看部分核心代码的实现（为直观起见，省略并修改了部分代码）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PatchEmbed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;224&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_chans&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_patches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_chans&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;patch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# (B, 3, 14, 14) ==&amp;gt; (B, 768, 14, 14)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm_layer&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Identity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# (B, 768, 14, 14)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flatten&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# BCHW -- BNC (B, 196, 768)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;通过代码可知，利用16x16的卷积核，将原图打成14x14个patch，每个patch的通道维度从3变为768，再Flatten并变维为$(B, N, C)$。具体可看下图的图片输入及Linear Projection部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit.png&#34;
	width=&#34;934&#34;
	height=&#34;461&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;202&#34;
		data-flex-basis=&#34;486px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;此外，为了和NLP分类任务保持一致，这里也在所有patch前面增加了一个patch，即分类头CLS，因此最终传给Transformer的是$(B, N+1, C)$，在这里的例子中是$(B, 197, 768)$&lt;/p&gt;
&lt;p&gt;之后还需要做位置编码，ViT使用的是可训练的1维位置嵌入，shape和$(B, N+1, C)$保持一致，然后直接和每个patch相加。&lt;/p&gt;
&lt;p&gt;接着就是具体Transformer Encoder部分，经过LayerNorm之后，shape依旧是$(197, 768)$，在MSA部分，先将输入映射到QKV，假设有12个头，则QKV的shape为$(197, 64)$，输出后再拼接成$(197, 768)$，再经过一层LayerNorm，然后送入MLP。这里MLP的操作也比较简单，完成了：$(197, 768) \rightarrow (197, 3072) \rightarrow (197, 768)$的操作。当然，在每次送入LN层前有一个残差$x + f(x)$的操作。&lt;/p&gt;
&lt;p&gt;因为每个block的输入和输出都是$(197, 768)$，因此可以堆叠多个block，最后输出CLS作为分类任务的依据。&lt;/p&gt;
&lt;p&gt;具体流程也可以参考下面的公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit_eq.png&#34;
	width=&#34;1148&#34;
	height=&#34;171&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT-equation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;671&#34;
		data-flex-basis=&#34;1611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;注：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;这里的位置编码，原文实验显示，无论使用(1, 2, &amp;hellip;, N)的1D方式，还是(11, 12, 13, &amp;hellip;., )的2D方式，性能差距都不大；也就是没有位置编码和有位置编码会有一定的性能差距，而不同的位置编码方式之间的性能差距则比较小。文中推测这是因为使用的是patch，而非pixel的输入，因此空间之间的信息差异就没那么重要了；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到Transformer没有CNN那样的inductive bias，也就是局部性和平移不变性，那么能不能适当的将两者混合一下呢(Hybrid)，因此ViT利用Conv2d提取特征图的方式得到了patch,也就是上面代码部分的16x16卷积操作；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ViT一般是现在一个很大的数据集上进行预训练，再针对下游任务进行微调(like bert)，根据以前的经验，使用比预训练更高分辨率的图片进行微调更有用。需要注意的是，虽然微调增加图片分辨率对Transformer没有影响，但是前期预训练好的位置编码可能就意义不大了，文中推荐采取二维插值的办法；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上面提到增加了CLS分类头，那么能否不用它，而是直接对最终的$(196, 768)$做平均，然后进行分类呢？实验证明二者性能也差不多。（那为什么要使用CLS？只是为了和BERT一类的方法保持一致性）；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;位置编码和CLS头可以简单按照下面的方法添加：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;position_embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;196&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;class_patch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Parameter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;exp&#34;&gt;Exp.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据集上，模型主要用了：ImageNet (1K class, 1.3M image)、ImageNet-21K (21k class, 14M image)和JFT (18k class, 303M 高分辨率image)做预训练，用了CIFAR-10等多个数据集做测试(包括微调和few-shot的方式)；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型变体上，base和large和BERT一样，但是ViT扩展了Huge的版本：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit_model.png&#34;
	width=&#34;690&#34;
	height=&#34;163&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT variants&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;423&#34;
		data-flex-basis=&#34;1015px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;后续的文献和模型应用中，有特定的表示方法，如ViT-L/16表示ViT Large, patch的大小是16x16；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;比较的baseline主要是两个：BiT(Big Transfer，ResNet-based)和Noisy Student(semi-supervised, EfficientNet-based)，他们是下面数据集的SOTA，其中Noisy Student是ImageNet的SOTA，BiT是其他几个的SOTA；具体实验参数是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.abelcse.cn/p/paper-notes-for-visual-language-models/vit_exp_1.png&#34;
	width=&#34;953&#34;
	height=&#34;308&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ViT-Exp1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;309&#34;
		data-flex-basis=&#34;742px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中TPUv3-core-days表示以：使用一个TPUv3单核训练一天，为标准单位。&lt;em&gt;可以看到，ViT-H/14 要2500个，普通机构是消耗不起的&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但我们依旧能看到，ViT可以说是全胜，这也证明了开头论文说的&lt;strong&gt;继续增大训练数据的规模后，ViT的性能在逐步增加，并实现SOTA&lt;/strong&gt;; （但是后面也做了实验，实验结果大概是：&lt;strong&gt;数据集较小时，建议还是使用ResNet，数据集很大时用ViT来预训练才会有用&lt;/strong&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ViT的训练时间也变少了（相对两个baseline来说）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclu&#34;&gt;Conclu.&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;ViT适合用在数据集较大的视觉预训练任务上，如果数据集较小，使用ResNet更合适；&lt;/li&gt;
&lt;li&gt;ViT相对CNN-based的方法，训练更省时间，但预训练的成本依旧是一般机构无法承担的；&lt;/li&gt;
&lt;li&gt;混合结构Hybrid，即上面代码中利用卷积的方式，而非直接按照图片像素切分成patch，在小模型上表现更好，但随着模型变大，就不如直接切分了（原文中也比较疑惑，因为混合结构应该是兼具二者长处的，&lt;em&gt;个人认为可能是模型大了后，Transformer不再需要inductive bias的帮助，甚至它可能会影响SA的学习，因此模型越大，纯SA的Transformer就更好&lt;/em&gt;）&lt;/li&gt;
&lt;li&gt;当前的ViT主要用在分类任务上，那么还有很多的，如目标检测、分割等任务需要进一步的研究&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;clip&#34;&gt;CLIP&lt;/h2&gt;
&lt;p&gt;Learning Transferable Visual Models From Natural Language Supervision. (ICML 2021 CCF-A)&lt;/p&gt;
&lt;h3 id=&#34;abs--int-1&#34;&gt;Abs. &amp;amp; Int.&lt;/h3&gt;
&lt;p&gt;先前用于分类的SOTA模型，需要通过对预定义好的类别进行学习，这种方式使得这类模型的通用性和扩展性不好，因此一旦需要预测新的类别时，就需要额外的标注数据进行训练。那么，通过直接从文本中学习图像也许可以是一种更节省更直观的替代方案。&lt;/p&gt;
&lt;p&gt;在NLP任务中，以GPT3为例，通过利用大规模语料进行学习的预训练模型，即使不增加额外数据或只使用很少的数据微调，也能够很好地应用于下游任务。这种利用大量网络语料的方法所产生的效果已经比高质量人工标注数据带来的性能提升更强了。&lt;/p&gt;
&lt;p&gt;但是在CV这边，却主要还是依靠人工标注的数据，那么能不能借箭NLP这种方法，使用来自于网络的文本和图像，而不再依靠手工标注的数据呢？&lt;/p&gt;
&lt;p&gt;事实上，以前也有很多工作采取了这种方式，但他们依旧不如全监督的模型。这主要的原因在于这些方法所使用的数据的规模太少。&lt;/p&gt;
&lt;h3 id=&#34;model-1&#34;&gt;Model.&lt;/h3&gt;
&lt;h4 id=&#34;dataset&#34;&gt;Dataset.&lt;/h4&gt;
</description>
        </item>
        
    </channel>
</rss>
