<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(4) > ul > li:nth-child(1) > ul > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(4) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(4) > ul > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(4) > ul > li:nth-child(1) > ul > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Normalization and Attention | abel’s blog</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Normalization and Attention" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Record abel’s idea and information." /> <meta property="og:description" content="Record abel’s idea and information." /> <link rel="canonical" href="https://blog.abelcode.tech/docs/tech/minimind/norm-and-attn/" /> <meta property="og:url" content="https://blog.abelcode.tech/docs/tech/minimind/norm-and-attn/" /> <meta property="og:site_name" content="abel’s blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-04-25T19:37:49+08:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Normalization and Attention" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-25T19:37:49+08:00","datePublished":"2025-04-25T19:37:49+08:00","description":"Record abel’s idea and information.","headline":"Normalization and Attention","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.abelcode.tech/docs/tech/minimind/norm-and-attn/"},"url":"https://blog.abelcode.tech/docs/tech/minimind/norm-and-attn/"}</script> <!-- End Jekyll SEO tag --> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> abel's blog <!-- <div class="site-title-text"> abel's blog </div> --> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/docs/hello-world.html" class="nav-list-link">Hello</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Ideas category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/idea/" class="nav-list-link">Ideas</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/idea/2025/" class="nav-list-link">Ideas 2025</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Paper category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/paper/" class="nav-list-link">Paper</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/paper/diffusion/" class="nav-list-link">Diffusion Models</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Tech category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/tech/" class="nav-list-link">Tech</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Build LLM from Scratch category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/tech/minimind/" class="nav-list-link">Build LLM from Scratch</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/tech/minimind/norm-and-attn/" class="nav-list-link">Normalization and Attention</a></li></ul></li><li class="nav-list-item"><a href="/docs/tech/server/" class="nav-list-link">Server Management Notes</a></li></ul></li><li class="nav-list-item"><a href="/about/" class="nav-list-link">About</a></li></ul> </nav> <script defer src="https://busuanzi.9420.ltd/js"></script> <footer class="site-footer"> <p>Total Visits: <span id="busuanzi_site_pv"></span></p> <p>Personal blog, don't repost.</p> <p>Jekyll with Just the Docs.</p> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div></div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/docs/tech/">Tech</a></li> <li class="breadcrumb-nav-list-item"><a href="/docs/tech/minimind/">Build LLM from Scratch</a></li> <li class="breadcrumb-nav-list-item"><span>Normalization and Attention</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1> Normalization and Attention </h1> <p>Creation Time: 2025-04-25 19:37</p><hr> <p class="label label-green">tech notes</p> <blockquote class="note"> <p>本节将对大模型中常用的RMSNorm和基于旋转位置编码的注意力机制做介绍</p> </blockquote> <h2 id="1-normalization"> <a href="#1-normalization" class="anchor-heading" aria-labelledby="1-normalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1 Normalization </h2> <h3 id="11-layernorm"> <a href="#11-layernorm" class="anchor-heading" aria-labelledby="11-layernorm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1.1 LayerNorm: </h3> <p>layerNorm是自然语言处理任务中最为常用的一种正则化函数，和BatchNorm不同的在于，它计算的是每个样本的隐藏层的正则。</p> <p>对于输入\(x \in \mathbb{R}^{B \times L \times H}\)（其中B表示batch size，L表示序列长度，H表示特征维度），LayerNorm通过如下步骤完成对特征维度的正则化处理。</p> <ul> <li>按照公式（1.1）得到均值\(\mu\)（一阶原点矩）：</li> </ul> \[\mu = \frac{1}{H}\sum^H_{i=1} x_i \tag{1.1}\] <ul> <li>按照公式（1.2）得到方差\(\sigma^2\)（二阶中心矩）：</li> </ul> \[\sigma^2 = \frac{1}{H} \sum ^H _{i=1} (x_i - \mu)^2 \tag{1.2}\] <ul> <li>最后再按照公式（1.3）得到正则化值：</li> </ul> \[\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \tag{1.3}\] <p>其中，\(\epsilon\)是为了避免除零错误而引入的一个很小的数，\(\gamma\)和\(\beta\)分别表示可学习的缩放参数和偏置。</p> <p>一个简单的实现如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
</code></pre></div></div><hr /> <p>而在LLaMA等大模型中，用的更多的是RMSNorm（Root Mean Square Norm），具体计算如下：</p> <h3 id="12-rmsnorm"> <a href="#12-rmsnorm" class="anchor-heading" aria-labelledby="12-rmsnorm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1.2 RMSNorm: </h3> <ul> <li>类似的输入\(x\)，首先根据公式（1.4）计算出均方根（二阶原点矩开根号）：</li> </ul> \[\text{RMS}(x) = \sqrt{\frac{1}{H} \sum ^H _{i=1}x^2_i} \tag{1.4}\] <ul> <li>之后按公式（1.5）得到正则化的值：</li> </ul> \[\text{RMSNorm}(x) = \gamma \frac{x}{\text{RMS}(x) + \epsilon} \tag{1.5}\] <p>其中\(\gamma\)和\(\epsilon\)分别是可学习的缩放参数和防止除零操作。</p> <p>和LayerNorm做对比，RMSNorm的主要区别在于：直接计算二阶原点矩，而不是先计算二阶中心矩，因此少了求均值的操作（没有了减均值的步骤）</p> <p>正是因为少了一步操作，RMSNorm计算步骤更少，因此速度更快，对于对计算量消耗巨大的大模型而言非常有益。</p> <p>实际验证发现RMSNorm与LayerNorm相比，性能上的损失并不大，但其带来的速度增益更可观。</p> <p>其简单的实现如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">rms</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span>
</code></pre></div></div><hr /> <p>这里做了一个（并不严谨）非常简单的小实验，在NYT24数据集上做分类（不求三元组，只求每个样本中可能存在的关系，即多标签分类）</p> <ul> <li>模型方面，使用BERT的分词器和nn.Embedding构建了一个非常简单的模型，其参数量仅为7.92M</li> </ul> <div style="display: flex; justify-content: space-between;"> <img src="./LayerNorm_f1.png" alt="LayerNorm f1-score" style="zoom:60%; margin-right: 10px;" /> <img src="./RMSNorm_f1.png" alt="RMSNorm f1-score" style="zoom:60%;" /> </div> <center>Figure 1.1 LayerNorm和RMSNorm的F1值变化曲线</center> <p>如图1.1所示，两个不同正则化函数的最好F1值分别为：</p> <ul> <li> <p>LayerNorm: 0.9585</p> </li> <li> <p>RMSNorm: 0.9559</p> </li> </ul> <p>可见二者的性能差别并不大。</p> <div style="display: flex; justify-content: space-between;"> <img src="./LayerNorm_step_loss.png" alt="LayerNorm step loss" style="zoom:60%; margin-right: 10px;" /> <img src="./RMSNorm_step_loss.png" alt="RMSNorm step loss" style="zoom:60%;" /> </div> <center>Figure 1.2 LayerNorm和RMSNorm的loss变化曲线</center> <p>图1.2显示了两个正则函数的loss变化，虽然一开始RMSNorm有着略微大一些的loss，但并不影响RMSNorm的学习效率。</p> <p>二者最终的训练时间分别为：</p> <ul> <li> <p>LayerNorm: 231.2874秒</p> </li> <li> <p>RMSNorm: 214.5342秒</p> </li> </ul> <p>可见RMSNorm确实有着更少的学习时间，所节省的时间在比7.92M大的多的模型上会更具优势。</p><hr /> <h2 id="2-attention-with-rope"> <a href="#2-attention-with-rope" class="anchor-heading" aria-labelledby="2-attention-with-rope"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2 Attention with RoPE </h2> <p>下文需要先了解绝对位置编码和相对位置编码，可参见：</p> <ul> <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li> <li><a href="https://spaces.ac.cn/archives/8130">让研究人员绞尽脑汁的Transformer位置编码</a></li> </ul> <p>旋转位置编码的来源和详细介绍可以参见：</p> <ul> <li><a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></li> <li><a href="https://spaces.ac.cn/archives/8265">Transformer升级之路：2、博采众长的旋转式位置编码</a></li> </ul> <p>RoFormer的提出动机非常简单，苏剑林在博客中自称是通过\(e^{im\theta}\bar{e^{in\theta}} = e^{i(m-n)\theta}\)发现可以构建出一种实际上是相对，表达上又是绝对形式的位置编码。</p> <p>在介绍具体含义前，先引入两个前置知识：</p> <h3 id="21-前置知识"> <a href="#21-前置知识" class="anchor-heading" aria-labelledby="21-前置知识"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.1 前置知识 </h3> <p>2.1.1 <strong>二维平面的旋转</strong></p> <p>从计算机图形学的视角来看，什么是<strong>旋转</strong>？</p> <p>假设有二维向量\(v = \begin{bmatrix} x \\ y \end{bmatrix}\)，对他做二维平面旋转，旋转角度为\(\theta\)，可以通过旋转矩阵实现。旋转矩阵的形式如公式（2.1）所示。</p> \[R_{\theta} = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin \theta &amp; \cos \theta \\ \end{bmatrix} \tag{2.1}\] <p>因此旋转后的向量如公式（2.2）所示。</p> \[v' = R_{\theta} \cdot v \tag{2.2}\] <p>这个操作在计算机图像学中表示“将图像绕原点旋转\(\theta\)度”（在实现时需要先计算出图像的原点坐标）。</p> <p>2.1.2 <strong>二维复平面的旋转</strong></p> <p>复数\(e^{i\theta}\)表示了一个沿着单元圆逆时针旋转角度为\(\theta\)的点的位置（欧拉公式），即\(e^{i\theta} = \cos\theta + i \sin \theta\)，其中\(\cos\theta\)是水平方向的坐标（实部），\(\sin\theta\)是竖直方向的坐标（虚部）。</p> <p>而两个点\(z_1 = e^{im\theta}\)和\(z_2 = e^{in\theta}\)之间的乘积如公式（2.3）表示：</p> \[\begin{equation} \begin{aligned} z_1 \times z_2 &amp;= e^{im\theta} \times e^{in\theta}\\ &amp; = \cos m\theta \cos n\theta + i^2 \sin m \theta \sin n \theta + i (\cos m \theta \sin n \theta + \sin m \theta \cos n \theta) \\ &amp; = \cos (m+n)\theta + i \sin (m+n)\theta \\ &amp; = e ^ {i (m+n) \theta} \end{aligned} \tag{2.3} \end{equation}\] <p>也就是说，二者的乘积实际上表示了两个点在复平面上的角度之和。</p> <p>如果用\(z_2\)的共轭\(\bar{z_2} = e^{-i n \theta}\)，就变成了公式（2.4）所示的角度之差：</p> \[z_1 \times \bar{z_2} = e^{i (m-n)\theta} \tag{2.4}\] <p>因此就能够建模出两个复数之间的相对性。</p><hr /> <p>根据前置知识，假设二维平面上有一个点为\((x, y)\)，则它旋转\(\theta\)后的坐标如公式（2.5）所示：</p> \[\begin{bmatrix} \cos \theta &amp; - \sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x \cos \theta - y \sin \theta \\ x \sin \theta + y \cos \theta \end{bmatrix} \tag{2.5}\] <p>同理，假设二维复平面有 \(z = x + i y\)，则乘上旋转角度的复数变化\(e ^ {i\theta}\)后的复数如公式（2.6）所示：</p> \[\begin{equation} \begin{aligned} e^{i\theta} \cdot z &amp; = (\cos \theta + i \sin \theta)(x + i y) \\ &amp; = (x \cos \theta - y \sin \theta) + i (x \sin \theta + y \cos \theta) \end{aligned} \tag{2.6} \end{equation}\] <p>可见，公式（2.6）变化后的实部和虚部恰好和与公式（2.5）旋转后的坐标相对应，所以本质上一个复数乘以\(e^{i \theta}\)，就是乘了一个旋转矩阵。</p> <h3 id="22-旋转位置编码"> <a href="#22-旋转位置编码" class="anchor-heading" aria-labelledby="22-旋转位置编码"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.2 旋转位置编码 </h3> <p>本来想写不少的，但发现已经有不少优秀的博客做了解释，可以直接参考，如：</p> <ul> <li><a href="https://www.zhihu.com/tardis/bd/art/647109286">十分钟读懂旋转编码（RoPE）</a></li> </ul> <p>简单来说，旋转位置编码就是实现了使每个token的位置可以直接被计算出来，就像绝对位置编码那样，同时又能在注意力计算时建模出token之间的相对位置关系。</p> <p>在实际计算时，对于位置为\(i\)的token，将其特征向量分组（按照奇偶进行分组），从而有公式（2.7）所示的转换。</p> \[\text{RoPE}(x_i) = (x_i^{even} \cos i\theta - x_i^{odd} \sin i\theta) \oplus (x_i^{even} \sin i\theta + x_i ^{odd}\cos i\theta) \tag{2.7}\] <p>因此，当计算注意力相关性分数\(q_m^Tk_n\)时，可以得到含有相对位置信息\((m-n)\)的特征。</p> <p>也就是说，RoPE将每个token的特征向量都看作是由若干个二维子空间组成的向量，然后对于每个二维子空间上都根据token所在位置\(i\)旋转角度\(i\theta\)。</p> <p>而在计算相关性分数时，由于复数的性质（或者说旋转矩阵的特点），自然的就会得到任意两个token之间的位置信息，相当于把任意两个token看作是一个旋转了\((i-j)\theta\)度。</p> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2019-2025 abel.</p> <div class="d-flex mt-2"> </div> </footer> </div> </div> </div> <script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.6/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script> </body> </html>
