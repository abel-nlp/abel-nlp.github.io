<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>todo-list on abel&#39;s blog</title>
        <link>https://blog.abelcse.cn/tags/todo-list/</link>
        <description>Recent content in todo-list on abel&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Tue, 02 May 2023 09:46:16 +0800</lastBuildDate><atom:link href="https://blog.abelcse.cn/tags/todo-list/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Literature Curation Plan</title>
        <link>https://blog.abelcse.cn/p/literature-curation-plan/</link>
        <pubDate>Tue, 02 May 2023 09:46:16 +0800</pubDate>
        
        <guid>https://blog.abelcse.cn/p/literature-curation-plan/</guid>
        <description>&lt;img src="https://blog.abelcse.cn/p/literature-curation-plan/paper.png" alt="Featured image of post Literature Curation Plan" /&gt;&lt;h2 id=&#34;important&#34;&gt;Important&lt;/h2&gt;
&lt;h3 id=&#34;t000&#34;&gt;#T000&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Attention is All you Need. (#T001 Transformer)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Augmenting Reinforcement Learning with Human Feedback. (#T004 RLHF)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Toolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; RWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; LIMA: Less Is More for Alignment. (#T008 LIMA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; LLaMA: Open and Efficient Foundation Language Models. (#T010 LLaMA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; LoRA: Low-Rank Adaptation of Large Language Models. (#T011 LoRA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;survey&#34;&gt;Survey&lt;/h2&gt;
&lt;h3 id=&#34;s000&#34;&gt;#S000&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.  (#S001 Prompt)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Multimodal Deep Learning. (#S002 Multimodal)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;report&#34;&gt;Report&lt;/h2&gt;
&lt;h3 id=&#34;r000&#34;&gt;#R000&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Improving Language Understanding by Generative Pre-Training. (#R001 GPT)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Language Models are Unsupervised Multitask Learners. (#R002 GPT2)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Language Models are Few-Shot Learners. (#R003 GPT3)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Training language models to follow instructions with human feedback. (#R004 InstructGPT)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; GPT-4 Technical Report. (#R005 GPT4-Report1)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;re&#34;&gt;RE&lt;/h2&gt;
&lt;h3 id=&#34;e000&#34;&gt;#E000&lt;/h3&gt;
&lt;h4 id=&#34;joint&#34;&gt;Joint&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;few-shot&#34;&gt;Few-shot&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; (#E009 FewRel)
FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel)
FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Few-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Query-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Relation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ner&#34;&gt;NER&lt;/h2&gt;
&lt;h3 id=&#34;n000&#34;&gt;#N000&lt;/h3&gt;
&lt;h4 id=&#34;discontinuous&#34;&gt;Discontinuous&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kge&#34;&gt;KGE&lt;/h2&gt;
&lt;h3 id=&#34;k000&#34;&gt;#K000&lt;/h3&gt;
&lt;h2 id=&#34;mrc&#34;&gt;MRC&lt;/h2&gt;
&lt;h3 id=&#34;m000&#34;&gt;#M000&lt;/h3&gt;
&lt;h2 id=&#34;mm&#34;&gt;MM&lt;/h2&gt;
&lt;h3 id=&#34;c000&#34;&gt;#C000&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale.  (#C001 ViT)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation.  (#C005 NMT)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (#C008 ALBEF)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Content will be continuously added.&lt;/em&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
