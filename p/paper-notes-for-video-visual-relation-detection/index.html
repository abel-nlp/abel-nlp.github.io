<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Notes for Video Visual Relation Detection Task.'>
<title>Paper Notes for Video Visual Relation Detection</title>

<link rel='canonical' href='https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/'>

<link rel="stylesheet" href="/scss/style.min.8113fc0702e9d6a0d9674053f9a6b06b0f023098371d52312e996de177dc1466.css"><meta property='og:title' content='Paper Notes for Video Visual Relation Detection'>
<meta property='og:description' content='Notes for Video Visual Relation Detection Task.'>
<meta property='og:url' content='https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/'>
<meta property='og:site_name' content='abel&#39;s blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='paper-notes' /><meta property='article:tag' content='multi-modal' /><meta property='article:tag' content='collection' /><meta property='article:published_time' content='2023-11-06T10:53:42&#43;08:00'/><meta property='article:modified_time' content='2023-11-06T10:53:42&#43;08:00'/><meta property='og:image' content='https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/cover.jpg' />
<meta name="twitter:title" content="Paper Notes for Video Visual Relation Detection">
<meta name="twitter:description" content="Notes for Video Visual Relation Detection Task."><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://blog.abelcse.cn/p/paper-notes-for-video-visual-relation-detection/cover.jpg' />
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hue3f7bdfccc0be810572359657e66a4ad_56159_300x0_resize_q75_box.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">😴</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">abel&#39;s blog</a></h1>
            <h2 class="site-description">Record abel&#39;s Paper and Tech Notes here.</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/abel-nlp'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#dataset">Dataset</a>
      <ol>
        <li><a href="#vidvrd-imagenet-vidvrd">VidVRD (ImageNet-VidVRD)</a>
          <ol>
            <li><a href="#abs--int">Abs. &amp; Int.</a></li>
            <li><a href="#rel">Rel.</a></li>
            <li><a href="#dataset-1">Dataset.</a></li>
            <li><a href="#method">Method.</a></li>
          </ol>
        </li>
        <li><a href="#vidor">VidOR</a>
          <ol>
            <li><a href="#abs--int-1">Abs. &amp; Int.</a></li>
            <li><a href="#rel-1">Rel.</a></li>
            <li><a href="#stat">Stat.</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#vrd-gcn">VRD-GCN</a>
      <ol>
        <li><a href="#abs--int-2">Abs. &amp; Int.</a></li>
        <li><a href="#method-1">Method.</a>
          <ol>
            <li><a href="#片段关系预测">片段关系预测</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/paper/" style="background-color: 00BFFF; color: #fff;">
                Paper
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/paper-notes-for-video-visual-relation-detection/">Paper Notes for Video Visual Relation Detection</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Notes for Video Visual Relation Detection Task.
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Nov 06, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    13 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="dataset">Dataset</h2>
<h3 id="vidvrd-imagenet-vidvrd">VidVRD (ImageNet-VidVRD)</h3>
<p>Video Visual Relation Detection. (ACM MM 2017)</p>
<h4 id="abs--int">Abs. &amp; Int.</h4>
<p>作为连接视觉和语言的桥梁，以关系三元组<code>&lt;主语，谓语，宾语&gt;</code>为形式的目标间视觉关系提供了超过目标本身的更为全面的视觉内容理解，如<code>&lt;person, touch, dog&gt;</code>和<code>&lt;cat, above, sofa&gt;</code>等。</p>
<p>本文提出了一种新的视觉任务，叫作<strong>Video Visual Relation Detection (VidVRD)</strong>，用于在视频而非静态的图片上进行关系检测。与静态图相比，视频为检测视觉关系提供了更自然的一组特征，比如动态的关系如<code>&lt;A, follow, B&gt;, &lt;A, towards, B&gt;</code>（A跟着B，A朝着B），以及类似<code>&lt;A, chase, B&gt;</code>然后<code>&lt;A, hold, B&gt;</code>（A追逐B，然后A抓住B）这种时序关系。然而，VidVRD任务相比于图像关系检测任务ImgVRD，在技术上具有更大的挑战性，因为其很难对准确的目标进行追踪和应付多种多样的关系表现。</p>
<p>为此，本文提出了一个包含目标轨迹小片段候选、短期关系预测和贪心关系关联的VidVRD方法，并提供了一个用于任务评估的数据集，其中包含1000个人工标注的视频。与以前的相关方法相比，本文在该数据集上实现了最佳性能。</p>
<p>缩小视觉和语言之间的差距对多媒体分析至关重要，相关工作涉及<em>visual concept annotations, semantic description with captioning</em> 和 <em>visual question-answering</em>、字幕语义描述和视觉问答等。视觉关系检测<em>VRD</em>是最近为了更全面地理解目标之外的视觉内容而做出的工作，目的是为了捕获目标之间的各种互动，它可以有效地支持众多的视觉语言任务，如<em>captioning, visual search</em> 和 <em>visual question-answering</em>等。</p>
<p>视觉关系包括用边界框定位的一对目标和连接他们之间的谓语，也就是关系。如图1-1(a)所示，其中两个目标可以有不同的关系，同一个关系也可连接起不同外观的目标对。在本文中将使用关系三元组来表示由<code>&lt;subject, predicate, object&gt;</code>构成的视觉关系类型。因为这种组合的可能性非常多，传统的目标检测方法在这个任务中并不适合，即使已经有的几个相关任务的方法也更适用于静态图片而非视频。与静态图片相比，视频为检测视觉关系提供了一组更为自然的特征。图1-1(b)中，从视频的时空内容中提取的运动特征有助于区分相似的关系，如<em>walk</em> 和 <em>run</em>。此外，一些动态的视觉关系只能在视频中被检测到，如<code>&lt;dog, run past, person&gt;</code>和<code>&lt;dog, faster than, person&gt;</code>。因此，与ImgVRD相比，VidVRD是一个更通用更可行的任务。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/p1_1.png"
	width="1021"
	height="519"
	
	loading="lazy"
	
		alt="figure 1-1"
	
	
		class="gallery-image" 
		data-flex-grow="196"
		data-flex-basis="472px"
	
></p>
<p>VidVRD和ImgVRD之间的另一个显著不同是：视频中的视觉关系通常是随时间变化的，而图像中的关系则是固定的。目标可能会因为遮挡或离开画面而导致视觉关系出现和消失，即使两个目标始终出现在同一个视频帧中，它们之间的互动关系也可能随着时间而发生变化。如图1-2所示，其中的狗和飞盘同时出现在$t_2$和$t_7$之间，但它们之间的关系却从追逐变成了撕咬。因此，应当重新设定义VidVRD任务，以处理视觉关系的可变性。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/p1_2.png"
	width="1039"
	height="407"
	
	loading="lazy"
	
		alt="figure 1-2"
	
	
		class="gallery-image" 
		data-flex-grow="255"
		data-flex-basis="612px"
	
></p>
<h5 id="definition">Definition</h5>
<p>为了与ImgVRD的定义保持一致，本文将VidVRD任务定义如下：给定一组目标类别$C$和谓语(关系)类别$P$，VidVRD旨在检测出视频中的视觉关系实例$C \times P \times C$，其中视觉关系实例由关系三元组<code>&lt;subject, predicate, object&gt;</code>$\in C \times P \times C$表示，并带有主语和宾语的轨迹信息$T_s$和$T_o$，具体来说，$T_s$和$T_o$是两个边界框的序列，在视觉关系的最长持续时间内构成主语和宾语。如图1-2中，给定的视频关系可以用三元组<code>&lt;dog, chasem frisbee&gt;</code>和<code>&lt;dog, bite, frisbee&gt;</code>来表示，狗和飞盘分别用$(t_2, t_4)$和$(t_5, t_7)$之间的红色与绿色轨迹定位。</p>
<p>与ImgVRD相比，VidVRD面临更多的技术挑战。</p>
<p>首先，VidVRD需要用边界框轨迹来定位目标，这比在ImgVRD中为每个目标提供一个边界框更加困难，因为轨迹准确性受到每一帧的目标定位及目标跟踪的性能影响。本文所提出的VidVRD方法通过在视频的每个重叠短片段中生成目标轨迹小片段，然后根据预测出的关系将小片段和目标轨迹关联起来。其次，VidVRD需要在最长持续时间中时序地定位视觉关系。为了完成该任务，本文提出了一个贪心关联算法，其通过判断相邻片段中检测到的视觉关系是否具有相同关系三元组，且目标轨迹小片段具有足够高的重叠度来决定是否合并他们。最后，与ImgVRD相比，VidVRD需要预测更多类型的视觉关系，因为有的视觉关系只能在视频中检测到，为了有效预测关系，本文提出了关系预测模型，该模型从主语/宾语的轨迹小片段中提取出多种特征，包括外观特征、运动特征和相对特征。然后将这些特征编码成关系特征，再分别使用单独的主语、宾语和关系预测器来预测视觉关系。</p>
<p>截至目前，还没有针对VidVRD的数据集，不过有几个针对ImgVRD的数据集，如 <em>Visual Relationship dataset, Visual Genome</em>，因此，本文构建了一个VidVRD的数据集进行评估。本文设计了一个谓语描述机制，并从 <em>ILSVRC2016-VID</em> 中构建该数据集。该数据集包含1000个视频，其中有人工标注的视觉关系和目标边界轨迹信息。</p>
<p>本文的主要贡献在于：</p>
<ol>
<li>提出了新颖的VidVRD任务，旨在探索视频中目标之间的各种关系，与ImgVRD相比，它提供了更可行地VRD任务；</li>
<li>提出了一种新的VidVRD方法，该方法通过目标轨迹小片段候选、关系预测和贪心关系关联来检测视频中的视觉关系；</li>
<li>贡献了第一个VidVRD数据集，带有1000个人工标注的视频数据。</li>
</ol>
<h4 id="rel">Rel.</h4>
<h5 id="video-object-detection">Video object detection</h5>
<p>视频目标检测旨在从给定视频中检测出预定义的目标，并通过边界框轨迹对其进行定位。目前最好的方法主要是通过整合目标检测和多目标跟踪来实现。</p>
<p>最近，复杂的深度神经网络在图像目标检测方面取得了很好的性能。然而视频中存在模糊、运动和遮挡等情况，阻碍了利用边界框轨迹对目标进行精确定位的操作，因此视频中的目标检测仍然存在准确率低的问题。</p>
<p>另一方面，由于目标检测器漏检率较高，采用逐个检测追踪策略的多目标追踪往往会产生较短的轨迹，因此有研究开发了额外的合并算法，以获得时间上更一致的目标轨迹。</p>
<p>为此，本文的方法利用视频目标检测器生成短时的目标小轨迹候选来避免上面两个问题，并且该方案适合所有好的目标检测和多目标追踪方法。</p>
<h5 id="visual-relation-detection">Visual relation detection</h5>
<p>近期的研究工作主要集中于图像的关系检测上，人们普遍认为视觉关系检测的基本挑战在于，如何通过从少量训练实例中学习来建模和预测大量的关系。为解决该问题，现有的大部分方法都是分别预测视觉关系三元组中的主语、宾语和谓语。但目前的ImgVRD方法不适用于VidVRD任务，如其中的动态关系和视频关系的可变性。因此，本文提出了一种视频特定关系特征和一个新的训练准则，用于学习单独的预测模型。</p>
<p>本文的方法是首次在视频上进行视觉关系检测的尝试，虽然之前的一些工作也和视频视觉关系任务相关，但它们所追求的目标和VidVRD完全不同。</p>
<h5 id="action-recognition">Action recognition</h5>
<p>动作是视觉关系中一种主要的谓词类别，VidVRD可以借鉴动作识别方面的进展，在动作识别中，特征表示在处理大的类别差距、背景干扰和摄像机运动方面起着重要的作用。为了解决这些问题，人们开发了手工特征和深度神经网络的方法。受相关工作的启发，本文将改进的密集轨迹(iDT)作为部分特征，因为iDT在大多数动作识别数据集上取得了出色的性能，尤其是在训练数据不足的情况下。值得注意的是，本文的方法旨在检测目标之间比动作更一般的关系，如空间关系和比较（相对）关系。</p>
<h4 id="dataset-1">Dataset.</h4>
<p>本文基于 <em>ILSVRC2016-VID</em> 构建了第一个用于评估VidVRD任务的数据集，包含人工标注边界框的30类目标的视频共1000个，视频中视觉关系清晰丰富，忽略了目标单一，视觉关系模糊的视频。数据集被划分为训练集和测试集，分别包含800个和200个视频。</p>
<p>此外，还补充了5个经常出现的目标类别，即 <em>person, ball, sofa, skateboard</em> 和 <em>frisbee</em>，因此共有35个目标种类。这些类别都是相对比较独立的，也就是不包括目标之间的分属关系，比如<code>&lt;bicycle, with, wheel&gt;</code>。</p>
<p>通过对及物动词、比较动词、空间描述和不及物动词等进行筛选和变换，共有132种关系被包含在数据集中。</p>
<p>具体的数据划分等，见表1-1：</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/t1_1.png"
	width="874"
	height="446"
	
	loading="lazy"
	
		alt="table 1-1"
	
	
		class="gallery-image" 
		data-flex-grow="195"
		data-flex-basis="470px"
	
></p>
<h4 id="method">Method.</h4>
<h3 id="vidor">VidOR</h3>
<p>Annotating Objects and Relations in User-Generated Videos. (ICMR 2019)</p>
<h4 id="abs--int-1">Abs. &amp; Int.</h4>
<p>要进行细粒度的视频内容分析，理解目标之间的关系必不可少。然而，现存的工作受到数据集规模太小和评价指标不直接的限制。这些限制主要是因为构建大规模详细标注的视频数据集费时费力。因此，本文提出了一种pipeline的视频标注方法，能够以少量的花费构建数据集，并构造了新的视频数据集VidOR，该数据集包含了10k个视频（大概84小时），共80个目标类别和50个关系类别。</p>
<p>细粒度的视频内容分析很重要，目前一些视频字幕描述和问答任务已经证明了这一点。随着粒度的不断细化，理解目标之间的关系也变得重要，目前的研究表明理解目标之间的关系对视频内容分析生成更稳健的表示是有效的。但是目前相关的工作都是较为隐性地建模和评估目标之间的关系，对模型是否和如何很好理解细粒度内容并不很清楚。</p>
<p>但是在视频中进行目标和关系的识别是很有挑战性的任务，这需要对目标的外观、身份、动作和目标之间的交互。如图1-1中的狗所示，其外观可能因为它的活动、光照和遮挡而发生较大变化，这对给跨视频帧去确定狗的身份带来较大的困扰，而这却是进一步总结视频内容的必要线索。此外，动作和交互表示的差异也对模型学习根本的模式和稳健的推理带来了新的挑战。为此有很多工作，如<em>video object detection</em>和<em>video visual relation detection</em>在深入地研究这些问题，但他们主要受到了可用数据集规模太小的限制。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/vidor_1.png"
	width="2092"
	height="357"
	
	loading="lazy"
	
		alt="figure 1-1"
	
	
		class="gallery-image" 
		data-flex-grow="585"
		data-flex-basis="1406px"
	
></p>
<p>对上面的内容，我们可以总结为：</p>
<ol>
<li>细粒度的视频内容分析非常重要，比如生成视频字幕，进行视频问答等；</li>
<li>但是进行细粒度的视频内容分析并不容易，这主要是因为现存的数据集规模小，无法设计更好的模型；</li>
<li>而视频中目标的关系检测是很重要的部分，能够帮助进行视频内容分析。而要进行关系检测，需要考虑目标的外观、身份、动作和目标之间的交互，这是具有挑战性的，甚至连一个这样的大规模数据集都还没有。</li>
</ol>
<p>因此，本文的目的就是构建一个包含详细标注的目标和关系的更大规模数据集，视频均来源于网上以确保反映真实场景。标注信息通过边界框轨迹对目标进行时空定位，对关系进行时间定位。和类似的图片数据集相比，本文希望构建一个视频的benchmark，并且能够对动作和动态关系进行学习和推理。</p>
<p>但是构建这样的数据集代价高昂，假设平均一个视频150帧，10k个视频就需要标注1.5m张图片，这已经达到了当前一些图像数据集的规模。</p>
<p>如果假设相邻帧之间具有相似性，那么就可以在固定的间隔中进行手工标注，然后再进行插值。但这种方式只适合电影及监控视频这种连贯的视频类型。而由用户生成的视频内容则是完全自由的，质量一般不太高，视频的连续性也不强，所以不能简单的进行插值，而需要选择一种更好的策略进行关键帧的选取和人工标注。</p>
<p>此外，如何将标注任务划分为子任务也是比较复杂的问题。假设我们将标注视频中的目标和关系视为一个单一任务，那么标注者就需要接受相关的培训，花费大量的精力。因此，更好的选择是将标注任务变成更宏观的子任务，如判断目标的类型，确定是否有边框，边框是否正确等等。</p>
<p>为此，本文将设计一种新的pipeline方法，它适用于长度从几秒到几分钟不等的用户生成的视频。</p>
<h4 id="rel-1">Rel.</h4>
<h5 id="video-annotation">Video Annotation</h5>
<p>当前的时序标注大致可以分为两类，一类是通过标注目标在视频中的开始和结束帧进行时序的定位，因此需要标注者仔细地浏览整个视频；另一类方法是在较短的视频片段钟标记目标是否存在，然后自动合并各片段的结果以实现时间定位，这种方法相对节省时间，但可能产生粗粒度的标注结果。</p>
<p>在本文的方法中，通过为不同的关系仔细挑选合适的方法，从而结合了两类方法的优点。</p>
<h5 id="video-datasets">Video Datasets</h5>
<p>视频数据集按照标注程度来划分。一种是按照整个视频级别来进行标注，如CCV、Kinetics和MSR-VTT，他们都是用于事件识别，动作识别和视频字幕任务的典型benchmark；第二类是片段级别的标注，比如THUMOS、MultiTHUMOS和ActivityNet Caption，他们一般用于动作、活动和事件的时序定位的评测；第三类是目标级别的标注，TrackingNet就是其中一个大规模的标注了目标边框轨迹的目标追踪数据集。ImageNet-VID是一个超过30类目标的目标检测数据集。AVA则是一个时空活动定位的视觉动作数据集；最后则是关系级别的标注，VidVRD是基于ImageNet-VID构建的当前唯一一个提供目标和关系的视频数据集，但是该数据集相对较小，并且标注的比较稀疏。</p>
<h4 id="stat">Stat.</h4>
<h5 id="annotation-pipeline">Annotation Pipeline</h5>
<p>为了更好地进行标注，本文提出了如图1-2所示的包含目标和关系标注的流水线标注方案。为了完成标注任务，标注者需要完成三个子任务：浏览视频以发现新的目标；绘制这些目标的边框；将被遮挡或者离开镜头外的目标的帧标注为不可见。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/vidor_2.png"
	width="2018"
	height="777"
	
	loading="lazy"
	
		alt="figure 1-2"
	
	
		class="gallery-image" 
		data-flex-grow="259"
		data-flex-basis="623px"
	
></p>
<p><em><strong>多是数据集的描述和统计，后续将简要总结出来。</strong></em></p>
<hr>
<h2 id="vrd-gcn">VRD-GCN</h2>
<p>Video Relation Detection with Spatio-Temporal Graph. (ACM MM 2019)</p>
<h3 id="abs--int-2">Abs. &amp; Int.</h3>
<p>理解视觉信息是计算机视觉的核心目标。视觉关系检测就是其中很有挑战的一项任务，因为我们从视觉内容中获取的信息不仅仅是目标的集合，还包括它们之间的交互关系，也就是需要捕获细粒度的视觉线索，包括目标的位置以及它们之间如何进行交互。</p>
<p>视频中目标之间的关系是深入理解动态视觉内容的重要组成部分，它对很多高层次的视觉任务，如视觉问答、视觉字幕生成等有更精确的促进作用，但视频中的关系检测和推理却很少被探索。</p>
<p>目前已经有很多针对静态图片的关系检测方法，并取得了非常好的结果。一种自然的方法是直接使用它们的方法来进行视频的关系检测，然而由于视频和图像的本质区别：增加了时序维度，使得这些方法在视频任务上并不能获得理想的结果，因为时间通道的视频信息使得很多动态关系要和时序与空间进行关联，这使得在视频中进行关系预测变得更为复杂。</p>
<p>如果我们能够利用时间和空间信息，如在空间维度上，知道了<code>&lt;bicycle, move_right, car&gt;</code>，就能够帮助检测<code>&lt;person, right, car&gt;</code>，在时间维度上，知道<code>&lt;car, faster, bicycle&gt;</code>，就有助于检测到后续的<code>&lt;car, move_past, bicycle&gt;</code>（如图2-1）。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/p2-1.png"
	width="918"
	height="450"
	
	loading="lazy"
	
		alt="figure 2-1"
	
	
		class="gallery-image" 
		data-flex-grow="204"
		data-flex-basis="489px"
	
></p>
<p>本文将视频抽象为全连接的空间-时间图，通过在这种3D图中利用图卷积网络传递信息和推理，从而提出了新的方法：VRD-GCN。简单来说，本文通过空间-时间内容更好地完成动态关系的预测，并利用孪生网络的在线关联方法更准确地完成关系实例<code>&lt;subject, predicate, object&gt;</code>的关联。本文在ImageNet-VidVRD数据集上进行了实验，达到了当时的SOTA。</p>
<h3 id="method-1">Method.</h3>
<p>视频视觉关系检测任务的定义是：给定目标集合$O$，关系（谓语）集合$P$，任意长度的视频，利用主语和宾语的轨迹$T_s, T_o$（即主语和宾语目标在不同帧的目标框序列），需要该任务从中检测出所有<code>&lt;subject, predicate, object&gt;</code>$\in O \times P \times O$的关系实例。如图2-2所示，我们可以将任务分解为三个独立的部分：多目标追踪、关系预测和关系实例关联。</p>
<p>简单来讲，首先将视频切分为片段，并从片段中提取出轨迹段（Trajectory proposals），然后利用本文的方法VRD-GCN来预测所有目标对的关联；最后，利用孪生网络的在线关联方法来关联所有检测到的短期关系实例，形成最终的视频关系实例。需要注意的是，这里的轨迹信息和预处理方法直接使用<!-- raw HTML omitted -->Dataset-VidVRD(ImageNet-VidVRD)<!-- raw HTML omitted -->的结果。</p>
<p><img src="/p/paper-notes-for-video-visual-relation-detection/p2-2.png"
	width="1837"
	height="402"
	
	loading="lazy"
	
		alt="figure 2-2"
	
	
		class="gallery-image" 
		data-flex-grow="456"
		data-flex-basis="1096px"
	
></p>
<h4 id="片段关系预测">片段关系预测</h4>
<p>直接上来看，空间上相互接近的目标有比较强的关联，并且相邻时间段的目标也回保持强关联。因此，本文将视频抽象成全连接的时空图，参考图2-1所示，其中每个目标被视为连接到前一个、当前和下一个片段中所有其他节点的轨迹节点。而处理该图的方法则选用了图卷积神经网络。</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/paper-notes/">paper-notes</a>
        
            <a href="/tags/multi-modal/">multi-modal</a>
        
            <a href="/tags/collection/">collection</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>个人博客，请勿转载。Personal blog, please do not repost.</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css"integrity="sha256-J&#43;iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s="crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js"integrity="sha256-InsNdER1b2xUewP&#43;pKCUJpkhiqwHgqiPXDlIk7GzBu4="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js"integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI="crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/paper-notes-for-visual-language-models/">
        
        
            <div class="article-image">
                <img src="/p/paper-notes-for-visual-language-models/vit.0cbf68317b630fb5074c16d4f01711c6.png" 
                        width="934" 
                        height="461" 
                        loading="lazy"
                        alt="Featured image of post Paper Notes for Visual Language Models"
                        
                        data-hash="md5-DL9oMXtjD7UHTBbU8BcRxg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Paper Notes for Visual Language Models</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2019 - 
        
        2023 abel&#39;s blog
    </section>
    
    <section class="powerby">
        Built with Hugo <br />
        
        Theme Stack designed by Jimmy
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=WenKai:wght@300;400;700&display=swap";
        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
